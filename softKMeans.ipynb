{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate $S$ with Soft K-Means {#sec-estS-soft-kmeans}\n",
    "\n",
    "The basic idea of using Soft K-Means to estimate $S$ is:\n",
    "\n",
    "- We first estimate distribution parameters using contrained K-Means, the exact procedure we covered in @sec-cop-kmeans.\n",
    "- In each iteration, we use Soft Kmeans algorithm to update $\\theta, \\phi$ (refer to @sec-soft-kmeans) and use [Metropolisâ€“Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) to accept or reject a proposed $S$.\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import utils \n",
    "import json \n",
    "import pandas as pd \n",
    "import utils \n",
    "from scipy.stats import kendalltau\n",
    "import sys\n",
    "import os\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_soft_kmeans_for_biomarker(\n",
    "        data,\n",
    "        biomarker,\n",
    "        order_dict,\n",
    "        n_participants,\n",
    "        non_diseased_participants,\n",
    "        hashmap_of_normalized_stage_likelihood_dicts,\n",
    "        diseased_stages,\n",
    "        seed=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate mean and std for both the affected and non-affected clusters for a single biomarker.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The data containing measurements.\n",
    "        biomarker (str): The biomarker to process.\n",
    "        order_dict (dict): Dictionary mapping biomarkers to their order.\n",
    "        n_participants (int): Number of participants in the study.\n",
    "        non_diseased_participants (list): List of non-diseased participants.\n",
    "        hashmap_of_normalized_stage_likelihood_dicts (dict): Hash map of \n",
    "            dictionaries containing stage likelihoods for each participant.\n",
    "        diseased_stages (list): List of diseased stages.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Means and standard deviations for affected and non-affected clusters.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        # Set the seed for numpy's random number generator\n",
    "        rng = np.random.default_rng(seed)\n",
    "    else:\n",
    "        rng = np.random \n",
    "\n",
    "    # DataFrame for this biomarker\n",
    "    biomarker_df = data[\n",
    "        data['biomarker'] == biomarker].reset_index(\n",
    "            drop=True).sort_values(\n",
    "                by = 'participant', ascending = True)\n",
    "    # Extract measurements\n",
    "    measurements = np.array(biomarker_df['measurement'])\n",
    "\n",
    "    this_biomarker_order = order_dict[biomarker]\n",
    "\n",
    "    affected_cluster = []\n",
    "    non_affected_cluster = []\n",
    "\n",
    "    for p in range(n_participants):\n",
    "        if p in non_diseased_participants:\n",
    "            non_affected_cluster.append(measurements[p])\n",
    "        else:\n",
    "            if this_biomarker_order == 1:\n",
    "                affected_cluster.append(measurements[p])\n",
    "            else:\n",
    "                normalized_stage_likelihood_dict = hashmap_of_normalized_stage_likelihood_dicts[\n",
    "                    p]\n",
    "                # Calculate probabilities for affected and non-affected states\n",
    "                affected_prob = sum(\n",
    "                    normalized_stage_likelihood_dict[s] for s in diseased_stages if s >= this_biomarker_order\n",
    "                )\n",
    "                non_affected_prob = sum(\n",
    "                    normalized_stage_likelihood_dict[s] for s in diseased_stages if s < this_biomarker_order\n",
    "                )\n",
    "                if affected_prob > non_affected_prob:\n",
    "                    affected_cluster.append(measurements[p])\n",
    "                elif affected_prob < non_affected_prob:\n",
    "                    non_affected_cluster.append(measurements[p])\n",
    "                else:\n",
    "                    # Assign to either cluster randomly if probabilities are equal\n",
    "                    if rng.random() > 0.5:\n",
    "                        affected_cluster.append(measurements[p])\n",
    "                    else:\n",
    "                        non_affected_cluster.append(measurements[p])\n",
    "\n",
    "    # Compute means and standard deviations\n",
    "    theta_mean = np.mean(affected_cluster) if affected_cluster else np.nan\n",
    "    theta_std = np.std(affected_cluster) if affected_cluster else np.nan\n",
    "    phi_mean = np.mean(\n",
    "        non_affected_cluster) if non_affected_cluster else np.nan\n",
    "    phi_std = np.std(non_affected_cluster) if non_affected_cluster else np.nan\n",
    "    return theta_mean, theta_std, phi_mean, phi_std\n",
    "\n",
    "def soft_kmeans_theta_phi_estimates(\n",
    "        iteration,\n",
    "        prior_theta_phi_estimates,\n",
    "        data_we_have,\n",
    "        biomarkers,\n",
    "        order_dict,\n",
    "        n_participants,\n",
    "        non_diseased_participants,\n",
    "        hashmap_of_normalized_stage_likelihood_dicts,\n",
    "        diseased_stages,\n",
    "        seed=None):\n",
    "    \"\"\"\n",
    "    Get the DataFrame of theta and phi using the soft K-means algorithm for all biomarkers.\n",
    "\n",
    "    Parameters:\n",
    "        data_we_have (pd.DataFrame): DataFrame containing the data.\n",
    "        biomarkers (list): List of biomarkers in string.\n",
    "        order_dict (dict): Dictionary mapping biomarkers to their order.\n",
    "        n_participants (int): Number of participants in the study.\n",
    "        non_diseased_participants (list): List of non-diseased participants.\n",
    "        hashmap_of_normalized_stage_likelihood_dicts (dict): Hash map of dictionaries containing stage likelihoods for each participant.\n",
    "        diseased_stages (list): List of diseased stages.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        a dictionary containing the means and standard deviations for theta and phi for each biomarker.\n",
    "    \"\"\"\n",
    "    # List of dicts to store the estimates\n",
    "    # In each dic, key is biomarker, and values are theta and phi params\n",
    "    hashmap_of_means_stds_estimate_dicts = {}\n",
    "    for biomarker in biomarkers:\n",
    "        dic = {'biomarker': biomarker}\n",
    "        prior_theta_phi_estimates_biomarker = prior_theta_phi_estimates[biomarker]\n",
    "        theta_mean, theta_std, phi_mean, phi_std = calculate_soft_kmeans_for_biomarker(\n",
    "            data_we_have,\n",
    "            biomarker,\n",
    "            order_dict,\n",
    "            n_participants,\n",
    "            non_diseased_participants,\n",
    "            hashmap_of_normalized_stage_likelihood_dicts,\n",
    "            diseased_stages,\n",
    "            seed\n",
    "        )\n",
    "        if theta_std == 0 or math.isnan(theta_std):\n",
    "            theta_mean = prior_theta_phi_estimates_biomarker['theta_mean']\n",
    "            theta_std = prior_theta_phi_estimates_biomarker['theta_std']\n",
    "        if phi_std == 0 or math.isnan(phi_std):\n",
    "            phi_mean = prior_theta_phi_estimates_biomarker['phi_mean']\n",
    "            phi_std = prior_theta_phi_estimates_biomarker['phi_std']\n",
    "        dic['theta_mean'] = theta_mean\n",
    "        dic['theta_std'] = theta_std\n",
    "        dic['phi_mean'] = phi_mean\n",
    "        dic['phi_std'] = phi_std\n",
    "        hashmap_of_means_stds_estimate_dicts[biomarker] = dic\n",
    "    return hashmap_of_means_stds_estimate_dicts\n",
    "\n",
    "def calculate_all_participant_ln_likelihood_and_update_hashmap(\n",
    "        iteration,\n",
    "        data_we_have,\n",
    "        current_order_dict,\n",
    "        n_participants,\n",
    "        non_diseased_participant_ids,\n",
    "        theta_phi_estimates,\n",
    "        diseased_stages,\n",
    "):\n",
    "    data = data_we_have.copy()\n",
    "    data['S_n'] = data.apply(\n",
    "        lambda row: current_order_dict[row['biomarker']], axis=1)\n",
    "    all_participant_ln_likelihood = 0\n",
    "    # key is participant id\n",
    "    # value is normalized_stage_likelihood_dict\n",
    "    hashmap_of_normalized_stage_likelihood_dicts = {}\n",
    "    for p in range(n_participants):\n",
    "        pdata = data[data.participant == p].reset_index(drop=True)\n",
    "        if p in non_diseased_participant_ids:\n",
    "            this_participant_likelihood = utils.compute_likelihood(\n",
    "                pdata, k_j=0, theta_phi=theta_phi_estimates)\n",
    "            this_participant_ln_likelihood = np.log(\n",
    "                this_participant_likelihood + 1e-10)\n",
    "        else:\n",
    "            # normalized_stage_likelihood_dict = None\n",
    "            # initiaze stage_likelihood\n",
    "            stage_likelihood_dict = {}\n",
    "            for k_j in diseased_stages:\n",
    "                kj_likelihood = utils.compute_likelihood(\n",
    "                    pdata, k_j, theta_phi_estimates)\n",
    "                # update each stage likelihood for this participant\n",
    "                stage_likelihood_dict[k_j] = kj_likelihood\n",
    "            # Add a small epsilon to avoid division by zero\n",
    "            likelihood_sum = sum(stage_likelihood_dict.values())\n",
    "            epsilon = 1e-10\n",
    "            if likelihood_sum == 0:\n",
    "                # print(\"Invalid likelihood_sum: zero encountered.\")\n",
    "                likelihood_sum = epsilon  # Handle the case accordingly\n",
    "            normalized_stage_likelihood = [\n",
    "                l/likelihood_sum for l in stage_likelihood_dict.values()]\n",
    "            normalized_stage_likelihood_dict = dict(\n",
    "                zip(diseased_stages, normalized_stage_likelihood))\n",
    "            hashmap_of_normalized_stage_likelihood_dicts[p] = normalized_stage_likelihood_dict\n",
    "\n",
    "            # calculate weighted average\n",
    "            this_participant_likelihood = np.mean(likelihood_sum)\n",
    "            this_participant_ln_likelihood = np.log(\n",
    "                this_participant_likelihood)\n",
    "        all_participant_ln_likelihood += this_participant_ln_likelihood\n",
    "    return all_participant_ln_likelihood, hashmap_of_normalized_stage_likelihood_dicts\n",
    "\n",
    "\n",
    "def metropolis_hastings_soft_kmeans(\n",
    "    data_we_have,\n",
    "    iterations,\n",
    "    n_shuffle,\n",
    "):\n",
    "    '''Implement the metropolis-hastings algorithm using soft kmeans\n",
    "    Inputs: \n",
    "        - data: data_we_have\n",
    "        - iterations: number of iterations\n",
    "        - log_folder_name: the folder where log files locate\n",
    "\n",
    "    Outputs:\n",
    "        - best_order: a numpy array\n",
    "        - best_likelihood: a scalar \n",
    "    '''\n",
    "    n_participants = len(data_we_have.participant.unique())\n",
    "    biomarkers = data_we_have.biomarker.unique()\n",
    "    n_biomarkers = len(biomarkers)\n",
    "    n_stages = n_biomarkers + 1\n",
    "    non_diseased_participant_ids = data_we_have.loc[\n",
    "        data_we_have.diseased == False].participant.unique()\n",
    "    diseased_stages = np.arange(start=1, stop=n_stages, step=1)\n",
    "    # obtain the iniial theta and phi estimates\n",
    "    prior_theta_phi_estimates = utils.get_theta_phi_estimates(\n",
    "        data_we_have)\n",
    "    theta_phi_estimates = prior_theta_phi_estimates.copy()\n",
    "\n",
    "    # initialize empty lists\n",
    "    acceptance_count = 0\n",
    "    all_current_accepted_order_dicts = []\n",
    "\n",
    "    current_accepted_order = np.random.permutation(np.arange(1, n_stages))\n",
    "    current_accepted_order_dict = dict(zip(biomarkers, current_accepted_order))\n",
    "    current_accepted_likelihood = -np.inf\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # in each iteration, we have updated current_order_dict and theta_phi_estimates\n",
    "\n",
    "        new_order = current_accepted_order.copy()\n",
    "        utils.shuffle_order(new_order, n_shuffle)\n",
    "        current_order_dict = dict(zip(biomarkers, new_order))\n",
    "        all_participant_ln_likelihood, \\\n",
    "            hashmap_of_normalized_stage_likelihood_dicts = calculate_all_participant_ln_likelihood_and_update_hashmap(\n",
    "                _,\n",
    "                data_we_have,\n",
    "                current_order_dict,\n",
    "                n_participants,\n",
    "                non_diseased_participant_ids,\n",
    "                theta_phi_estimates,\n",
    "                diseased_stages,\n",
    "            )\n",
    "\n",
    "        # Now, update theta_phi_estimates using soft kmeans\n",
    "        # based on the updated hashmap of normalized stage likelihood dicts\n",
    "        theta_phi_estimates = soft_kmeans_theta_phi_estimates(\n",
    "            _,\n",
    "            prior_theta_phi_estimates,\n",
    "            data_we_have,\n",
    "            biomarkers,\n",
    "            current_order_dict,\n",
    "            n_participants,\n",
    "            non_diseased_participant_ids,\n",
    "            hashmap_of_normalized_stage_likelihood_dicts,\n",
    "            diseased_stages,\n",
    "            seed=None,\n",
    "        )\n",
    "\n",
    "        # Log-Sum-Exp Trick\n",
    "        max_likelihood = max(all_participant_ln_likelihood,\n",
    "                             current_accepted_likelihood)\n",
    "        prob_of_accepting_new_order = np.exp(\n",
    "            (all_participant_ln_likelihood - max_likelihood) -\n",
    "            (current_accepted_likelihood - max_likelihood)\n",
    "        )\n",
    "\n",
    "        # prob_of_accepting_new_order = np.exp(\n",
    "        #     all_participant_ln_likelihood - current_accepted_likelihood)\n",
    "\n",
    "        # np.exp(a)/np.exp(b) = np.exp(a - b)\n",
    "        # if a > b, then np.exp(a - b) > 1\n",
    "\n",
    "        # it will definitly update at the first iteration\n",
    "        if np.random.rand() < prob_of_accepting_new_order:\n",
    "            acceptance_count += 1\n",
    "            current_accepted_order = new_order\n",
    "            current_accepted_likelihood = all_participant_ln_likelihood\n",
    "            current_accepted_order_dict = current_order_dict\n",
    "\n",
    "        acceptance_ratio = acceptance_count*100/(_+1)\n",
    "        all_current_accepted_order_dicts.append(current_accepted_order_dict)\n",
    "\n",
    "        if (_+1) % 10 == 0:\n",
    "            formatted_string = (\n",
    "                f\"iteration {_ + 1} done, \"\n",
    "                f\"current accepted likelihood: {current_accepted_likelihood}, \"\n",
    "                f\"current acceptance ratio is {acceptance_ratio:.2f} %, \"\n",
    "                f\"current accepted order is {current_accepted_order_dict.values()}, \"\n",
    "            )\n",
    "            print(formatted_string)\n",
    "\n",
    "    # print(\"done!\")\n",
    "    return all_current_accepted_order_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/hongtaoh/Desktop/github/ebmBook\n",
      "Data directory: /Users/hongtaoh/Desktop/github/ebmBook/data\n",
      "Temp results directory: /Users/hongtaoh/Desktop/github/ebmBook/soft_kmeans/temp_json_results\n",
      "Image directory: /Users/hongtaoh/Desktop/github/ebmBook/soft_kmeans/img\n",
      "Processing with j=200, r=0.75, m=3\n",
      "Data file found: /Users/hongtaoh/Desktop/github/ebmBook/data/150|200_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wx/xz5y_06d15q5pgl_mhv76c8r0000gn/T/ipykernel_23669/1838063648.py:260: RuntimeWarning: overflow encountered in exp\n",
      "  prob_of_accepting_new_order = np.exp(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10 done, current accepted likelihood: -4550.744705628294, current acceptance ratio is 60.00 %, current accepted order is dict_values([6, 3, 9, 2, 7, 1, 5, 4, 8, 10]), \n",
      "150|200_3 is done! Results written to /Users/hongtaoh/Desktop/github/ebmBook/soft_kmeans/temp_json_results/temp_results_200_0.75_3.json\n"
     ]
    }
   ],
   "source": [
    "n_shuffle = 2\n",
    "iterations = 10\n",
    "burn_in = 2\n",
    "thining = 2\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "print(f\"Current working directory: {base_dir}\")\n",
    "data_dir = os.path.join(base_dir, \"data\")\n",
    "\n",
    "soft_kmeans_dir = os.path.join(base_dir, 'soft_kmeans')\n",
    "temp_results_dir = os.path.join(soft_kmeans_dir, \"temp_json_results\")\n",
    "img_dir = os.path.join(soft_kmeans_dir, 'img')\n",
    "results_file = os.path.join(soft_kmeans_dir, \"results.json\")\n",
    "\n",
    "os.makedirs(soft_kmeans_dir, exist_ok=True)\n",
    "os.makedirs(temp_results_dir, exist_ok=True)\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Temp results directory: {temp_results_dir}\")\n",
    "print(f\"Image directory: {img_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Read parameters from command line arguments\n",
    "    j = 200\n",
    "    r = 0.75\n",
    "    m = 3\n",
    "\n",
    "    print(f\"Processing with j={j}, r={r}, m={m}\")\n",
    "\n",
    "    combstr = f\"{int(j*r)}|{j}\"\n",
    "    heatmap_folder = img_dir\n",
    "    \n",
    "    img_filename = f\"{int(j*r)}-{j}_{m}\"\n",
    "    filename = f\"{combstr}_{m}\"\n",
    "    data_file = f\"{data_dir}/{filename}.csv\"\n",
    "    data_we_have = pd.read_csv(data_file)\n",
    "    n_biomarkers = len(data_we_have.biomarker.unique())\n",
    "\n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"Data file not found: {data_file}\")\n",
    "        sys.exit(1)  # Exit early if the file doesn't exist\n",
    "    else:\n",
    "        print(f\"Data file found: {data_file}\")\n",
    "\n",
    "    # Define the temporary result file\n",
    "    temp_result_file = os.path.join(temp_results_dir, f\"temp_results_{j}_{r}_{m}.json\")\n",
    "    \n",
    "    dic = {}\n",
    "\n",
    "    if combstr not in dic:\n",
    "        dic[combstr] = []\n",
    "\n",
    "    accepted_order_dicts = metropolis_hastings_soft_kmeans(\n",
    "        data_we_have,\n",
    "        iterations,\n",
    "        n_shuffle,\n",
    "    )\n",
    "\n",
    "    utils.save_heatmap(\n",
    "        accepted_order_dicts,\n",
    "        burn_in, \n",
    "        thining, \n",
    "        folder_name=heatmap_folder,\n",
    "        file_name=f\"{img_filename}\", \n",
    "        title=f'heatmap of {filename}')\n",
    "    \n",
    "    most_likely_order_dic = utils.obtain_most_likely_order_dic(\n",
    "        accepted_order_dicts, burn_in, thining)\n",
    "    most_likely_order = list(most_likely_order_dic.values())\n",
    "    tau, p_value = kendalltau(most_likely_order, range(1, n_biomarkers + 1))\n",
    "    \n",
    "    dic[combstr].append(tau)\n",
    "    \n",
    "    # Write the results to a unique temporary file inside the temp folder\n",
    "    with open(temp_result_file, \"w\") as file:\n",
    "        json.dump(dic, file, indent=4)\n",
    "    print(f\"{filename} is done! Results written to {temp_result_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "We plot the resulting $S$ probalistically using a heatmap. \n",
    "\n",
    "![Result of Soft K-Means](soft_kmeans/img/150-200_3.png){#fig-soft-kmeans-result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'150|200': [0.15555555555555553]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
