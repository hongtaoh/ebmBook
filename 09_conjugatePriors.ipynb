{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate $S$ with Conjugate Priors {#sec-estS-conjugate-priors}\n",
    "\n",
    "The basic idea of using conjugate Priors to estimate $S$ is:\n",
    "\n",
    "- We first estimate distribution parameters using contrained K-Means, the exact procedure we covered in @sec-cop-kmeans.\n",
    "- We first randomly assign each diseased participant a stage. Then, in each iteration, we use conjugate priors algorithm to update $\\theta, \\phi$ (refer to @sec-conjugate-priors) and also $k_j$. We use [Metropolisâ€“Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) to accept or reject a proposed $S$.\n",
    "\n",
    "![Conjugate Priors Algorithm](img/cp_algo.png){#fig-cp-algo}\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import utils \n",
    "import json \n",
    "import pandas as pd \n",
    "import utils \n",
    "from scipy.stats import kendalltau\n",
    "import sys\n",
    "import os\n",
    "import math \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def estimate_params_exact(m0, n0, s0_sq, v0, data):\n",
    "    '''This is to estimate means and vars based on conjugate priors\n",
    "    Inputs:\n",
    "        - data: a vector of measurements \n",
    "        - m0: prior estimate of $\\mu$.\n",
    "        - n0: how strongly is the prior belief in $m_0$ is held.\n",
    "        - s0_sq: prior estimate of $\\sigma^2$.\n",
    "        - v0: prior degress of freedome, influencing the certainty of $s_0^2$.\n",
    "\n",
    "    Outputs:\n",
    "        - mu estiate, std estimate\n",
    "    '''\n",
    "    # Data summary\n",
    "    sample_mean = np.mean(data)\n",
    "    sample_size = len(data)\n",
    "    sample_var = np.var(data, ddof=1)  # ddof=1 for unbiased estimator\n",
    "\n",
    "    # Update hyperparameters for the Normal-Inverse Gamma posterior\n",
    "    updated_m0 = (n0 * m0 + sample_size * sample_mean) / (n0 + sample_size)\n",
    "    updated_n0 = n0 + sample_size\n",
    "    updated_v0 = v0 + sample_size\n",
    "    updated_s0_sq = (1 / updated_v0) * ((sample_size - 1) * sample_var + v0 * s0_sq +\n",
    "                                        (n0 * sample_size / updated_n0) * (sample_mean - m0)**2)\n",
    "    updated_alpha = updated_v0/2\n",
    "    updated_beta = updated_v0*updated_s0_sq/2\n",
    "\n",
    "    # Posterior estimates\n",
    "    mu_posterior_mean = updated_m0\n",
    "    sigma_squared_posterior_mean = updated_beta/updated_alpha\n",
    "\n",
    "    mu_estimation = mu_posterior_mean\n",
    "    std_estimation = np.sqrt(sigma_squared_posterior_mean)\n",
    "\n",
    "    return mu_estimation, std_estimation\n",
    "\n",
    "def get_theta_phi_conjugate_priors(biomarkers, data_we_have, theta_phi_kmeans):\n",
    "    '''To get estimated parameters, returns a hashmap\n",
    "    Input:\n",
    "    - biomarkers: biomarkers \n",
    "    - data_we_have: participants data filled with initial or updated participant_stages\n",
    "    - theta_phi_kmeans: a hashmap of dicts, which are the prior theta and phi values\n",
    "        obtained from the initial constrained kmeans algorithm\n",
    "\n",
    "    Output: \n",
    "    - a hashmap of dictionaries. Key is biomarker name and value is a dictionary.\n",
    "    Each dictionary contains the theta and phi mean/std values for a specific biomarker. \n",
    "    '''\n",
    "    # empty list of dictionaries to store the estimates\n",
    "    hashmap_of_means_stds_estimate_dicts = {}\n",
    "\n",
    "    for biomarker in biomarkers:\n",
    "        # Initialize dictionary outside the inner loop\n",
    "        dic = {'biomarker': biomarker}\n",
    "        for affected in [True, False]:\n",
    "            data_full = data_we_have[(data_we_have.biomarker == biomarker) & (\n",
    "                data_we_have.affected == affected)]\n",
    "            if len(data_full) > 1:\n",
    "                measurements = data_full.measurement\n",
    "                s0_sq = np.var(measurements, ddof=1)\n",
    "                m0 = np.mean(measurements)\n",
    "                mu_estimate, std_estimate = estimate_params_exact(\n",
    "                    m0=m0, n0=1, s0_sq=s0_sq, v0=1, data=measurements)\n",
    "                if affected:\n",
    "                    dic['theta_mean'] = mu_estimate\n",
    "                    dic['theta_std'] = std_estimate\n",
    "                else:\n",
    "                    dic['phi_mean'] = mu_estimate\n",
    "                    dic['phi_std'] = std_estimate\n",
    "            # If there is only one observation or not observation at all, resort to theta_phi_kmeans\n",
    "            # YES, IT IS POSSIBLE THAT DATA_FULL HERE IS NULL\n",
    "            # For example, if a biomarker indicates stage of (num_biomarkers), but all participants' stages\n",
    "            # are smaller than that stage; so that for all participants, this biomarker is not affected\n",
    "            else:\n",
    "                # print(theta_phi_kmeans)\n",
    "                if affected:\n",
    "                    dic['theta_mean'] = theta_phi_kmeans[biomarker]['theta_mean']\n",
    "                    dic['theta_std'] = theta_phi_kmeans[biomarker]['theta_std']\n",
    "                else:\n",
    "                    dic['phi_mean'] = theta_phi_kmeans[biomarker]['phi_mean']\n",
    "                    dic['phi_std'] = theta_phi_kmeans[biomarker]['phi_std']\n",
    "        # print(f\"biomarker {biomarker} done!\")\n",
    "        hashmap_of_means_stds_estimate_dicts[biomarker] = dic\n",
    "    return hashmap_of_means_stds_estimate_dicts\n",
    "\n",
    "def compute_all_participant_ln_likelihood_and_update_participant_stages(\n",
    "        n_participants,\n",
    "        data,\n",
    "        non_diseased_participant_ids,\n",
    "        estimated_theta_phi,\n",
    "        disease_stages,\n",
    "        participant_stages,\n",
    "):\n",
    "    all_participant_ln_likelihood = 0\n",
    "    for p in range(n_participants):\n",
    "        # this participant data\n",
    "        pdata = data[data.participant == p].reset_index(drop=True)\n",
    "\n",
    "        \"\"\"If this participant is not diseased (i.e., if we know k_j is equal to 0)\n",
    "        We still need to compute the likelihood of this participant seeing this sequence of biomarker data\n",
    "        but we do not need to estimate k_j like below\n",
    "\n",
    "        We still need to compute the likelihood because we need to add it to all_participant_ln_likelihood\n",
    "        \"\"\"\n",
    "        if p in non_diseased_participant_ids:\n",
    "            this_participant_likelihood = utils.compute_likelihood(\n",
    "                pdata, k_j=0, theta_phi=estimated_theta_phi)\n",
    "            this_participant_ln_likelihood = np.log(\n",
    "                this_participant_likelihood + 1e-10)\n",
    "        else:\n",
    "            # initiaze stage_likelihood\n",
    "            stage_likelihood_dict = {}\n",
    "            for k_j in disease_stages:\n",
    "                # even though data above has everything, it is filled up by random stages\n",
    "                # we don't like it and want to know the true k_j. All the following is to update participant_stages\n",
    "                participant_likelihood = utils.compute_likelihood(\n",
    "                    pdata, k_j, estimated_theta_phi)\n",
    "                # update each stage likelihood for this participant\n",
    "                stage_likelihood_dict[k_j] = participant_likelihood\n",
    "            likelihood_sum = sum(stage_likelihood_dict.values())\n",
    "            normalized_stage_likelihood = [\n",
    "                l/likelihood_sum for l in stage_likelihood_dict.values()]\n",
    "            sampled_stage = np.random.choice(\n",
    "                disease_stages, p=normalized_stage_likelihood)\n",
    "            participant_stages[p] = sampled_stage\n",
    "\n",
    "            # use weighted average likelihood because we didn't know the exact participant stage\n",
    "            # all above to calculate participant_stage is only for the purpous of calculate theta_phi\n",
    "            this_participant_likelihood = np.mean(likelihood_sum)\n",
    "            this_participant_ln_likelihood = np.log(\n",
    "                this_participant_likelihood + 1e-10)\n",
    "        \"\"\"\n",
    "        All the codes in between are calculating this_participant_ln_likelihood. \n",
    "        If we already know kj=0, then\n",
    "        it's very simple. If kj is unknown, we need to calculate the likelihood of seeing \n",
    "        this sequence of biomarker\n",
    "        data at different stages, and get the relative likelihood before \n",
    "        we get a sampled stage (this is for estimating theta and phi). \n",
    "        Then we calculate this_participant_ln_likelihood using average likelihood. \n",
    "        \"\"\"\n",
    "        all_participant_ln_likelihood += this_participant_ln_likelihood\n",
    "    return all_participant_ln_likelihood\n",
    "\n",
    "def update_data_by_the_new_participant_stages(data, participant_stages, n_participants):\n",
    "    '''This is to fill up data_we_have. \n",
    "    Basically, add two columns: k_j, affected, and modify diseased column\n",
    "    based on the initial or updated participant_stages\n",
    "    Note that we assume here we've already got S_n\n",
    "\n",
    "    Inputs:\n",
    "        - data_we_have\n",
    "        - participant_stages: np array \n",
    "        - participants: 0-99\n",
    "    '''\n",
    "    participant_stage_dic = dict(\n",
    "        zip(np.arange(0, n_participants), participant_stages))\n",
    "    data['k_j'] = data.apply(\n",
    "        lambda row: participant_stage_dic[row.participant], axis=1)\n",
    "    data['diseased'] = data.apply(lambda row: row.k_j > 0, axis=1)\n",
    "    data['affected'] = data.apply(lambda row: row.k_j >= row.S_n, axis=1)\n",
    "    return data\n",
    "\n",
    "\"\"\"The version without reverting back to the max order\n",
    "\"\"\"\n",
    "def metropolis_hastings_with_conjugate_priors(\n",
    "    data_we_have,\n",
    "    iterations,\n",
    "    n_shuffle\n",
    "):\n",
    "    n_participants = len(data_we_have.participant.unique())\n",
    "    biomarkers = data_we_have.biomarker.unique()\n",
    "    n_biomarkers = len(biomarkers)\n",
    "    n_stages = n_biomarkers + 1\n",
    "    diseased_stages = np.arange(start=1, stop=n_stages, step=1)\n",
    "\n",
    "    non_diseased_participant_ids = data_we_have.loc[\n",
    "        data_we_have.diseased == False].participant.unique()\n",
    "\n",
    "    # initialize empty lists\n",
    "    acceptance_count = 0\n",
    "    all_current_accepted_order_dicts = []\n",
    "\n",
    "    # initialize an ordering and likelihood\n",
    "    # note that it should be a random permutation of numbers 1-10\n",
    "    current_accepted_order = np.random.permutation(np.arange(1, n_stages))\n",
    "    current_accepted_order_dict = dict(zip(biomarkers, current_accepted_order))\n",
    "    current_accepted_likelihood = -np.inf\n",
    "\n",
    "    participant_stages = np.zeros(n_participants)\n",
    "    for idx in range(n_participants):\n",
    "        if idx not in non_diseased_participant_ids:\n",
    "            # 1-len(diseased_stages), inclusive on both ends\n",
    "            participant_stages[idx] = random.randint(1, len(diseased_stages))\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        new_order = current_accepted_order.copy()\n",
    "        utils.shuffle_order(new_order, n_shuffle)\n",
    "        current_order_dict = dict(zip(biomarkers, new_order))\n",
    "\n",
    "        # copy the data to avoid modifying the original\n",
    "        data = data_we_have.copy()\n",
    "        data['S_n'] = data.apply(\n",
    "            lambda row: current_order_dict[row['biomarker']], axis=1)\n",
    "        # add kj and affected for the whole dataset based on participant_stages\n",
    "        # also modify diseased col (because it will be useful for the new theta_phi_kmeans)\n",
    "        data = update_data_by_the_new_participant_stages(\n",
    "            data, participant_stages, n_participants)\n",
    "        # should be inside the for loop because once the participant_stages change, \n",
    "        # the diseased column changes as well. \n",
    "        theta_phi_kmeans = utils.get_theta_phi_estimates(\n",
    "            data_we_have,\n",
    "        )\n",
    "        estimated_theta_phi = get_theta_phi_conjugate_priors(\n",
    "            biomarkers, data, theta_phi_kmeans)\n",
    "\n",
    "        all_participant_ln_likelihood = compute_all_participant_ln_likelihood_and_update_participant_stages(\n",
    "            n_participants,\n",
    "            data,\n",
    "            non_diseased_participant_ids,\n",
    "            estimated_theta_phi,\n",
    "            diseased_stages,\n",
    "            participant_stages,\n",
    "        )\n",
    "\n",
    "        # ratio = likelihood/best_likelihood\n",
    "        # because we are using np.log(likelihood) and np.log(best_likelihood)\n",
    "        # np.exp(a)/np.exp(b) = np.exp(a - b)\n",
    "        # if a > b, then np.exp(a - b) > 1\n",
    "\n",
    "        # Log-Sum-Exp Trick\n",
    "        max_likelihood = max(all_participant_ln_likelihood,\n",
    "                             current_accepted_likelihood)\n",
    "        prob_of_accepting_new_order = np.exp(\n",
    "            (all_participant_ln_likelihood - max_likelihood) -\n",
    "            (current_accepted_likelihood - max_likelihood)\n",
    "        )\n",
    "        \n",
    "        # it will definitly update at the first iteration\n",
    "        if np.random.rand() < prob_of_accepting_new_order:\n",
    "            acceptance_count += 1\n",
    "            current_accepted_order = new_order\n",
    "            current_accepted_likelihood = all_participant_ln_likelihood\n",
    "            current_accepted_order_dict = current_order_dict\n",
    "\n",
    "        acceptance_ratio = acceptance_count*100/(_+1)\n",
    "        all_current_accepted_order_dicts.append(current_accepted_order_dict)\n",
    "\n",
    "        # if _ >= burn_in and _ % thining == 0:\n",
    "        if (_+1) % 10 == 0:\n",
    "            formatted_string = (\n",
    "                f\"iteration {_ + 1} done, \"\n",
    "                f\"current accepted likelihood: {current_accepted_likelihood}, \"\n",
    "                f\"current acceptance ratio is {acceptance_ratio:.2f} %, \"\n",
    "                f\"current accepted order is {current_accepted_order_dict.values()}, \"\n",
    "            )\n",
    "            \n",
    "    return all_current_accepted_order_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/hongtaoh/Desktop/github/ebmBook\n",
      "Data directory: /Users/hongtaoh/Desktop/github/ebmBook/data\n",
      "Temp results directory: /Users/hongtaoh/Desktop/github/ebmBook/conjugate_priors/temp_json_results\n",
      "Image directory: /Users/hongtaoh/Desktop/github/ebmBook/conjugate_priors/img\n",
      "Processing with j=200, r=0.75, m=3\n",
      "Data file found: /Users/hongtaoh/Desktop/github/ebmBook/data/150|200_3.csv\n",
      "150|200_3 is done! Results written to /Users/hongtaoh/Desktop/github/ebmBook/conjugate_priors/temp_json_results/temp_results_200_0.75_3.json\n"
     ]
    }
   ],
   "source": [
    "n_shuffle = 2\n",
    "iterations = 10\n",
    "burn_in = 2\n",
    "thining = 2\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "print(f\"Current working directory: {base_dir}\")\n",
    "data_dir = os.path.join(base_dir, \"data\")\n",
    "conjugate_priors_dir = os.path.join(base_dir, 'conjugate_priors')\n",
    "temp_results_dir = os.path.join(conjugate_priors_dir, \"temp_json_results\")\n",
    "img_dir = os.path.join(conjugate_priors_dir, 'img')\n",
    "results_file = os.path.join(conjugate_priors_dir, \"results.json\")\n",
    "\n",
    "os.makedirs(conjugate_priors_dir, exist_ok=True)\n",
    "os.makedirs(temp_results_dir, exist_ok=True)\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Temp results directory: {temp_results_dir}\")\n",
    "print(f\"Image directory: {img_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Read parameters from command line arguments\n",
    "    j = 200\n",
    "    r = 0.75\n",
    "    m = 3\n",
    "\n",
    "    print(f\"Processing with j={j}, r={r}, m={m}\")\n",
    "\n",
    "    combstr = f\"{int(j*r)}|{j}\"\n",
    "    heatmap_folder = img_dir\n",
    "    \n",
    "    img_filename = f\"{int(j*r)}-{j}_{m}\"\n",
    "    filename = f\"{combstr}_{m}\"\n",
    "    data_file = f\"{data_dir}/{filename}.csv\"\n",
    "    data_we_have = pd.read_csv(data_file)\n",
    "    n_biomarkers = len(data_we_have.biomarker.unique())\n",
    "\n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"Data file not found: {data_file}\")\n",
    "        sys.exit(1)  # Exit early if the file doesn't exist\n",
    "    else:\n",
    "        print(f\"Data file found: {data_file}\")\n",
    "\n",
    "    # Define the temporary result file\n",
    "    temp_result_file = os.path.join(temp_results_dir, f\"temp_results_{j}_{r}_{m}.json\")\n",
    "\n",
    "    # temp_result_file = f\"{temp_results_dir}/temp_results_{j}_{r}_{m}.json\"\n",
    "    \n",
    "    dic = {}\n",
    "\n",
    "    if combstr not in dic:\n",
    "        dic[combstr] = []\n",
    "\n",
    "    accepted_order_dicts = metropolis_hastings_with_conjugate_priors(\n",
    "        data_we_have,\n",
    "        iterations,\n",
    "        n_shuffle,\n",
    "    )\n",
    "\n",
    "    utils.save_heatmap(\n",
    "        accepted_order_dicts,\n",
    "        burn_in, \n",
    "        thining, \n",
    "        folder_name=heatmap_folder,\n",
    "        file_name=f\"{img_filename}\", \n",
    "        title=f'heatmap of {filename}')\n",
    "    \n",
    "    most_likely_order_dic = utils.obtain_most_likely_order_dic(\n",
    "        accepted_order_dicts, burn_in, thining)\n",
    "    most_likely_order = list(most_likely_order_dic.values())\n",
    "    tau, p_value = kendalltau(most_likely_order, range(1, n_biomarkers + 1))\n",
    "    \n",
    "    dic[combstr].append(tau)\n",
    "    \n",
    "    # Write the results to a unique temporary file inside the temp folder\n",
    "    with open(temp_result_file, \"w\") as file:\n",
    "        json.dump(dic, file, indent=4)\n",
    "    print(f\"{filename} is done! Results written to {temp_result_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "We plot the resulting $S$ probalistically using a heatmap. \n",
    "\n",
    "![Result of Conjugate Priors](conjugate_priors/img/150-200_3.png){#fig-conjugate_priors-result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'150|200': [0.28888888888888886]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
