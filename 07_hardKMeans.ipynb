{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate $S$ with Hard KMeans {#sec-estS-hard-kmeans}\n",
    "\n",
    "The basic idea of using hard K Means to estimate $S$ is:\n",
    "\n",
    "- We first estimate distribution parameters using hard K Means, the exact procedure we covered in @sec-hard-kmeans.\n",
    "- We use [Metropolisâ€“Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) to accept or reject a proposed $S$.\n",
    "\n",
    "![Hard K-Means Algorithm](img/hard_kmeans_algo.png){#fig-hard-kmeans}\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import utils \n",
    "import json \n",
    "import pandas as pd \n",
    "import utils \n",
    "from scipy.stats import kendalltau\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def calculate_all_participant_ln_likelihood(\n",
    "        iteration,\n",
    "        data_we_have,\n",
    "        current_order_dict,\n",
    "        n_participants,\n",
    "        non_diseased_participant_ids,\n",
    "        theta_phi_estimates,\n",
    "        diseased_stages,\n",
    "):\n",
    "    data = data_we_have.copy()\n",
    "    data['S_n'] = data.apply(\n",
    "        lambda row: current_order_dict[row['biomarker']], axis=1)\n",
    "    all_participant_ln_likelihood = 0\n",
    "\n",
    "    for p in range(n_participants):\n",
    "        pdata = data[data.participant == p].reset_index(drop=True)\n",
    "        if p in non_diseased_participant_ids:\n",
    "            this_participant_likelihood = utils.compute_likelihood(\n",
    "                pdata, k_j=0, theta_phi=theta_phi_estimates)\n",
    "            this_participant_ln_likelihood = np.log(\n",
    "                this_participant_likelihood + 1e-10)\n",
    "        else:\n",
    "            # normalized_stage_likelihood_dict = None\n",
    "            # initiaze stage_likelihood\n",
    "            stage_likelihood_dict = {}\n",
    "            for k_j in diseased_stages:\n",
    "                kj_likelihood = utils.compute_likelihood(\n",
    "                    pdata, k_j, theta_phi_estimates)\n",
    "                # update each stage likelihood for this participant\n",
    "                stage_likelihood_dict[k_j] = kj_likelihood\n",
    "            # Add a small epsilon to avoid division by zero\n",
    "            likelihood_sum = sum(stage_likelihood_dict.values())\n",
    "\n",
    "            # calculate weighted average\n",
    "            this_participant_likelihood = np.mean(likelihood_sum)\n",
    "            this_participant_ln_likelihood = np.log(\n",
    "                this_participant_likelihood + 1e-10)\n",
    "        all_participant_ln_likelihood += this_participant_ln_likelihood\n",
    "    return all_participant_ln_likelihood\n",
    "\n",
    "def metropolis_hastings_hard_kmeans(\n",
    "    data_we_have,\n",
    "    iterations,\n",
    "    n_shuffle,\n",
    "):\n",
    "    '''Implement the metropolis-hastings algorithm using simple clustering\n",
    "    Inputs: \n",
    "        - data: data_we_have\n",
    "        - iterations: number of iterations\n",
    "        - log_folder_name: the folder where log files locate\n",
    "\n",
    "    Outputs:\n",
    "        - best_order: a numpy array\n",
    "        - best_likelihood: a scalar \n",
    "    '''\n",
    "    n_participants = len(data_we_have.participant.unique())\n",
    "    biomarkers = data_we_have.biomarker.unique()\n",
    "    n_biomarkers = len(biomarkers)\n",
    "    n_stages = n_biomarkers + 1\n",
    "    non_diseased_participant_ids = data_we_have.loc[\n",
    "        data_we_have.diseased == False].participant.unique()\n",
    "    diseased_stages = np.arange(start=1, stop=n_stages, step=1)\n",
    "    # obtain the iniial theta and phi estimates\n",
    "    theta_phi_estimates = utils.get_theta_phi_estimates(\n",
    "        data_we_have)\n",
    "\n",
    "    # initialize empty lists\n",
    "    acceptance_count = 0\n",
    "    all_current_accepted_order_dicts = []\n",
    "\n",
    "    current_accepted_order = np.random.permutation(np.arange(1, n_stages))\n",
    "    current_accepted_order_dict = dict(zip(biomarkers, current_accepted_order))\n",
    "    current_accepted_likelihood = -np.inf\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # in each iteration, we have updated current_order_dict and theta_phi_estimates\n",
    "\n",
    "        new_order = current_accepted_order.copy()\n",
    "        utils.shuffle_order(new_order, n_shuffle)\n",
    "        current_order_dict = dict(zip(biomarkers, new_order))\n",
    "        all_participant_ln_likelihood = calculate_all_participant_ln_likelihood(\n",
    "                _,\n",
    "                data_we_have,\n",
    "                current_order_dict,\n",
    "                n_participants,\n",
    "                non_diseased_participant_ids,\n",
    "                theta_phi_estimates,\n",
    "                diseased_stages,\n",
    "            )\n",
    "\n",
    "\n",
    "        # Log-Sum-Exp Trick\n",
    "        max_likelihood = max(all_participant_ln_likelihood,\n",
    "                             current_accepted_likelihood)\n",
    "        prob_of_accepting_new_order = np.exp(\n",
    "            (all_participant_ln_likelihood - max_likelihood) -\n",
    "            (current_accepted_likelihood - max_likelihood)\n",
    "        )\n",
    "\n",
    "        # prob_of_accepting_new_order = np.exp(\n",
    "        #     all_participant_ln_likelihood - current_accepted_likelihood)\n",
    "\n",
    "        # np.exp(a)/np.exp(b) = np.exp(a - b)\n",
    "        # if a > b, then np.exp(a - b) > 1\n",
    "\n",
    "        # it will definitly update at the first iteration\n",
    "        if np.random.rand() < prob_of_accepting_new_order:\n",
    "            acceptance_count += 1\n",
    "            current_accepted_order = new_order\n",
    "            current_accepted_likelihood = all_participant_ln_likelihood\n",
    "            current_accepted_order_dict = current_order_dict\n",
    "\n",
    "        acceptance_ratio = acceptance_count*100/(_+1)\n",
    "        all_current_accepted_order_dicts.append(current_accepted_order_dict)\n",
    "\n",
    "        if (_+1) % 10 == 0:\n",
    "            formatted_string = (\n",
    "                f\"iteration {_ + 1} done, \"\n",
    "                f\"current accepted likelihood: {current_accepted_likelihood}, \"\n",
    "                f\"current acceptance ratio is {acceptance_ratio:.2f} %, \"\n",
    "                f\"current accepted order is {current_accepted_order_dict.values()}, \"\n",
    "            )\n",
    "            print(formatted_string)\n",
    "\n",
    "    # print(\"done!\")\n",
    "    return all_current_accepted_order_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/hongtaoh/Desktop/github/ebmBook\n",
      "Data directory: /Users/hongtaoh/Desktop/github/ebmBook/data\n",
      "Temp results directory: /Users/hongtaoh/Desktop/github/ebmBook/hard_kmeans/temp_json_results\n",
      "Image directory: /Users/hongtaoh/Desktop/github/ebmBook/hard_kmeans/img\n",
      "Processing with j=200, r=0.75, m=3\n",
      "Data file found: /Users/hongtaoh/Desktop/github/ebmBook/data/150|200_3.csv\n",
      "iteration 10 done, current accepted likelihood: -4491.553963056519, current acceptance ratio is 70.00 %, current accepted order is dict_values([1, 5, 2, 3, 9, 8, 10, 6, 4, 7]), \n",
      "150|200_3 is done! Results written to /Users/hongtaoh/Desktop/github/ebmBook/hard_kmeans/temp_json_results/temp_results_200_0.75_3.json\n"
     ]
    }
   ],
   "source": [
    "n_shuffle = 2\n",
    "iterations = 10\n",
    "burn_in = 2\n",
    "thining = 2\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "print(f\"Current working directory: {base_dir}\")\n",
    "data_dir = os.path.join(base_dir, \"data\")\n",
    "\n",
    "cop_kmeans_dir = os.path.join(base_dir, 'hard_kmeans')\n",
    "temp_results_dir = os.path.join(cop_kmeans_dir, \"temp_json_results\")\n",
    "img_dir = os.path.join(cop_kmeans_dir, 'img')\n",
    "results_file = os.path.join(cop_kmeans_dir, \"results.json\")\n",
    "\n",
    "os.makedirs(cop_kmeans_dir, exist_ok=True)\n",
    "os.makedirs(temp_results_dir, exist_ok=True)\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Temp results directory: {temp_results_dir}\")\n",
    "print(f\"Image directory: {img_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Read parameters from command line arguments\n",
    "    j = 200\n",
    "    r = 0.75\n",
    "    m = 3\n",
    "\n",
    "    print(f\"Processing with j={j}, r={r}, m={m}\")\n",
    "\n",
    "    combstr = f\"{int(j*r)}|{j}\"\n",
    "    heatmap_folder = img_dir\n",
    "    \n",
    "    img_filename = f\"{int(j*r)}-{j}_{m}\"\n",
    "    filename = f\"{combstr}_{m}\"\n",
    "    data_file = f\"{data_dir}/{filename}.csv\"\n",
    "    data_we_have = pd.read_csv(data_file)\n",
    "    n_biomarkers = len(data_we_have.biomarker.unique())\n",
    "\n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"Data file not found: {data_file}\")\n",
    "        sys.exit(1)  # Exit early if the file doesn't exist\n",
    "    else:\n",
    "        print(f\"Data file found: {data_file}\")\n",
    "\n",
    "    # Define the temporary result file\n",
    "    temp_result_file = os.path.join(temp_results_dir, f\"temp_results_{j}_{r}_{m}.json\")\n",
    "    \n",
    "    dic = {}\n",
    "\n",
    "    if combstr not in dic:\n",
    "        dic[combstr] = []\n",
    "\n",
    "    accepted_order_dicts = metropolis_hastings_hard_kmeans(\n",
    "        data_we_have,\n",
    "        iterations,\n",
    "        n_shuffle,\n",
    "    )\n",
    "\n",
    "    utils.save_heatmap(\n",
    "        accepted_order_dicts,\n",
    "        burn_in, \n",
    "        thining, \n",
    "        folder_name=heatmap_folder,\n",
    "        file_name=f\"{img_filename}\", \n",
    "        title=f'heatmap of {filename}')\n",
    "    \n",
    "    most_likely_order_dic = utils.obtain_most_likely_order_dic(\n",
    "        accepted_order_dicts, burn_in, thining)\n",
    "    most_likely_order = list(most_likely_order_dic.values())\n",
    "    tau, p_value = kendalltau(most_likely_order, range(1, n_biomarkers + 1))\n",
    "    \n",
    "    dic[combstr].append(tau)\n",
    "    \n",
    "    # Write the results to a unique temporary file inside the temp folder\n",
    "    with open(temp_result_file, \"w\") as file:\n",
    "        json.dump(dic, file, indent=4)\n",
    "    print(f\"{filename} is done! Results written to {temp_result_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "We plot the resulting $S$ probalistically using a heatmap. We also quantify the difference between our result with the real $S$ using [Kendall's Tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient). It ranges from $-1$ (completely different) to $1$ (exactly the same). $0$ indicate complete randomness. \n",
    "\n",
    "![Result of Hard K-Means](hard_kmeans/img/150-200_3.png){#fig-hard-kmeans-result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'150|200': [0.3333333333333333]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
