[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Event Based Models for Disease Progression",
    "section": "",
    "text": "1 Introduction\nEvent-based Model (EBM) is used to model the order of degenerative diseases. In another word, we want to estimate the order in which different biological factors (“biomarkers”) get affected by a specific disease. The order is categorized by multiple stages in the disease progression.\nFor instance, Alzheimer may have the following stages:\n\n\n\n\n\n\nFigure 1.1: Alzheimer Disease Progression (Credit: https://preventad.com/alzheimers-disease/)\n\n\n\nWe estimate this order based on the biomarker data from patients’ visits. These data are typically results of neuropsych (e.g., MMSE) and/or biological examiations (e.g., blood pressure). Visits data can be longitudinal and/or cross-sectional, i.e., single visits from a cohort of patients.\nKnowing the disease progression is important because it helps prevent and hopefully cure the disease. It also helps health professionals prepare for the disease’s further development.\nWe have several assumptions in EBM:\n\n\n\n\n\n\nFigure 1.2: Assumptions of EBM\n\n\n\n\nThe disease is irreversible (i.e., a patient cannot go from stage 2 to stage 1)\nThe order in which different biomarkers get affected by the disease is the same across all patients.\nBiomarker data can be approximated by a Gaussian distribution.\n\nThis book contains chapters that explain step by step how we use the event-based model to estimate the order of disease progression based on cross-sectional patients’ biomarker data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "genModel.html",
    "href": "genModel.html",
    "title": "2  EBM as A Generative Model",
    "section": "",
    "text": "2.1 Generative Process\nThe generative process of biomarker measurements can be described as:\n\\[\nX_{nj} \\mid S, k_j, \\theta_n, \\phi_n \\sim I(z_j = 1) \\bigg[ I(S(n) \\leq k_j) \\, p(X_{nj} \\mid \\theta_{n}) + I(S(n) &gt; k_j) \\, p(X_{nj} \\mid \\phi_{n}) \\bigg] \\\\\n+ \\left(1 - I(z_j = 1) \\right) p(X_{nj} \\mid \\phi_{n})\n\\tag{2.1}\\]\nThis model says that given that we know \\(S, k_j, \\theta_n, \\text{and } \\phi_n\\), we can draw the biomarker measurement from a distribution.\n\\(S \\sim \\mathrm{UniformPermutation}(\\cdot)\\)\n\\(S\\) follows a distribution of uniform permutation. That means the ordering of biomarkers is random.\n\\(k_j \\sim \\mathrm{DiscreteUniform}(N)\\)\n\\(k_j\\) follows a discrete uniform distribution, which means a participant is equally likely to fall in a progression stage (e.g., from \\(0\\) to \\(5\\), where \\(0\\) indicate this participant is healthy.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EBM as A Generative Model</span>"
    ]
  },
  {
    "objectID": "genModel.html#graphical-explanation",
    "href": "genModel.html#graphical-explanation",
    "title": "2  EBM as A Generative Model",
    "section": "2.2 Graphical Explanation",
    "text": "2.2 Graphical Explanation\nIn the following, we explain the generative model in three different scenarios using graphical models: (1) All participants are healthy; (2) Both healthy and diseased participants, but all biomarkers are affected among diseased people; (3) Both healthy and diseased participants, but we do not whether biomarkers are affected or not among patients.\n\n2.2.1 Scenario 1\nIf all participants are healthy:\n\\[\nX_{nj} \\sim p(X_{nj} \\mid \\phi_{n})\n\\tag{2.2}\\]\nWhere\n\\(X_{nj}\\) indicates the measurement of biomarker \\(n\\) in participant \\(j\\).\n\\(\\phi_{n}\\) represents \\(\\mathcal N(\\phi_{\\mu}, \\phi_{\\sigma})\\) for biomarker \\(n\\).\nThe graphical model would look like:\n\n\n\n\n\n\nFigure 2.2: Graphical Model of Scenario 1\n\n\n\n\n\n2.2.2 Scenario 2\nIf we have oth diseased and healthy participants, and all biomarkers are affected among diseased participants.\n\\[\nX_{nj} \\sim I(z_j == 1) p(X_{nj} \\mid \\theta_n) + (1-I(z_j == 1))p(X_{nj} \\mid \\phi_n)\n\\tag{2.3}\\]\nWhere:\n\\(z_j = 1\\) indicates this participant is diseased and \\(z_j = 1\\) represents a healthy participant.\n\\(I(True) = 1\\) and \\(I(False) = 0\\).\n\\(\\theta_{n}\\) represents \\(\\mathcal N(\\theta_{\\mu}, \\theta_{\\sigma})\\) for biomarker \\(n\\).\nThe graphical model would look like:\n\n\n\n\n\n\nFigure 2.3: Graphical Model of Scenario 2\n\n\n\n\n\n2.2.3 Scenario 3\nIf we have both healthy and diseased participants, but we do not whether biomarkers are affected or not among patients, see Equation 2.1.\nThis is the model in usual cases.\nThe graphical model looks like:\n\n\n\n\n\n\nFigure 2.4: Graphical Model of Scenario 3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EBM as A Generative Model</span>"
    ]
  },
  {
    "objectID": "calcLikelihood.html",
    "href": "calcLikelihood.html",
    "title": "3  Calculating the Likelihood of Biomarker Measurements",
    "section": "",
    "text": "3.1 Known \\(k_j\\)\n\\[\np(X_{j} | S, z_j = 1, k_j) = \\underbrace{\\prod_{i=1}^{k_j}{p(X_{S(i)j} \\mid \\theta_{S(i)} )}}_{\\text{Affected biomarker likelihood}} \\,\n\\underbrace{\\prod_{i=k_j+1}^N{p(X_{S(i)j} \\mid \\phi_{S(i)})}}_{\\text{Non-affected biomarker likelihood}}\n\\tag{3.1}\\]\nThis equation compuates the likelihood of the observed biomarker data of a specific participant, given that we know the disease stage this patient is at (\\(k_j\\)).\nWe assume that the patient is at stage \\(2\\) of this disease; hence \\(k_j = 2\\).\nNext, we are going to calculate \\(p(X_j|S, z_j = 1, k_j)\\):\nWhen \\(i = 1\\), we have \\(S_{(i)} = b\\) and \\(X_{S_{(i)}} = X_b = 45\\). So\n\\[p(X_{S_{(i)}} | \\theta_{S(i)}) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{X_b - \\mu}{\\sigma} \\right)^2}\\]\nBecause \\(k_j = 2\\), so biomarker \\(b\\) and \\(a\\) are affected. We should use the distribution of \\(\\theta_b\\); therefore, we should plug in \\(\\mu = 45, \\sigma = 5\\) in the above equation.\nWe can do the same for \\(i\\) = 2, 3, and 4.\nSo\n\\[p(X_j | S, k_j = 2) = p (X_b | \\theta_b) \\times p (X_a | \\theta_a) \\times p (X_d | \\phi_d) \\times p (X_c | \\phi_c)\\]\nThe above is the likelihood of the given biomarker data when \\(k_j = 2\\).\nNote that \\(p (X_b | \\theta_b)\\) is probability density, a value of a probability density function at a specific point; so it is not a probability itself.\nMultiplying multiple probability densities will give us a likelihood.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Calculating the Likelihood of Biomarker Measurements</span>"
    ]
  },
  {
    "objectID": "calcLikelihood.html#known-k_j",
    "href": "calcLikelihood.html#known-k_j",
    "title": "3  Calculating the Likelihood of Biomarker Measurements",
    "section": "",
    "text": "\\(S\\) is an orded array of biomarkers that are affected by the disease, for example, \\([b, a, d, c]\\). This means that biomarker \\(b\\) is affected at stage 1. At stage 2, biomarker \\(b\\) and \\(a\\) will be affected.\n\\(S(i)\\) is the \\(i^{th}\\) biomarker according to \\(S\\). For example \\(S_1\\) will be biomarker \\(b\\).\n\\(k_j\\) indicates the stage the patient is at, for example, \\(k_j = 2\\). This means that the disease has effected biomarker \\(a\\) and \\(b\\). Biomarker \\(c\\) and \\(d\\) have not been affected yet.\n\\(\\theta_{S(i)}\\) is the parameters for the probability density function (PDF) of observed value of biomarker \\(S(i)\\) when this biomarker has been affected by the disease. Let’s assume this distribution is a Gaussian distribution with means of \\([45, 50, 55, 60]\\) and a standard deviation of \\(5\\) for biomarker \\(b\\), \\(a\\), \\(d\\), and \\(c\\).\n\\(\\phi_{S(i)}\\) is the parameters for the probability density function (PDF) of observed value of biomarker \\(S(i)\\) when this biomarker has NOT been affected by the disease. Let’s assume this distribution is a Gaussian distribution with means of \\([25, 30, 35, 40]\\) and a standard deviation of \\(3\\) for biomarker \\(b\\), \\(a\\), \\(d\\), and \\(c\\).\n\\(X_j\\) is an array representing the patient’s observed data for all biomarker. Assume the data is \\([77, 45, 53, 90]\\) for biomarker \\(b\\), \\(a\\), \\(d\\), and \\(c\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Calculating the Likelihood of Biomarker Measurements</span>"
    ]
  },
  {
    "objectID": "calcLikelihood.html#unknown-k_j",
    "href": "calcLikelihood.html#unknown-k_j",
    "title": "3  Calculating the Likelihood of Biomarker Measurements",
    "section": "3.2 Unknown \\(k_j\\)",
    "text": "3.2 Unknown \\(k_j\\)\n\\[\nP(X_{j} | S) = \\sum_{k_j=0}^N{P(k_j) p(X_{j} \\mid S, k_j)}\n\\tag{3.2}\\]\nSuppose we have the same information above, except that we do not know at which disease stage the patient is, i.e., we do not know \\(k_j\\). We have the observed biomarker data: \\(X_j = [77, 45, 53, 90]\\). And I wonder: what is the likelihood of seeing this specific ovserved data?\nWe assume that all five stages (including \\(k_j = 0\\)) are equally likely.\nWe do not know \\(k_j\\), so the best option is to calculate the “average” likelihood of all the biomarker data.\nBased on Equation 3.1, we can calculate the following:\n\\(L_1 = p(X_j | S, k_j = 1)\\)\n\\(L_2 = p(X_j | S, k_j = 2)\\)\n\\(L_3 = p(X_j | S, k_j = 3)\\)\n\\(L_4 = p(X_j | S, k_j = 4)\\)\nAlso note that we need to consider \\(L_0\\) because in the equation above, \\(k_j\\) starts from \\(0\\).\n\\[L_0 = p(X_j | S, k_j = 0) = p (X_b | \\phi_b) \\times p (X_a | \\phi_a) \\times p (X_d | \\phi_d) \\times p (X_c | \\phi_c)\\]\n\\[L_1 = p(X_j | S, k_j = 1) = p (X_b | \\theta_b) \\times p (X_a | \\phi_a) \\times p (X_d | \\phi_d) \\times p (X_c | \\phi_c)\\]\n\\[L_2 = p(X_j | S, k_j = 2) = p (X_b | \\theta_b) \\times p (X_a | \\theta_a) \\times p (X_d | \\phi_d) \\times p (X_c | \\phi_c)\\]\n\\[L_3 = p(X_j | S, k_j = 3) = p (X_b | \\theta_b) \\times p (X_a | \\theta_a) \\times p (X_d | \\theta_d) \\times p (X_c | \\phi_c)\\]\n\\[L_4 = p(X_j | S, k_j = 4) = p (X_b | \\theta_b) \\times p (X_a | \\theta_a) \\times p (X_d | \\theta_d) \\times p (X_c | \\theta_c)\\]\n\\(P(k_j)\\) is the prior likelihood of being at stage \\(k\\). Event based models assume a uniform prior on \\(k_j\\). Therefore:\n\\(P(X_{j} | z_j=1, S) = \\frac{1}{5} \\left(L_0 + L_1 + L_2 + L_3 + L_4 \\right)\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Calculating the Likelihood of Biomarker Measurements</span>"
    ]
  },
  {
    "objectID": "gen.html",
    "href": "gen.html",
    "title": "4  Generate Synthetic Data",
    "section": "",
    "text": "4.1 Obtain Estimated Distribution Parameters\nIn Chapter 2, we mentioned that EBM can be used as a generative model and we need to know \\(S, \\theta, \\phi\\) and \\(k_j\\).\nFirst, we obtained \\(S, \\theta, \\phi\\) from Chen et al. (2016):\nThis is our estimation:\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport json \nimport scipy.stats as stats\nfrom typing import List, Optional, Tuple, Dict\nimport os \nimport seaborn as sns\nimport altair as alt\nCode\nall_ten_biomarker_names = np.array([\n    'MMSE', 'ADAS', 'AB', 'P-Tau', 'HIP-FCI', \n    'HIP-GMI', 'AVLT-Sum', 'PCC-FCI', 'FUS-GMI', 'FUS-FCI'])\n# in the order above\n# cyan, normal\nphi_means = [28, -6, 250, -25, 5, 0.4, 40, 12, 0.6, -10]\n# black, abnormal\ntheta_means = [22, -20, 150, -50, -5, 0.3, 20, 5, 0.5, -20]\n# cyan, normal\nphi_std_times_three = [2, 4, 150, 50, 5, 0.7, 45, 12, 0.2, 10]\nphi_stds = [std_dev/3 for std_dev in phi_std_times_three]\n# black, abnormal\ntheta_std_times_three = [8, 12, 50, 100, 20, 1, 20, 10, 0.2, 18]\ntheta_stds = [std_dev/3 for std_dev in theta_std_times_three]\n\n# to get the real_theta_phi means and stds\nhashmap_of_dicts = {}\nfor i, biomarker in enumerate(all_ten_biomarker_names):\n    dic = {}\n    # dic = {\"biomarker\": biomarker}\n    dic['theta_mean'] = theta_means[i]\n    dic['theta_std'] = theta_stds[i]\n    dic['phi_mean'] = phi_means[i]\n    dic['phi_std'] = phi_stds[i]\n    hashmap_of_dicts[biomarker] = dic \nhashmap_of_dicts\n\nreal_theta_phi = pd.DataFrame(hashmap_of_dicts).transpose().reset_index(names=['biomarker'])\nreal_theta_phi\n\n\n\n\n\n\n\n\n\nbiomarker\ntheta_mean\ntheta_std\nphi_mean\nphi_std\n\n\n\n\n0\nMMSE\n22.0\n2.666667\n28.0\n0.666667\n\n\n1\nADAS\n-20.0\n4.000000\n-6.0\n1.333333\n\n\n2\nAB\n150.0\n16.666667\n250.0\n50.000000\n\n\n3\nP-Tau\n-50.0\n33.333333\n-25.0\n16.666667\n\n\n4\nHIP-FCI\n-5.0\n6.666667\n5.0\n1.666667\n\n\n5\nHIP-GMI\n0.3\n0.333333\n0.4\n0.233333\n\n\n6\nAVLT-Sum\n20.0\n6.666667\n40.0\n15.000000\n\n\n7\nPCC-FCI\n5.0\n3.333333\n12.0\n4.000000\n\n\n8\nFUS-GMI\n0.5\n0.066667\n0.6\n0.066667\n\n\n9\nFUS-FCI\n-20.0\n6.000000\n-10.0\n3.333333\nStore the parameters to a JSON file:\nwith open('files/real_theta_phi.json', 'w') as fp:\n    json.dump(hashmap_of_dicts, fp)\nCode\nbiomarkers = all_ten_biomarker_names\nn_biomarkers = len(biomarkers)\n\ndef plot_distribution_pair(ax, mu1, sigma1, mu2, sigma2, title):\n    \"\"\"mu1, sigma1: theta\n    mu2, sigma2: phi\n    \"\"\"\n    xmin = min(mu1 - 4*sigma1, mu2-4*sigma2)\n    xmax = max(mu1 + 4*sigma1, mu2 + 4*sigma2)\n    x = np.linspace(xmin, xmax, 1000)\n    y1 = stats.norm.pdf(x, loc = mu1, scale = sigma1)\n    y2 = stats.norm.pdf(x, loc = mu2, scale = sigma2)\n    ax.plot(x, y1, label = \"Abnormal\", color = \"black\")\n    ax.plot(x, y2, label = \"Normal\", color = \"cyan\")\n    ax.fill_between(x, y1, alpha = 0.3, color = \"black\")\n    ax.fill_between(x, y2, alpha = 0.3, color = \"cyan\")\n    ax.set_title(title)\n    ax.legend()\n\nfig, axes = plt.subplots(2, n_biomarkers//2, figsize=(20, 10))\nfor i, biomarker in enumerate(biomarkers):\n    ax = axes.flatten()[i] \n    mu1, sigma1, mu2, sigma2 = real_theta_phi[\n        real_theta_phi.biomarker == biomarker].reset_index().iloc[0, :][2:].values\n    plot_distribution_pair(\n        ax, mu1, sigma1, mu2, sigma2, title = biomarker)\nYou can compare this to Figure 4.1.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "gen.html#obtain-estimated-distribution-parameters",
    "href": "gen.html#obtain-estimated-distribution-parameters",
    "title": "4  Generate Synthetic Data",
    "section": "",
    "text": "Figure 4.1: Theta and Phi from Chen’s Paper\n\n\n\n\n\n\n\n\n\nFigure 4.2: S from Chen’s Paper",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "gen.html#the-generating-process",
    "href": "gen.html#the-generating-process",
    "title": "4  Generate Synthetic Data",
    "section": "4.2 The Generating Process",
    "text": "4.2 The Generating Process\nIn the following, we explain our data generation process.\nWe have the following parameters:\n\\(J\\): Number of participants.\n\\(R\\): The percentage of healthy participants.\n\\(M\\): Number of datasets per combination of \\(j\\) and \\(r\\).\nWe set these parameters:\n\njs = [50, 200, 500]\nrs = [0.1, 0.25, 0.5, 0.75, 0.9]\nnum_of_datasets_per_combination = 50\n\nSo, there will be \\(3 \\times 5 \\times 50 = 750\\) datasets to be generated.\nWe define our generate_data_from_ebm function:\n\ndef generate_data_from_ebm(\n    n_participants: int,\n    S_ordering: List[str],\n    real_theta_phi_file: str,\n    healthy_ratio: float,\n    output_dir: str,\n    m,  # combstr_m\n    seed: Optional[int] = 0\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Simulate an Event-Based Model (EBM) for disease progression.\n\n    Args:\n    n_participants (int): Number of participants.\n    S_ordering (List[str]): Biomarker names ordered according to the order \n        in which each of them get affected by the disease.\n    real_theta_phi_file (str): Directory of a JSON file which contains \n        theta and phi values for all biomarkers.\n        See real_theta_phi.json for example format.\n    output_dir (str): Directory where output files will be saved.\n    healthy_ratio (float): Proportion of healthy participants out of n_participants.\n    seed (Optional[int]): Seed for the random number generator for reproducibility.\n\n    Returns:\n    pd.DataFrame: A DataFrame with columns 'participant', \"biomarker\", 'measurement', \n        'diseased'.\n    \"\"\"\n    # Parameter validation\n    assert n_participants &gt; 0, \"Number of participants must be greater than 0.\"\n    assert 0 &lt;= healthy_ratio &lt;= 1, \"Healthy ratio must be between 0 and 1.\"\n\n    # Set the seed for numpy's random number generator\n    rng = np.random.default_rng(seed)\n\n    # Load theta and phi values from the JSON file\n    try:\n        with open(real_theta_phi_file) as f:\n            real_theta_phi = json.load(f)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {real_theta_phi} not fount\")\n    except json.JSONDecodeError:\n        raise ValueError(\n            f\"File {real_theta_phi_file} is not a valid JSON file.\")\n\n    n_biomarkers = len(S_ordering)\n    n_stages = n_biomarkers + 1\n\n    n_healthy = int(n_participants * healthy_ratio)\n    n_diseased = int(n_participants - n_healthy)\n\n    # Generate disease stages\n    kjs = np.concatenate((np.zeros(n_healthy, dtype=int),\n                         rng.integers(1, n_stages, n_diseased)))\n    # shuffle so that it's not 0s first and then disease stages bur all random\n    rng.shuffle(kjs)\n\n    # Initiate biomarker measurement matrix (J participants x N biomarkers) with None\n    X = np.full((n_participants, n_biomarkers), None, dtype=object)\n\n    # Create distributions for each biomarker\n    theta_dist = {biomarker: stats.norm(\n        real_theta_phi[biomarker]['theta_mean'],\n        real_theta_phi[biomarker]['theta_std']\n    ) for biomarker in S_ordering}\n\n    phi_dist = {biomarker: stats.norm(\n        real_theta_phi[biomarker]['phi_mean'],\n        real_theta_phi[biomarker]['phi_std']\n    ) for biomarker in S_ordering}\n\n    # Populate the matrix with biomarker measurements\n    for j in range(n_participants):\n        for n, biomarker in enumerate(S_ordering):\n            # because for each j, we generate X[j, n] in the order of S_ordering,\n            # the final dataset will have this ordering as well.\n            k_j = kjs[j]\n            S_n = n + 1\n\n            # Assign biomarker values based on the participant's disease stage\n            # affected, or not_affected, is regarding the biomarker, not the participant\n            if k_j &gt;= 1:\n                if k_j &gt;= S_n:\n                    # rvs() is affected by np.random()\n                    X[j, n] = (\n                        j, biomarker, theta_dist[biomarker].rvs(random_state=rng), k_j, S_n, 'affected')\n                else:\n                    X[j, n] = (j, biomarker, phi_dist[biomarker].rvs(random_state=rng),\n                               k_j, S_n, 'not_affected')\n            # if the participant is healthy\n            else:\n                X[j, n] = (j, biomarker, phi_dist[biomarker].rvs(random_state=rng),\n                           k_j, S_n, 'not_affected')\n\n    df = pd.DataFrame(X, columns=S_ordering)\n    # make this dataframe wide to long\n    df_long = df.melt(var_name=\"Biomarker\", value_name=\"Value\")\n    data = df_long['Value'].apply(pd.Series)\n    data.columns = ['participant', \"biomarker\",\n                    'measurement', 'k_j', 'S_n', 'affected_or_not']\n\n    # biomarker_name_change_dic = dict(\n    #     zip(S_ordering, range(1, n_biomarkers + 1)))\n    data['diseased'] = data.apply(lambda row: row.k_j &gt; 0, axis=1)\n    # data.drop(['k_j', 'S_n', 'affected_or_not'], axis=1, inplace=True)\n    # data['biomarker'] = data.apply(\n    #     lambda row: f\"{row.biomarker} ({biomarker_name_change_dic[row.biomarker]})\", axis=1)\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filename = f\"{int(healthy_ratio*n_participants)}|{n_participants}_{m}\"\n    data.to_csv(f'{output_dir}/{filename}.csv', index=False)\n    # print(\"Data generation done! Output saved to:\", filename)\n    return data\n\n\nS_ordering = np.array([\n    'HIP-FCI', 'PCC-FCI', 'AB', 'P-Tau', 'MMSE', 'ADAS', \n    'HIP-GMI', 'AVLT-Sum', 'FUS-GMI', 'FUS-FCI'\n])\n\n# where the generated data will be saved\noutput_dir = 'data'\n\n# We run the following only once; once the data is generated, we no longer run it\n# We still show the codes to present our generation process\ntorun = False\n\n\nif torun:\n    real_theta_phi_file = 'files/real_theta_phi.json'\n    for j in js:\n        for r in rs:\n            for m in range(0, num_of_datasets_per_combination):\n                generate_data_from_ebm(\n                    n_participants=j,\n                    S_ordering=S_ordering,\n                    real_theta_phi_file=real_theta_phi_file,\n                    healthy_ratio=r,\n                    output_dir=output_dir,\n                    m=m,\n                    seed = int(j*10 + (r * 100) + m),\n                )\n        print(f'Done for J={j}')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "gen.html#visualize-synthetic-data",
    "href": "gen.html#visualize-synthetic-data",
    "title": "4  Generate Synthetic Data",
    "section": "4.3 Visualize Synthetic Data",
    "text": "4.3 Visualize Synthetic Data\nAbove, we have generated 750 datasets, named in the fashion of 150|200_3, which means the third dataset when \\(j = 200\\) and \\(r = 0.75\\).\nNext, we try to visualize this dataset.\n\ndf = pd.read_csv(f\"{output_dir}/150|200_3.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nparticipant\nbiomarker\nmeasurement\nk_j\nS_n\naffected_or_not\ndiseased\n\n\n\n\n0\n0\nHIP-FCI\n3.135981\n0\n1\nnot_affected\nFalse\n\n\n1\n1\nHIP-FCI\n12.593704\n2\n1\naffected\nTrue\n\n\n2\n2\nHIP-FCI\n6.220776\n0\n1\nnot_affected\nFalse\n\n\n3\n3\nHIP-FCI\n3.545100\n0\n1\nnot_affected\nFalse\n\n\n4\n4\nHIP-FCI\n3.966541\n0\n1\nnot_affected\nFalse\n\n\n\n\n\n\n\n\ndf.shape\n\n(2000, 7)\n\n\nThis dataset has \\(2000\\) rows because we have \\(200\\) participants and \\(10\\) biomarkers.\n\n4.3.1 Distribution of all biomarker values\n\n\nCode\nalt.renderers.enable('png')\nalt.Chart(df).transform_density(\n    'measurement',\n    as_=['measurement', 'Density'],\n    groupby=['biomarker']\n).mark_area().encode(\n    x=\"measurement:Q\",\n    y=\"Density:Q\",\n    facet = alt.Facet(\n        \"biomarker:N\",\n        columns = 5\n    ),\n    color=alt.Color(\n        'biomarker:N'\n    )\n).properties(\n    width= 100,\n    height = 180,\n).properties(\n    title='Distribution of biomarker measurments'\n)\n\n\n\n\n\nDistribution of biomarker measurments\n\n\n\n\n\n\n4.3.2 Distribution of A Specific Biomarker\n\n\nCode\nidx = 1\nbiomarkers = df.biomarker.unique()\nbio_data = df[df.biomarker==biomarkers[idx]]\nalt.Chart(bio_data).transform_density(\n    'measurement',\n    as_=['measurement', 'Density'],\n    groupby=['affected_or_not']\n).mark_area().encode(\n    x=\"measurement:Q\",\n    y=\"Density:Q\",\n    facet = alt.Facet(\n        \"affected_or_not:N\",\n    ),\n    color=alt.Color(\n        'affected_or_not:N'\n    )\n).properties(\n    width= 240,\n    height = 200,\n).properties(\n    title=f'Distribution of {biomarker} measurements'\n)\n\n\n\n\n\nDistribution of HIP-FCI measurements, compring bewteen affected and non-affected group\n\n\n\n\n\n\n4.3.3 Looking into A Specific Participant\n\npidx = 1\np_data = df[df.participant == pidx]\np_data\n\n\n\n\n\n\n\n\nparticipant\nbiomarker\nmeasurement\nk_j\nS_n\naffected_or_not\ndiseased\n\n\n\n\n1\n1\nHIP-FCI\n12.593704\n2\n1\naffected\nTrue\n\n\n201\n1\nPCC-FCI\n7.164017\n2\n2\naffected\nTrue\n\n\n401\n1\nAB\n182.033823\n2\n3\nnot_affected\nTrue\n\n\n601\n1\nP-Tau\n-25.345325\n2\n4\nnot_affected\nTrue\n\n\n801\n1\nMMSE\n27.600823\n2\n5\nnot_affected\nTrue\n\n\n1001\n1\nADAS\n-4.920415\n2\n6\nnot_affected\nTrue\n\n\n1201\n1\nHIP-GMI\n0.099052\n2\n7\nnot_affected\nTrue\n\n\n1401\n1\nAVLT-Sum\n30.270797\n2\n8\nnot_affected\nTrue\n\n\n1601\n1\nFUS-GMI\n0.658954\n2\n9\nnot_affected\nTrue\n\n\n1801\n1\nFUS-FCI\n-11.701559\n2\n10\nnot_affected\nTrue\n\n\n\n\n\n\n\n\n\nCode\npidx =1 # participant index\np_data = df[df.participant == pidx]\nalt.Chart(p_data).mark_bar().encode(\n    x='biomarker',\n    y='measurement',\n    color=alt.Color(\n        'affected_or_not:N'\n    ),\n    tooltip=['biomarker', 'affected_or_not', 'measurement']\n).interactive().properties(\n    title=f'Distribution of biomarker measurements for participant #{idx} (k_j = {p_data.k_j.to_list()[0]})'\n)\n\n\n\n\n\nDistribution of biomarker measurements for a specific participant\n\n\n\n\n\n\n\n\nChen, Guangyu, Hao Shu, Gang Chen, B Douglas Ward, Piero G Antuono, Zhijun Zhang, Shi-Jiang Li, Alzheimer’s Disease Neuroimaging Initiative, et al. 2016. “Staging Alzheimer’s Disease Risk by Sequencing Brain Function and Structure, Cerebrospinal Fluid, and Cognition Biomarkers.” Journal of Alzheimer’s Disease 54 (3): 983–93.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "estPara.html",
    "href": "estPara.html",
    "title": "5  Estimate Distribution Parameters",
    "section": "",
    "text": "5.1 Constrained K-Means\nThe first method we can use is constrained K-means, implemented by Babaki (2017).\nWe choose constrained K-Means instead of the standard K-Means algorithm because we know all healthy participants have to belong to the same cluster. The constrained K-Means algorithm can satisfy this constraint.\nCode\ndef compute_theta_phi_for_biomarker(biomarker_df):\n    \"\"\"get theta and phi parameters for this biomarker using constrained k-means\n    input: \n        - biomarker_df: a pd.dataframe of a specific biomarker\n    output: \n        - a tuple: theta_mean, theta_std, phi_mean, phi_std\n    \"\"\"\n    n_clusters = 2\n    measurements = np.array(biomarker_df['measurement']).reshape(-1, 1)\n    healthy_df = biomarker_df[biomarker_df['diseased'] == False]\n    must_link = [(x, 0) for x in healthy_df.index]\n    # imeplement Constrained K-means algorithm\n    # https://github.com/Behrouz-Babaki/COP-Kmeans\n    clusters, centers = cop_kmeans(dataset=measurements, k=n_clusters, ml=must_link)\n    predictions = np.array(clusters)\n    healthy_predictions = predictions[healthy_df.index]\n\n    # double check the result\n    cluster_counts = np.bincount(predictions)\n    if not all(c &gt; 1 for c in cluster_counts):\n        raise ValueError(f\"Not all clusters have more than one node.\")\n    if len(cluster_counts) != n_clusters:\n        raise ValueError(f\"Number of clusters is not equal to {n_clusters}.\")\n    if len(set(healthy_predictions)) &gt; 1:\n        raise ValueError(\"Not all healthy participants belong to one cluster.\")\n    \n    phi_cluster_idx = healthy_predictions[0]\n    theta_cluster_idx = 1 - phi_cluster_idx\n\n    # two empty clusters to strore measurements\n    clustered_measurements = [[] for _ in range(2)]\n    # Store measurements into their cluster\n    for i, prediction in enumerate(predictions):\n        clustered_measurements[prediction].append(measurements[i][0])\n    \n     # Calculate means and standard deviations\n    theta_mean, theta_std = np.mean(\n        clustered_measurements[theta_cluster_idx]), np.std(\n            clustered_measurements[theta_cluster_idx])\n    phi_mean, phi_std = np.mean(\n        clustered_measurements[phi_cluster_idx]), np.std(\n            clustered_measurements[phi_cluster_idx])\n    \n    # check whether the prior_theta_phi contain 0s or nan\n    if math.isnan(theta_std) or theta_std == 0:\n        raise ValueError(f\"Invalid theta_std: {theta_std}\")\n    if math.isnan(phi_std) or phi_std == 0:\n        raise ValueError(f\"Invalid phi_std: {phi_std}\")\n    if theta_mean == 0 or math.isnan(theta_mean):\n        raise ValueError(f\"Invalid theta_mean: {theta_mean}\")\n    if phi_mean == 0 or math.isnan(phi_mean):\n        raise ValueError(f\"Invalid phi_mean: {phi_mean}\")\n\n    return theta_mean, theta_std, phi_mean, phi_std\n\ndef get_theta_phi_estimates(\n    data: pd.DataFrame,\n) -&gt; Dict[str, Dict[str, float]]:\n    \"\"\"\n    Obtain theta and phi estimates (mean and standard deviation) for each biomarker.\n\n    Args:\n    data (pd.DataFrame): DataFrame containing participant data with columns 'participant', \n        'biomarker', 'measurement', and 'diseased'.\n    # biomarkers (List[str]): A list of biomarker names.\n\n    Returns:\n    Dict[str, Dict[str, float]]: A dictionary where each key is a biomarker name,\n        and each value is another dictionary containing the means and standard deviations \n        for theta and phi of that biomarker, with keys 'theta_mean', 'theta_std', 'phi_mean', \n        and 'phi_std'.\n    \"\"\"\n    # empty hashmap of dictionaries to store the estimates\n    estimates = {}\n    biomarkers = data.biomarker.unique()\n    for biomarker in biomarkers:\n        # Filter data for the current biomarker\n        # reset_index is necessary here because we will use healthy_df.index later\n        biomarker_df = data[data['biomarker']\n                            == biomarker].reset_index(drop=True)\n        theta_mean, theta_std, phi_mean, phi_std = compute_theta_phi_for_biomarker(\n            biomarker_df)\n        estimates[biomarker] = {\n            'theta_mean': theta_mean,\n            'theta_std': theta_std,\n            'phi_mean': phi_mean,\n            'phi_std': phi_std\n        }\n    return estimates\nestimates = get_theta_phi_estimates(data = df)\nestimates_df = pd.DataFrame.from_dict(estimates, orient='index')\nestimates_df.reset_index(names = 'biomarker', inplace=True)\nestimates_df\n\n\n\n\n\n\n\n\nbiomarker\ntheta_mean\ntheta_std\nphi_mean\nphi_std\n\n\n\n\n0\nHIP-FCI\n-8.587833\n5.365053\n4.903437\n2.008974\n\n\n1\nPCC-FCI\n16.330398\n0.720160\n10.507041\n4.427907\n\n\n2\nAB\n288.610990\n9.079334\n229.407086\n61.967583\n\n\n3\nP-Tau\n-71.767059\n15.231429\n-23.737466\n16.637657\n\n\n4\nMMSE\n22.164115\n1.555676\n27.994652\n0.832845\n\n\n5\nADAS\n-20.582091\n3.853234\n-6.002048\n1.487443\n\n\n6\nHIP-GMI\n0.114735\n0.106857\n0.404924\n0.235082\n\n\n7\nAVLT-Sum\n23.638988\n5.848744\n42.028039\n14.229989\n\n\n8\nFUS-GMI\n0.645942\n0.019813\n0.579887\n0.067980\n\n\n9\nFUS-FCI\n-19.200644\n4.688806\n-9.568412\n3.003514\nwith open('files/real_theta_phi.json', 'r') as f:\n    truth = json.load(f)\ntruth_df = pd.DataFrame.from_dict(truth, orient='index')\ntruth_df.reset_index(names = 'biomarker', inplace=True)\ntruth_df\n\n\n\n\n\n\n\n\nbiomarker\ntheta_mean\ntheta_std\nphi_mean\nphi_std\n\n\n\n\n0\nMMSE\n22.0\n2.666667\n28.0\n0.666667\n\n\n1\nADAS\n-20.0\n4.000000\n-6.0\n1.333333\n\n\n2\nAB\n150.0\n16.666667\n250.0\n50.000000\n\n\n3\nP-Tau\n-50.0\n33.333333\n-25.0\n16.666667\n\n\n4\nHIP-FCI\n-5.0\n6.666667\n5.0\n1.666667\n\n\n5\nHIP-GMI\n0.3\n0.333333\n0.4\n0.233333\n\n\n6\nAVLT-Sum\n20.0\n6.666667\n40.0\n15.000000\n\n\n7\nPCC-FCI\n5.0\n3.333333\n12.0\n4.000000\n\n\n8\nFUS-GMI\n0.5\n0.066667\n0.6\n0.066667\n\n\n9\nFUS-FCI\n-20.0\n6.000000\n-10.0\n3.333333\nNow let’s compare the results using plots:\nCode\ndef obtain_theta_phi_params(biomarker, estimate_df, truth):\n    '''This is to obtain both true and estimated theta and phi params for each biomarker '''\n    biomarker_data_est = estimate_df[estimate_df.biomarker == biomarker].reset_index()\n    biomarker_data = truth[truth.biomarker == biomarker].reset_index()\n    # theta for affected\n    theta_mean_est = biomarker_data_est.theta_mean[0]\n    theta_std_est = biomarker_data_est.theta_std[0]\n\n    theta_mean = biomarker_data.theta_mean[0]\n    theta_std = biomarker_data.theta_std[0]\n\n    # phi for not affected\n    phi_mean_est = biomarker_data_est.phi_mean[0]\n    phi_std_est = biomarker_data_est.phi_std[0]\n\n    phi_mean = biomarker_data.phi_mean[0]\n    phi_std = biomarker_data.phi_std[0]\n\n    return theta_mean, theta_std, theta_mean_est, theta_std_est, phi_mean, phi_std, phi_mean_est, phi_std_est\n\ndef make_chart(biomarkers, estimate_df, truth, title):\n    alt.renderers.enable('png')\n    charts = []\n    for biomarker in biomarkers: \n        theta_mean, theta_std, theta_mean_est, theta_std_est, phi_mean, phi_std, phi_mean_est, phi_std_est = obtain_theta_phi_params(\n        biomarker, estimate_df, truth)\n        mean1, std1 = theta_mean, theta_std\n        mean2, std2 = theta_mean_est, theta_std_est\n\n        # Generating points on the x axis\n        x_thetas = np.linspace(min(mean1 - 3*std1, mean2 - 3*std2), \n                        max(mean1 + 3*std1, mean2 + 3*std2), 1000)\n\n        # Creating DataFrames for each distribution\n        df1 = pd.DataFrame({'x': x_thetas, 'pdf': norm.pdf(x_thetas, mean1, std1), 'Distribution': 'Actual'})\n        df2 = pd.DataFrame({'x': x_thetas, 'pdf': norm.pdf(x_thetas, mean2, std2), 'Distribution': 'Estimated'})\n\n        # Combining the DataFrames\n        df3 = pd.concat([df1, df2])\n\n        # Altair plot\n        chart_theta = alt.Chart(df3).mark_line().encode(\n            x='x',\n            y='pdf',\n            color=alt.Color('Distribution:N', legend=alt.Legend(title=\"Theta\"))\n        ).properties(\n            title=f'{biomarker}, Theta',\n            width=100,\n            height=100\n            )\n\n        mean1, std1 = phi_mean, phi_std\n        mean2, std2 = phi_mean_est, phi_std_est\n\n        # Generating points on the x axis\n        x_phis = np.linspace(min(mean1 - 3*std1, mean2 - 3*std2), \n                        max(mean1 + 3*std1, mean2 + 3*std2), 1000)\n\n        # Creating DataFrames for each distribution\n        df1 = pd.DataFrame({'x': x_phis, 'pdf': norm.pdf(x_phis, mean1, std1), 'Distribution': 'Actual'})\n        df2 = pd.DataFrame({'x': x_phis, 'pdf': norm.pdf(x_phis, mean2, std2), 'Distribution': 'Estimated'})\n\n        # Combining the DataFrames\n        df3 = pd.concat([df1, df2])\n\n        # Altair plot\n        chart_phi = alt.Chart(df3).mark_line().encode(\n            x='x',\n            y='pdf',\n            color=alt.Color('Distribution:N', legend=alt.Legend(title=\"Phi\"))\n        ).properties(\n            title=f'{biomarker}, Phi',\n            width=100,\n            height=100\n            )\n        \n        # Concatenate theta and phi charts horizontally\n        hconcat_chart = alt.hconcat(chart_theta, chart_phi).resolve_scale(color=\"independent\")\n\n        # Append the concatenated chart to the list of charts\n        charts.append(hconcat_chart)\n    # Concatenate all the charts vertically\n    final_chart = alt.vconcat(*charts).properties(title = title)\n\n    # Display the final chart\n    final_chart.display()\nmake_chart(biomarkers[0:4], \n           estimates_df, \n           truth_df, \n           title = \"Comparing Theta and Phi Distributions Using Constrained K-Means\")\n\n\n\n\nComparing Theta and Phi Distributions Using Constrained K-Means\nIt turns out the result is generally okay.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimate Distribution Parameters</span>"
    ]
  },
  {
    "objectID": "estPara.html#conjugate-priors",
    "href": "estPara.html#conjugate-priors",
    "title": "5  Estimate Distribution Parameters",
    "section": "5.2 Conjugate Priors",
    "text": "5.2 Conjugate Priors\nThe second method we may utilize is conjugate priors. Conjugacy occurs when the posterior distribution is in the same family of distribution as the prior distribution, but with new parameter values.\nWhy conjugacy is important? Because without it, one has to do the integration, which oftentimes is hard.\nThree major conjugate families:\n\nBeta-Binomial\nGamma-Poisson\nNormal-Normal\n\nIn our example, we assume that the measurement data for each biomarker follows a normal distribution; however, we do not know the exact \\(\\mu\\) and \\(\\sigma\\). Our job is to estimate the two parameters for each biomarker based on the data we have.\nAccording to An Introduction to Bayesian Thinking by Clyde et al. (2022), if the data comes from a normal distribution with unknown \\(\\mu\\) and \\(\\sigma\\), the conjugate prior for \\(\\mu\\) has a normal distribution with mean \\(m_0\\) and variance \\(\\frac{\\sigma^2}{n_0}\\). The conjugate prior for \\(\\frac{1}{\\sigma^2}\\) has a Gamma distribution with shape \\(\\frac{v_0}{2}\\) and rate \\(\\frac{v_0 s_0^{2}}{2}\\) where\n\n\\(m_0\\): prior estimate of \\(\\mu\\).\n\\(n_0\\): how strongly is the prior belief in \\(m_0\\) is held.\n\\(s_0^2\\): prior estimate of \\(\\sigma^2\\).\n\\(v_0\\): prior degress of freedome, influencing the certainty of \\(s_0^2\\).\n\nThat is to say:\n\\[\\mu | \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2/n_0)\\]\n\\[1/\\sigma^2 \\sim Gamma\\left(\\frac{v_0}{2}, \\frac{v_0 s_0^2}{2} \\right)\\]\nCombined, we have:\n\\[(\\mu, 1/\\sigma^2) \\sim NormalGamma(m_0, n_0, s_0^2, v_0)\\]\nThe posterior also follows a Normal-Gamma distribution:\n\\[(\\mu, 1/\\sigma^2) | data \\sim NormalGamma(m_n, n_n, s_n^2, v_n)\\]\nMore specifically\n\\[1/\\sigma^2 | data \\sim Gamma(v_n/2, s_n^2 v_n/2)\\]\n\\[\\mu | data, \\sigma^2 \\sim \\mathcal{N}(m_n, \\sigma^2/n_n)\\]\nBased on the above two equations, we know that the mean of posterior mean is \\(m_n\\) and the mean of the posterior variance is \\((s_n^2 v_n/2)/(v_n/2)\\). This is beceause the expected value of \\(Gamma(\\alpha, \\beta)\\) is \\(\\frac{\\alpha}{\\beta}\\).\nwhere\n\n\\(m_n\\): posterior mean, mode, and median for \\(\\mu\\)\n\\(n_n\\): posterior sample size\n\\(s_n^2\\): posterior variance\n\\(v_n\\): posterior degrees of freedome\n\nThe updating rules to get the new hyper-parameters:\n\n\\[m_n = \\frac{n}{n+n_0} \\bar{y} + \\frac{n_0}{n+n_0}m_0\\]\n\\[n_n = n_0 + n\\]\n\\[v_n = v_0 + n\\]\n\\[s_n^2 = \\frac{1}{v_n}\\left[s^2(n-1) + s_0^2v_0 + \\frac{n_0n}{n_n}(\\bar{y}-m_0)^2\\right]\\]\nwhere\n\n\\(n\\): sample size\n\\(\\bar{y}\\): sample mean\n\\(s^2\\): sample variance\n\n\n\nCode\ndef estimate_params_exact(m0, n0, s0_sq, v0, data):\n    '''This is to estimate means and vars based on conjugate priors\n    Inputs:\n        - data: a vector of measurements \n        - m0: prior estimate of $\\mu$.\n        - n0: how strongly is the prior belief in $m_0$ is held.\n        - s0_sq: prior estimate of $\\sigma^2$.\n        - v0: prior degress of freedome, influencing the certainty of $s_0^2$.\n\n    Outputs:\n        - mu estiate, std estimate\n    '''\n    # Data summary\n    sample_mean = np.mean(data)\n    sample_size = len(data)\n    sample_var = np.var(data, ddof=1)  # ddof=1 for unbiased estimator\n\n    # Update hyperparameters for the Normal-Inverse Gamma posterior\n    updated_m0 = (n0 * m0 + sample_size * sample_mean) / (n0 + sample_size)\n    updated_n0 = n0 + sample_size\n    updated_v0 = v0 + sample_size\n    updated_s0_sq = (1 / updated_v0) * ((sample_size - 1) * sample_var + v0 * s0_sq +\n                                        (n0 * sample_size / updated_n0) * (sample_mean - m0)**2)\n    updated_alpha = updated_v0/2\n    updated_beta = updated_v0*updated_s0_sq/2\n\n    # Posterior estimates\n    mu_posterior_mean = updated_m0\n    sigma_squared_posterior_mean = updated_beta/updated_alpha\n\n    mu_estimation = mu_posterior_mean\n    std_estimation = np.sqrt(sigma_squared_posterior_mean)\n\n    return mu_estimation, std_estimation\n\ndef get_theta_phi_conjugate_priors(biomarkers, data_we_have, theta_phi_kmeans):\n    '''To get estimated parameters, returns a hashmap\n    Input:\n    - biomarkers: biomarkers \n    - data_we_have: participants data filled with initial or updated participant_stages\n    - theta_phi_kmeans: a hashmap of dicts, which are the prior theta and phi values\n        obtained from the initial constrained kmeans algorithm\n\n    Output: \n    - a hashmap of dictionaries. Key is biomarker name and value is a dictionary.\n    Each dictionary contains the theta and phi mean/std values for a specific biomarker. \n    '''\n    # empty list of dictionaries to store the estimates\n    hashmap_of_means_stds_estimate_dicts = {}\n\n    for biomarker in biomarkers:\n        # Initialize dictionary outside the inner loop\n        dic = {'biomarker': biomarker}\n        for affected in ['affected', 'not_affected']:\n            data_full = data_we_have[(data_we_have.biomarker == biomarker) & (\n                data_we_have.affected_or_not == affected)]\n            if len(data_full) &gt; 1:\n                measurements = data_full.measurement\n                s0_sq = np.var(measurements, ddof=1)\n                m0 = np.mean(measurements)\n                mu_estimate, std_estimate = estimate_params_exact(\n                    m0=m0, n0=1, s0_sq=s0_sq, v0=1, data=measurements)\n                if affected == 'affected':\n                    dic['theta_mean'] = mu_estimate\n                    dic['theta_std'] = std_estimate\n                else:\n                    dic['phi_mean'] = mu_estimate\n                    dic['phi_std'] = std_estimate\n            # If there is only one observation or not observation at all, resort to theta_phi_kmeans\n            # YES, IT IS POSSIBLE THAT DATA_FULL HERE IS NULL\n            # For example, if a biomarker indicates stage of (num_biomarkers), but all participants' stages\n            # are smaller than that stage; so that for all participants, this biomarker is not affected\n            else:\n                print('not enough data here, so we have to use theta phi estimates from constrained kmeans')\n                # print(theta_phi_kmeans)\n                if affected == 'affected':\n                    dic['theta_mean'] = theta_phi_kmeans[biomarker]['theta_mean']\n                    dic['theta_std'] = theta_phi_kmeans[biomarker]['theta_std']\n                else:\n                    dic['phi_mean'] = theta_phi_kmeans[biomarker]['phi_mean']\n                    dic['phi_std'] = theta_phi_kmeans[biomarker]['phi_std']\n        # print(f\"biomarker {biomarker} done!\")\n        hashmap_of_means_stds_estimate_dicts[biomarker] = dic\n    return hashmap_of_means_stds_estimate_dicts\n\n\n\nconjugate_prior_theta_phi = get_theta_phi_conjugate_priors(\n    biomarkers = biomarkers, \n    data_we_have = df, \n    theta_phi_kmeans = estimates\n)\ncp_df = pd.DataFrame.from_dict(conjugate_prior_theta_phi, orient='index')\ncp_df.reset_index(drop=True, inplace=True)\ncp_df\n\n\n\n\n\n\n\n\nbiomarker\ntheta_mean\ntheta_std\nphi_mean\nphi_std\n\n\n\n\n0\nHIP-FCI\n-5.378366\n7.233991\n5.092800\n1.514402\n\n\n1\nPCC-FCI\n5.521792\n2.777207\n12.071769\n3.671679\n\n\n2\nAB\n151.143708\n14.806694\n251.973564\n51.382188\n\n\n3\nP-Tau\n-41.768257\n34.857945\n-24.739527\n14.928907\n\n\n4\nMMSE\n23.122406\n2.446874\n28.049683\n0.718493\n\n\n5\nADAS\n-19.633304\n4.582900\n-5.902198\n1.278311\n\n\n6\nHIP-GMI\n0.425625\n0.272876\n0.379542\n0.235348\n\n\n7\nAVLT-Sum\n21.664360\n3.755735\n40.700638\n14.480463\n\n\n8\nFUS-GMI\n0.482745\n0.055585\n0.590434\n0.063730\n\n\n9\nFUS-FCI\n-18.566905\n5.781937\n-9.648705\n3.099195\n\n\n\n\n\n\n\n\nmake_chart(\n    biomarkers[0:4], \n    cp_df, \n    truth_df, \n    title = \"Comparing Theta and Phi Distributions Using Conjugate Priors\"\n)\n\n\n\n\nComparing Theta and Phi Distributions Using Conjugate Prior",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimate Distribution Parameters</span>"
    ]
  },
  {
    "objectID": "estPara.html#conclusion",
    "href": "estPara.html#conclusion",
    "title": "5  Estimate Distribution Parameters",
    "section": "5.3 Conclusion",
    "text": "5.3 Conclusion\nThe result from conjugate priors looks better than that from constrained kmeans. However, to use conjugate priors, we assume that we know \\(S\\) and \\(k_j\\) for each participant, which are very hardly known. The constrained kmeans algorithm does not require any prior knowledge of \\(S\\) nor \\(k_j\\).\n\n\n\n\nBabaki, Behrouz. 2017. “COP-Kmeans Version 1.5.” https://doi.org/10.5281/zenodo.831850.\n\n\nClyde, M, M Cetinkaya-Rundel, C Rundel, D Banks, C Chai, and L Huang. 2022. “An Introduction to Bayesian Thinking: A Companion to the Statistics with r Course. 2020.” https://statswithr.github.io/book/.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimate Distribution Parameters</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Babaki, Behrouz. 2017. “COP-Kmeans Version 1.5.” https://doi.org/10.5281/zenodo.831850.\n\n\nChen, Guangyu, Hao Shu, Gang Chen, B Douglas Ward, Piero G Antuono,\nZhijun Zhang, Shi-Jiang Li, Alzheimer’s Disease Neuroimaging Initiative,\net al. 2016. “Staging Alzheimer’s Disease Risk by Sequencing Brain\nFunction and Structure, Cerebrospinal Fluid, and Cognition\nBiomarkers.” Journal of Alzheimer’s Disease 54 (3):\n983–93.\n\n\nClyde, M, M Cetinkaya-Rundel, C Rundel, D Banks, C Chai, and L Huang.\n2022. “An Introduction to Bayesian Thinking: A Companion to the\nStatistics with r Course. 2020.” https://statswithr.github.io/book/.",
    "crumbs": [
      "References"
    ]
  }
]