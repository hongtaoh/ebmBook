[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Event Based Models for Disease Progression",
    "section": "",
    "text": "1 Introduction\nYou can also read the material in PDF.\nThe Event-Based Model (EBM; Fonteijn et al. (2012)) is a probabilistic model that can be used to infer the order by which a disease affects the parts of a person’s body. In other words, it allows us to estimate the stages by which different biological factors (“biomarkers”) are affected by a disease.\nFor instance, Alzheimer may have the following stages:\n\n\n\n\n\n\nFigure 1.1: Alzheimer Disease Progression (Credit: https://preventad.com/alzheimers-disease/)\n\n\n\nWe estimate this order based on the biomarker data from patients’ visits. These data are typically results of neuropsych (e.g., MMSE) and/or biological examiations (e.g., blood pressure). Visits data can be longitudinal and/or cross-sectional, i.e., single visits from a cohort of patients.\nKnowing the disease progression is important because it helps prevent and hopefully cure the disease. It also helps health professionals prepare for the disease’s further development.\nThe EBM has been especially helpful at providing converging support for the stages of neural deterioration of neural degenerative diseases. Neural diseases are notoriously complex for many reasons. For instance, they are difficult to study in vivo due to the challenge of direct and accurate measurement of the brain at high resolution without harming the person.\nBy formulating the deterioration process as a probabilistic model, the EBM affords the ability to conduct complex reasoning from noisy patient data. Given the difficulty of this task, it is unclear how much (number of participants) and what kind of data (healthy percentage) is needed for reliable estimation and how to best understand the uncertainty of model estimates. Although prior work has not addressed the former question in great detail, prior work has derived uncertainty of its estimates indirectly using bootstrapping. In this monograph, we analyze the statistical consistency of different inference methods for the EBM model. To foreshadow, our results are reassuring, yet troubling, and promising. Inference methods used in prior work work well when there is a large number of participants (~500) and the percentage of healthy participants is near 50%. However, clinical studies with such large sample sizes are uncommon and so, in typical settings, the prior approximation methods may be unreliable. We test our own method using Markov chain Monte Carlo techniques and find it provides much more accurate estimates for small sample sizes that are robust across different percentages of healthy participants. The results of our analyses come with a caveat: they are based on simulated data. We discuss this caveat, other limitations, and how to interpret our results in the monograph.\nWe have several assumptions in EBM:\n\n\n\n\n\n\nFigure 1.2: Assumptions of EBM\n\n\n\n\nThe disease is irreversible (i.e., a patient cannot go from stage 2 to stage 1)\nThe order in which different biomarkers get affected by the disease is the same across all patients.\nBiomarker data can be approximated by a Gaussian distribution.\n\n\n\n\n\n\n\nPay Attention\n\n\n\nThe third assumption, i.e., Gaussian approximation, often will be violated in raw biomarker data. For example, measurements of the concentration of amyloid proteins associated with Alzheimer’s disease are necessarily non-negative. Further, their resolution has changed over the decades.\nHowever, for the purpose of our current method, we assume this is true. There are nonparametric versions of the EBM, for example, KDE EBM\n\n\nThis book contains chapters that explain step by step how we use the event-based model to estimate the order of disease progression based on cross-sectional patients’ biomarker data.\n\n\n\n\nFonteijn, Hubert M, Marc Modat, Matthew J Clarkson, Josephine Barnes, Manja Lehmann, Nicola Z Hobbs, Rachael I Scahill, et al. 2012. “An Event-Based Model for Disease Progression and Its Application in Familial Alzheimer’s Disease and Huntington’s Disease.” NeuroImage 60 (3): 1880–89.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_ebmExplain.html",
    "href": "02_ebmExplain.html",
    "title": "2  EBM Explained",
    "section": "",
    "text": "2.1 Overview of Event-Based Model (EBM)\nEBM provides a statistical model to understand disease progression through biomarkers. Using EBM, we can estimate the likelihood of biomarker measurements or generate synthetic data of biomarker measurements.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EBM Explained</span>"
    ]
  },
  {
    "objectID": "02_ebmExplain.html#key-concepts",
    "href": "02_ebmExplain.html#key-concepts",
    "title": "2  EBM Explained",
    "section": "2.2 Key Concepts",
    "text": "2.2 Key Concepts\nSuppose the order in which a disease affects biomarkers is \\(S\\). For example, \\(S\\) = [‘biomarker1’, ‘biomarker3’, ‘biomarker2’].\nWe also suppose biomarker measurements following Gaussian distributions. When a biomarker is affected by the disease, the parameters, i.e., mean and standard deviation of its normal distribution is called \\(\\theta\\). When it is not affected by the disease, the parameters are called \\(\\phi\\).\nThe disease stage of a participant is \\(k_j\\). To simplify things, let us assume the total number of disease stages is equal to the number of biomarkers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EBM Explained</span>"
    ]
  },
  {
    "objectID": "02_ebmExplain.html#ebm-applications",
    "href": "02_ebmExplain.html#ebm-applications",
    "title": "2  EBM Explained",
    "section": "2.3 EBM Applications",
    "text": "2.3 EBM Applications\nEBM can be used in two ways:\n\nCalculate the likelihood of biomarker measurements\nGenerate biomarker measurements\n\n\n2.3.1 Calculate the Likelihood of Biomarker Measurements\nSuppose we have one participant’s measurement data of five biomarkers:\n\n\n\n\n\n\nFigure 2.1: Sample Data\n\n\n\nNow, the question is:\n\nWhat is the likelihood of this participant having this sequence of biomarker data, given that we know \\(S, \\theta, \\phi\\).\n\n\\(S\\) is the order in which different biomarkers get affected by the disease. It is the column of \\(S_n\\) in the above data.\n\\(\\theta\\) for each biomarker is the \\(\\mu\\) and \\(\\alpha\\) of normal distribution of biomarker measurement when the biomarker is affected by the disease.\n\\(\\phi\\) for each biomarker is the \\(\\mu\\) and \\(\\alpha\\) of normal distribution of biomarker measurement when the biomarker is NOT affected by the disease.\nIn the following, we explain how to calculate this likelihood in two scenartios: (1) known \\(k_j\\) and (2) unknown \\(k_j\\).\n\n2.3.1.1 Known \\(k_j\\)\n\\[\np(X_{j} | S, z_j = 1, k_j) = \\underbrace{\\prod_{i=1}^{k_j}{p(X_{S(i)j} \\mid \\theta_{S(i)} )}}_{\\text{Affected biomarker likelihood}} \\,\n\\underbrace{\\prod_{i=k_j+1}^N{p(X_{S(i)j} \\mid \\phi_{S(i)})}}_{\\text{Non-affected biomarker likelihood}}\n\\tag{2.1}\\]\nThis equation compuates the likelihood of the observed biomarker data of a specific participant, given that we know the disease stage this patient is at (\\(k_j\\)).\n\n\\(S\\) is an orded array of biomarkers that are affected by the disease, for example, \\([b, a, d, c]\\). This means that biomarker \\(b\\) is affected at stage 1. At stage 2, biomarker \\(b\\) and \\(a\\) will be affected.\n\\(S(i)\\) is the \\(i^{th}\\) biomarker according to \\(S\\). For example \\(S_1\\) will be biomarker \\(b\\).\n\\(k_j\\) indicates the stage the patient is at, for example, \\(k_j = 2\\). This means that the disease has effected biomarker \\(a\\) and \\(b\\). Biomarker \\(c\\) and \\(d\\) have not been affected yet.\n\\(\\theta_{S(i)}\\) is the parameters for the probability density function (PDF) of observed value of biomarker \\(S(i)\\) when this biomarker has been affected by the disease. Let’s assume this distribution is a Gaussian distribution with means of \\([45, 50, 55, 60]\\) and a standard deviation of \\(5\\) for biomarker \\(b\\), \\(a\\), \\(d\\), and \\(c\\).\n\\(\\phi_{S(i)}\\) is the parameters for the probability density function (PDF) of observed value of biomarker \\(S(i)\\) when this biomarker has NOT been affected by the disease. Let’s assume this distribution is a Gaussian distribution with means of \\([25, 30, 35, 40]\\) and a standard deviation of \\(3\\) for biomarker \\(b\\), \\(a\\), \\(d\\), and \\(c\\).\n\\(X_j\\) is an array representing the patient’s observed data for all biomarker. Assume the data is \\([77, 45, 53, 90]\\) for biomarker \\(b\\), \\(a\\), \\(d\\), and \\(c\\).\n\nWe assume that the patient is at stage \\(2\\) of this disease; hence \\(k_j = 2\\).\nNext, we are going to calculate \\(p(X_j|S, z_j = 1, k_j)\\):\nWhen \\(i = 1\\), we have \\(S_{(i)} = b\\) and \\(X_{S_{(i)}} = X_b = 45\\). So\n\\[p(X_{S_{(i)}} | \\theta_{S(i)}) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{X_b - \\mu}{\\sigma} \\right)^2}\\]\nBecause \\(k_j = 2\\), so biomarker \\(b\\) and \\(a\\) are affected. We should use the distribution of \\(\\theta_b\\); therefore, we should plug in \\(\\mu = 45, \\sigma = 5\\) in the above equation.\nWe can do the same for \\(i\\) = 2, 3, and 4.\nSo\n\\[p(X_j | S, k_j = 2) = p (X_b | \\theta_b) \\times p (X_a | \\theta_a) \\times p (X_d | \\phi_d) \\times p (X_c | \\phi_c)\\]\nThe above is the likelihood of the given biomarker data when \\(k_j = 2\\).\nNote that \\(p (X_b | \\theta_b)\\) is probability density, a value of a probability density function at a specific point; so it is not a probability itself.\nMultiplying multiple probability densities will give us a likelihood.\n\n\n2.3.1.2 Unknown \\(k_j\\)\n\\[\nP(X_{j} | S) = \\sum_{k_j=0}^N{P(k_j) p(X_{j} \\mid S, k_j)}\n\\tag{2.2}\\]\nSuppose we have the same information above, except that we do not know at which disease stage the patient is, i.e., we do not know \\(k_j\\). We have the observed biomarker data: \\(X_j = [77, 45, 53, 90]\\). And I wonder: what is the likelihood of seeing this specific ovserved data?\nWe assume that all five stages (including \\(k_j = 0\\)) are equally likely.\nWe do not know \\(k_j\\), so the best option is to calculate the “average” likelihood of all the biomarker data.\nBased on Equation 2.1, we can calculate the following:\n\\(L_1 = p(X_j | S, k_j = 1)\\)\n\\(L_2 = p(X_j | S, k_j = 2)\\)\n\\(L_3 = p(X_j | S, k_j = 3)\\)\n\\(L_4 = p(X_j | S, k_j = 4)\\)\nIf this participant is healthy, then we know \\(k_j = 0\\), therefore:\n\\[L = L_0 = p(X_j | S, k_j = 0) = p (X_b | \\phi_b) \\times p (X_a | \\phi_a) \\times p (X_d | \\phi_d) \\times p (X_c | \\phi_c)\\]\nIf this participant is diseased but we do not know the actual \\(k_j\\), we can estimate it this way\n\\[L_1 = p(X_j | S, k_j = 1) = p (X_b | \\theta_b) \\times p (X_a | \\phi_a) \\times p (X_d | \\phi_d) \\times p (X_c | \\phi_c)\\]\n\\[L_2 = p(X_j | S, k_j = 2) = p (X_b | \\theta_b) \\times p (X_a | \\theta_a) \\times p (X_d | \\phi_d) \\times p (X_c | \\phi_c)\\]\n\\[L_3 = p(X_j | S, k_j = 3) = p (X_b | \\theta_b) \\times p (X_a | \\theta_a) \\times p (X_d | \\theta_d) \\times p (X_c | \\phi_c)\\]\n\\[L_4 = p(X_j | S, k_j = 4) = p (X_b | \\theta_b) \\times p (X_a | \\theta_a) \\times p (X_d | \\theta_d) \\times p (X_c | \\theta_c)\\]\n\\(P(k_j)\\) is the prior likelihood of being at stage \\(k\\). Event based models assume a uniform prior on \\(k_j\\). Therefore:\n\\(P(X_{j} | z_j=1, S) = \\frac{1}{4} \\left(L_1 + L_2 + L_3 + L_4 \\right)\\)\n\n\n\n\n\n\nTip\n\n\n\nWhen this participant is diseased but we do not know the actual stage of this participant, the above method is useful also because it hints at the relative likelihood of each possible stage. For example, if L2 is much larger than L1, L3, and L4, then we know this participant is most likely to be at stage 2.\n\n\n\n\n2.3.1.3 Extension\nIf we are more interested in the likelihood of a whole dataset consisting of all participants, we multiply all participants’ likelihood: \\(L = L_{P_1} \\times L_{P_2} \\times L_{P_3} ... \\times L_{P_j}\\). Because this number tends to be very large, we take the natural log of \\(L\\), i.e., \\(\\ln(L)\\).\n\n\n\n2.3.2 EBM as A Generative Model\nWe can use EBM to generate synthetic biomarker data if we know:\n\nThe order (\\(S\\)) in which different biomarkers get affected by the disease.\nParameters (i.e., mean and standard deviation) of biomarkers’ distribution when they are affected (\\(\\theta\\)) and not affected (\\(\\phi\\)) by the disease.\nStages (\\(k_j\\)) that each participant is in.\n\nData we can generate looks like Figure 2.1.\nThis data is from a single participant.\nAs we mentioned above, to generate this data, we need to know:\n\n\\(S\\), i.e., the order of biomarkers. In the above example, \\(S\\) is HIP-FCI, PCC-FCI, HIP-GMI, FUS-GMI, FUS-FCI.\n\\(\\mathcal N(\\theta_{\\mu}, \\theta_{\\sigma})\\) and \\(\\mathcal N(\\phi_{\\mu}, \\phi_{\\sigma})\\) for each of the five biomarkers, which are known but not shown directly here in the dataset.\n\\(k_j\\), which is 2 in the above example.\n\nWe explain how this data is constructed in the following, column by column.\nFirst, the participant id is \\(67\\). The biomarker indicates each of the five biomarkers examined and measured. The measurement is the biomarkers’ measurement. k_j is the participant’s stage. If this stage is above 0, it means Diseased = True. S_n indicates the \\(n\\)-th rank in the order. If k_j &lt; S_n, it means the participant’s stage hasn’t reached that biomarker’s rank; therefore, this biomarker is not affected. If k_j &gt;= S_n, then this biomarker is affected.\nIf a biomarker is affected, then its measurement comes from \\(\\mathcal N(\\theta_{\\mu}, \\theta_{\\sigma})\\) of that biomarker; if not_affected, \\(\\mathcal N(\\phi_{\\mu}, \\phi_{\\sigma})\\).\n\n2.3.2.1 Generative Process\nThe generative process of biomarker measurements can be described as:\n\\[\nX_{nj} \\mid S, k_j, \\theta_n, \\phi_n \\sim I(z_j = 1) \\bigg[ I(S(n) \\leq k_j) \\, p(X_{nj} \\mid \\theta_{n}) + I(S(n) &gt; k_j) \\, p(X_{nj} \\mid \\phi_{n}) \\bigg] \\\\\n+ \\left(1 - I(z_j = 1) \\right) p(X_{nj} \\mid \\phi_{n})\n\\tag{2.3}\\]\n\nThis model says that given that we know \\(S, k_j, \\theta_n, \\text{and } \\phi_n\\), we can draw the biomarker measurement from a distribution.\n\\(S \\sim \\mathrm{UniformPermutation}(\\cdot)\\)\n\\(S\\) follows a distribution of uniform permutation. That means the ordering of biomarkers is random.\n\\(k_j \\sim \\mathrm{DiscreteUniform}(N)\\)\n\\(k_j\\) follows a discrete uniform distribution, which means a participant is equally likely to fall in a progression stage (e.g., from \\(0\\) to \\(5\\), where \\(0\\) indicate this participant is healthy.)\n\n\n2.3.2.2 Graphical Explanation\nIn the following, we explain the generative model in three different scenarios using graphical models: (1) All participants are healthy; (2) Both healthy and diseased participants, but all biomarkers are affected among diseased people; (3) Both healthy and diseased participants, but we do not whether biomarkers are affected or not among patients.\n\n2.3.2.2.1 Scenario 1\nIf all participants are healthy:\n\\[\nX_{nj} \\sim p(X_{nj} \\mid \\phi_{n})\n\\tag{2.4}\\]\nWhere\n\\(X_{nj}\\) indicates the measurement of biomarker \\(n\\) in participant \\(j\\).\n\\(\\phi_{n}\\) represents \\(\\mathcal N(\\phi_{\\mu}, \\phi_{\\sigma})\\) for biomarker \\(n\\).\nThe graphical model would look like:\n\n\n\n\n\n\nFigure 2.2: Graphical Model of Scenario 1\n\n\n\n\n\n2.3.2.2.2 Scenario 2\nIf we have oth diseased and healthy participants, and all biomarkers are affected among diseased participants.\n\\[\nX_{nj} \\sim I(z_j == 1) p(X_{nj} \\mid \\theta_n) + (1-I(z_j == 1))p(X_{nj} \\mid \\phi_n)\n\\tag{2.5}\\]\nWhere:\n\\(z_j = 1\\) indicates this participant is diseased and \\(z_j = 1\\) represents a healthy participant.\n\\(I(True) = 1\\) and \\(I(False) = 0\\).\n\\(\\theta_{n}\\) represents \\(\\mathcal N(\\theta_{\\mu}, \\theta_{\\sigma})\\) for biomarker \\(n\\).\nThe graphical model would look like:\n\n\n\n\n\n\nFigure 2.3: Graphical Model of Scenario 2\n\n\n\n\n\n2.3.2.2.3 Scenario 3\nIf we have both healthy and diseased participants, but we do not whether biomarkers are affected or not among patients, see Equation 2.3.\nThis is the model in usual cases.\nThe graphical model looks like:\n\n\n\n\n\n\nFigure 2.4: Graphical Model of Scenario 3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EBM Explained</span>"
    ]
  },
  {
    "objectID": "03_gen.html",
    "href": "03_gen.html",
    "title": "3  Generate Synthetic Data",
    "section": "",
    "text": "3.1 Obtain Estimated Distribution Parameters\nIn Section 2.3.2, we mentioned that EBM can be used as a generative model and we need to know \\(S, \\theta, \\phi\\) and \\(k_j\\).\nFirst, we obtained \\(S, \\theta, \\phi\\) from Chen et al. (2016):\nThis is our estimation:\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport json \nimport scipy.stats as stats\nfrom typing import List, Optional, Tuple, Dict\nimport os \nimport seaborn as sns\nimport altair as alt\nCode\nall_ten_biomarker_names = np.array([\n    'MMSE', 'ADAS', 'AB', 'P-Tau', 'HIP-FCI', \n    'HIP-GMI', 'AVLT-Sum', 'PCC-FCI', 'FUS-GMI', 'FUS-FCI'])\n# in the order above\n# cyan, normal\nphi_means = [28, -6, 250, -25, 5, 0.4, 40, 12, 0.6, -10]\n# black, abnormal\ntheta_means = [22, -20, 150, -50, -5, 0.3, 20, 5, 0.5, -20]\n# cyan, normal\nphi_std_times_three = [2, 4, 150, 50, 5, 0.7, 45, 12, 0.2, 10]\nphi_stds = [std_dev/3 for std_dev in phi_std_times_three]\n# black, abnormal\ntheta_std_times_three = [8, 12, 50, 100, 20, 1, 20, 10, 0.2, 18]\ntheta_stds = [std_dev/3 for std_dev in theta_std_times_three]\n\n# to get the real_theta_phi means and stds\nhashmap_of_dicts = {}\nfor i, biomarker in enumerate(all_ten_biomarker_names):\n    dic = {}\n    # dic = {\"biomarker\": biomarker}\n    dic['theta_mean'] = theta_means[i]\n    dic['theta_std'] = theta_stds[i]\n    dic['phi_mean'] = phi_means[i]\n    dic['phi_std'] = phi_stds[i]\n    hashmap_of_dicts[biomarker] = dic \nhashmap_of_dicts\n\nreal_theta_phi = pd.DataFrame(hashmap_of_dicts).transpose().reset_index(names=['biomarker'])\nreal_theta_phi\n\n\n\n\n\n\n\n\n\nbiomarker\ntheta_mean\ntheta_std\nphi_mean\nphi_std\n\n\n\n\n0\nMMSE\n22.0\n2.666667\n28.0\n0.666667\n\n\n1\nADAS\n-20.0\n4.000000\n-6.0\n1.333333\n\n\n2\nAB\n150.0\n16.666667\n250.0\n50.000000\n\n\n3\nP-Tau\n-50.0\n33.333333\n-25.0\n16.666667\n\n\n4\nHIP-FCI\n-5.0\n6.666667\n5.0\n1.666667\n\n\n5\nHIP-GMI\n0.3\n0.333333\n0.4\n0.233333\n\n\n6\nAVLT-Sum\n20.0\n6.666667\n40.0\n15.000000\n\n\n7\nPCC-FCI\n5.0\n3.333333\n12.0\n4.000000\n\n\n8\nFUS-GMI\n0.5\n0.066667\n0.6\n0.066667\n\n\n9\nFUS-FCI\n-20.0\n6.000000\n-10.0\n3.333333\nStore the parameters to a JSON file:\nwith open('files/real_theta_phi.json', 'w') as fp:\n    json.dump(hashmap_of_dicts, fp)\nCode\nbiomarkers = all_ten_biomarker_names\nn_biomarkers = len(biomarkers)\n\ndef plot_distribution_pair(ax, mu1, sigma1, mu2, sigma2, title):\n    \"\"\"mu1, sigma1: theta\n    mu2, sigma2: phi\n    \"\"\"\n    xmin = min(mu1 - 4*sigma1, mu2-4*sigma2)\n    xmax = max(mu1 + 4*sigma1, mu2 + 4*sigma2)\n    x = np.linspace(xmin, xmax, 1000)\n    y1 = stats.norm.pdf(x, loc = mu1, scale = sigma1)\n    y2 = stats.norm.pdf(x, loc = mu2, scale = sigma2)\n    ax.plot(x, y1, label = \"Abnormal\", color = \"black\")\n    ax.plot(x, y2, label = \"Normal\", color = \"cyan\")\n    ax.fill_between(x, y1, alpha = 0.3, color = \"black\")\n    ax.fill_between(x, y2, alpha = 0.3, color = \"cyan\")\n    ax.set_title(title)\n    ax.legend()\n\nfig, axes = plt.subplots(2, n_biomarkers//2, figsize=(20, 10))\nfor i, biomarker in enumerate(biomarkers):\n    ax = axes.flatten()[i] \n    mu1, sigma1, mu2, sigma2 = real_theta_phi[\n        real_theta_phi.biomarker == biomarker].reset_index().iloc[0, :][2:].values\n    plot_distribution_pair(\n        ax, mu1, sigma1, mu2, sigma2, title = biomarker)\n\n\n\n\n\n\n\n\nFigure 3.3: evaluate theta and phi estimations\nYou can compare this to Figure 3.1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "03_gen.html#obtain-estimated-distribution-parameters",
    "href": "03_gen.html#obtain-estimated-distribution-parameters",
    "title": "3  Generate Synthetic Data",
    "section": "",
    "text": "Figure 3.1: Theta and Phi from Chen’s Paper\n\n\n\n\n\n\n\n\n\nFigure 3.2: S from Chen’s Paper",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "03_gen.html#the-generating-process",
    "href": "03_gen.html#the-generating-process",
    "title": "3  Generate Synthetic Data",
    "section": "3.2 The Generating Process",
    "text": "3.2 The Generating Process\nIn the following, we explain our data generation process.\nWe have the following parameters:\n\\(J\\): Number of participants.\n\\(R\\): The percentage of healthy participants.\n\\(M\\): Number of datasets per combination of \\(j\\) and \\(r\\).\nWe set these parameters:\n\njs = [50, 200, 500]\nrs = [0.1, 0.25, 0.5, 0.75, 0.9]\nnum_of_datasets_per_combination = 50\n\nSo, there will be \\(3 \\times 5 \\times 50 = 750\\) datasets to be generated.\nWe define our generate_data_from_ebm function:\n\ndef generate_data_from_ebm(\n    n_participants: int,\n    S_ordering: List[str],\n    real_theta_phi_file: str,\n    healthy_ratio: float,\n    output_dir: str,\n    m,  # combstr_m\n    seed: Optional[int] = 0\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Simulate an Event-Based Model (EBM) for disease progression.\n\n    Args:\n    n_participants (int): Number of participants.\n    S_ordering (List[str]): Biomarker names ordered according to the order \n        in which each of them get affected by the disease.\n    real_theta_phi_file (str): Directory of a JSON file which contains \n        theta and phi values for all biomarkers.\n        See real_theta_phi.json for example format.\n    output_dir (str): Directory where output files will be saved.\n    healthy_ratio (float): Proportion of healthy participants out of n_participants.\n    seed (Optional[int]): Seed for the random number generator for reproducibility.\n\n    Returns:\n    pd.DataFrame: A DataFrame with columns 'participant', \"biomarker\", 'measurement', \n        'diseased'.\n    \"\"\"\n    # Parameter validation\n    assert n_participants &gt; 0, \"Number of participants must be greater than 0.\"\n    assert 0 &lt;= healthy_ratio &lt;= 1, \"Healthy ratio must be between 0 and 1.\"\n\n    # Set the seed for numpy's random number generator\n    rng = np.random.default_rng(seed)\n\n    # Load theta and phi values from the JSON file\n    try:\n        with open(real_theta_phi_file) as f:\n            real_theta_phi = json.load(f)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {real_theta_phi} not fount\")\n    except json.JSONDecodeError:\n        raise ValueError(\n            f\"File {real_theta_phi_file} is not a valid JSON file.\")\n\n    n_biomarkers = len(S_ordering)\n    n_stages = n_biomarkers + 1\n\n    n_healthy = int(n_participants * healthy_ratio)\n    n_diseased = int(n_participants - n_healthy)\n\n    # Generate disease stages\n    kjs = np.concatenate((np.zeros(n_healthy, dtype=int),\n                         rng.integers(1, n_stages, n_diseased)))\n    # shuffle so that it's not 0s first and then disease stages bur all random\n    rng.shuffle(kjs)\n\n    # Initiate biomarker measurement matrix (J participants x N biomarkers) with None\n    X = np.full((n_participants, n_biomarkers), None, dtype=object)\n\n    # Create distributions for each biomarker\n    theta_dist = {biomarker: stats.norm(\n        real_theta_phi[biomarker]['theta_mean'],\n        real_theta_phi[biomarker]['theta_std']\n    ) for biomarker in S_ordering}\n\n    phi_dist = {biomarker: stats.norm(\n        real_theta_phi[biomarker]['phi_mean'],\n        real_theta_phi[biomarker]['phi_std']\n    ) for biomarker in S_ordering}\n\n    # Populate the matrix with biomarker measurements\n    for j in range(n_participants):\n        for n, biomarker in enumerate(S_ordering):\n            # because for each j, we generate X[j, n] in the order of S_ordering,\n            # the final dataset will have this ordering as well.\n            k_j = kjs[j]\n            S_n = n + 1\n\n            # Assign biomarker values based on the participant's disease stage\n            # affected, or not_affected, is regarding the biomarker, not the participant\n            if k_j &gt;= 1:\n                if k_j &gt;= S_n:\n                    # rvs() is affected by np.random()\n                    X[j, n] = (\n                        j, biomarker, theta_dist[biomarker].rvs(random_state=rng), k_j, S_n, 'affected')\n                else:\n                    X[j, n] = (j, biomarker, phi_dist[biomarker].rvs(random_state=rng),\n                               k_j, S_n, 'not_affected')\n            # if the participant is healthy\n            else:\n                X[j, n] = (j, biomarker, phi_dist[biomarker].rvs(random_state=rng),\n                           k_j, S_n, 'not_affected')\n\n    df = pd.DataFrame(X, columns=S_ordering)\n    # make this dataframe wide to long\n    df_long = df.melt(var_name=\"Biomarker\", value_name=\"Value\")\n    data = df_long['Value'].apply(pd.Series)\n    data.columns = ['participant', \"biomarker\",\n                    'measurement', 'k_j', 'S_n', 'affected_or_not']\n\n    # biomarker_name_change_dic = dict(\n    #     zip(S_ordering, range(1, n_biomarkers + 1)))\n    data['diseased'] = data.apply(lambda row: row.k_j &gt; 0, axis=1)\n    # data.drop(['k_j', 'S_n', 'affected_or_not'], axis=1, inplace=True)\n    # data['biomarker'] = data.apply(\n    #     lambda row: f\"{row.biomarker} ({biomarker_name_change_dic[row.biomarker]})\", axis=1)\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filename = f\"{int(healthy_ratio*n_participants)}|{n_participants}_{m}\"\n    data.to_csv(f'{output_dir}/{filename}.csv', index=False)\n    # print(\"Data generation done! Output saved to:\", filename)\n    return data\n\n\nS_ordering = np.array([\n    'HIP-FCI', 'PCC-FCI', 'AB', 'P-Tau', 'MMSE', 'ADAS', \n    'HIP-GMI', 'AVLT-Sum', 'FUS-GMI', 'FUS-FCI'\n])\n\n# where the generated data will be saved\noutput_dir = 'data'\n\n# We run the following only once; once the data is generated, we no longer run it\n# We still show the codes to present our generation process\ntorun = False\n\n\nif torun:\n    real_theta_phi_file = 'files/real_theta_phi.json'\n    for j in js:\n        for r in rs:\n            for m in range(0, num_of_datasets_per_combination):\n                generate_data_from_ebm(\n                    n_participants=j,\n                    S_ordering=S_ordering,\n                    real_theta_phi_file=real_theta_phi_file,\n                    healthy_ratio=r,\n                    output_dir=output_dir,\n                    m=m,\n                    seed = int(j*10 + (r * 100) + m),\n                )\n        print(f'Done for J={j}')",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "03_gen.html#visualize-synthetic-data",
    "href": "03_gen.html#visualize-synthetic-data",
    "title": "3  Generate Synthetic Data",
    "section": "3.3 Visualize Synthetic Data",
    "text": "3.3 Visualize Synthetic Data\nAbove, we have generated 750 datasets, named in the fashion of 150|200_3, which means the third dataset when \\(j = 200\\) and \\(r = 0.75\\).\nNext, we try to visualize this dataset.\n\ndf = pd.read_csv(f\"{output_dir}/150|200_3.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nparticipant\nbiomarker\nmeasurement\nk_j\nS_n\naffected_or_not\ndiseased\n\n\n\n\n0\n0\nHIP-FCI\n3.135981\n0\n1\nnot_affected\nFalse\n\n\n1\n1\nHIP-FCI\n12.593704\n2\n1\naffected\nTrue\n\n\n2\n2\nHIP-FCI\n6.220776\n0\n1\nnot_affected\nFalse\n\n\n3\n3\nHIP-FCI\n3.545100\n0\n1\nnot_affected\nFalse\n\n\n4\n4\nHIP-FCI\n3.966541\n0\n1\nnot_affected\nFalse\n\n\n\n\n\n\n\n\ndf.shape\n\n(2000, 7)\n\n\nThis dataset has \\(2000\\) rows because we have \\(200\\) participants and \\(10\\) biomarkers.\n\n3.3.1 Distribution of all biomarker values\n\n\nCode\nalt.renderers.enable('png')\nalt.Chart(df).transform_density(\n    'measurement',\n    as_=['measurement', 'Density'],\n    groupby=['biomarker']\n).mark_area().encode(\n    x=\"measurement:Q\",\n    y=\"Density:Q\",\n    facet = alt.Facet(\n        \"biomarker:N\",\n        columns = 5\n    ),\n    color=alt.Color(\n        'biomarker:N'\n    )\n).properties(\n    width= 100,\n    height = 180,\n).properties(\n    title='Distribution of biomarker measurments'\n)\n\n\n\n\n\n\n\n\nFigure 3.4: Distribution of biomarker measurments\n\n\n\n\n\n\n\n3.3.2 Distribution of A Specific Biomarker\n\n\nCode\nidx = 1\nbiomarkers = df.biomarker.unique()\nbio_data = df[df.biomarker==biomarkers[idx]]\nalt.Chart(bio_data).transform_density(\n    'measurement',\n    as_=['measurement', 'Density'],\n    groupby=['affected_or_not']\n).mark_area().encode(\n    x=\"measurement:Q\",\n    y=\"Density:Q\",\n    facet = alt.Facet(\n        \"affected_or_not:N\",\n    ),\n    color=alt.Color(\n        'affected_or_not:N'\n    )\n).properties(\n    width= 240,\n    height = 200,\n).properties(\n    title=f'Distribution of {biomarker} measurements'\n)\n\n\n\n\n\n\n\n\nFigure 3.5: Distribution of HIP-FCI measurements, compring bewteen affected and non-affected group\n\n\n\n\n\n\n\n3.3.3 Looking into A Specific Participant\n\npidx = 1\np_data = df[df.participant == pidx]\np_data\n\n\n\n\n\n\n\n\nparticipant\nbiomarker\nmeasurement\nk_j\nS_n\naffected_or_not\ndiseased\n\n\n\n\n1\n1\nHIP-FCI\n12.593704\n2\n1\naffected\nTrue\n\n\n201\n1\nPCC-FCI\n7.164017\n2\n2\naffected\nTrue\n\n\n401\n1\nAB\n182.033823\n2\n3\nnot_affected\nTrue\n\n\n601\n1\nP-Tau\n-25.345325\n2\n4\nnot_affected\nTrue\n\n\n801\n1\nMMSE\n27.600823\n2\n5\nnot_affected\nTrue\n\n\n1001\n1\nADAS\n-4.920415\n2\n6\nnot_affected\nTrue\n\n\n1201\n1\nHIP-GMI\n0.099052\n2\n7\nnot_affected\nTrue\n\n\n1401\n1\nAVLT-Sum\n30.270797\n2\n8\nnot_affected\nTrue\n\n\n1601\n1\nFUS-GMI\n0.658954\n2\n9\nnot_affected\nTrue\n\n\n1801\n1\nFUS-FCI\n-11.701559\n2\n10\nnot_affected\nTrue\n\n\n\n\n\n\n\n\n\nCode\npidx =1 # participant index\np_data = df[df.participant == pidx]\nalt.Chart(p_data).mark_bar().encode(\n    x='biomarker',\n    y='measurement',\n    color=alt.Color(\n        'affected_or_not:N'\n    ),\n    tooltip=['biomarker', 'affected_or_not', 'measurement']\n).interactive().properties(\n    title=f'Distribution of biomarker measurements for participant #{idx} (k_j = {p_data.k_j.to_list()[0]})'\n)\n\n\n\n\n\n\n\n\nFigure 3.6: Distribution of biomarker measurements for a specific participant\n\n\n\n\n\n\n\n\n\nChen, Guangyu, Hao Shu, Gang Chen, B Douglas Ward, Piero G Antuono, Zhijun Zhang, Shi-Jiang Li, Alzheimer’s Disease Neuroimaging Initiative, et al. 2016. “Staging Alzheimer’s Disease Risk by Sequencing Brain Function and Structure, Cerebrospinal Fluid, and Cognition Biomarkers.” Journal of Alzheimer’s Disease 54 (3): 983–93.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "04_estDistParams.html",
    "href": "04_estDistParams.html",
    "title": "4  Estimate Distribution Parameters",
    "section": "",
    "text": "4.1 Simple Clustering\nThe first method we can use is simple clustering. We clustering a certain biomarker’s measurements into two clusters. A clustering is successful if:\nIdeally, we wanted all healthy participants to be grouped into a single cluster, which is why we initially tried using the constrained K-Means algorithm implemented by Babaki (2017). However, the algorithm did not work as intended.\nWe therefore designed a simple clustering algorithm to satisfy our needs:\nCode\ndef compute_theta_phi_for_biomarker(biomarker_df, max_attempt = 50, seed = None):\n    \"\"\"get theta and phi parameters for this biomarker using simple clustering\n    input: \n        - biomarker_df: a pd.dataframe of a specific biomarker\n    output: \n        - a tuple: theta_mean, theta_std, phi_mean, phi_std\n    \"\"\"\n    if seed is not None:\n        # Set the seed for numpy's random number generator\n        rng = np.random.default_rng(seed)\n    else:\n        rng = np.random\n\n    n_clusters = 2\n    measurements = np.array(biomarker_df['measurement']).reshape(-1, 1)\n    healthy_df = biomarker_df[biomarker_df['diseased'] == False]\n\n    clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n    predictions = clustering.fit_predict(measurements)\n    \n    # Verify that AgglomerativeClustering produced exactly 2 clusters with more than 1 member each\n    cluster_counts = np.bincount(predictions) # array([3, 2])\n    if len(cluster_counts) != n_clusters or any(c &lt;= 1 for c in cluster_counts):\n        print(\"AgglomerativeClustering did not yield the required clusters, switching to KMeans.\")\n        \n        # If AgglomerativeClustering fails, attempt KMeans with a max_attempt limit\n        curr_attempt = 0\n        n_init_value = 10\n        clustering_setup = KMeans(n_clusters=n_clusters, n_init=n_init_value)\n        \n        while curr_attempt &lt; max_attempt:\n            clustering_result = clustering_setup.fit(measurements)\n            predictions = clustering_result.labels_\n            cluster_counts = np.bincount(predictions) # array([3, 2])\n            \n            if len(cluster_counts) == n_clusters and all(c &gt; 1 for c in cluster_counts):\n                break \n            curr_attempt += 1\n        else:\n            print(f\"KMeans failed. Try randomizing the predictions\")\n            predictions = rng.choice([0, 1], size=len(measurements))\n            cluster_counts = np.bincount(predictions)\n            if len(cluster_counts) != n_clusters or not all(c &gt; 1 for c in cluster_counts):\n                raise ValueError(f\"KMeans clustering failed to find valid clusters within max_attempt.\")\n    \n    healthy_predictions = predictions[healthy_df.index]\n    mode_result = mode(healthy_predictions, keepdims=False).mode\n    phi_cluster_idx = mode_result[0] if isinstance(mode_result, np.ndarray) else mode_result\n    theta_cluster_idx = 1 - phi_cluster_idx\n\n    # two empty clusters to strore measurements\n    clustered_measurements = [[] for _ in range(2)]\n    # Store measurements into their cluster\n    for i, prediction in enumerate(predictions):\n        clustered_measurements[prediction].append(measurements[i][0])\n    \n     # Calculate means and standard deviations\n    theta_mean, theta_std = np.mean(\n        clustered_measurements[theta_cluster_idx]), np.std(\n            clustered_measurements[theta_cluster_idx])\n    phi_mean, phi_std = np.mean(\n        clustered_measurements[phi_cluster_idx]), np.std(\n            clustered_measurements[phi_cluster_idx])\n    \n    # Check for invalid values\n    if any(np.isnan(v) or v == 0 for v in [theta_std, phi_std, theta_mean, phi_mean]):\n        raise ValueError(\"One of the calculated values is invalid (0 or NaN).\")\n\n    return theta_mean, theta_std, phi_mean, phi_std\n\ndef get_theta_phi_estimates(\n    data: pd.DataFrame,\n) -&gt; Dict[str, Dict[str, float]]:\n    \"\"\"\n    Obtain theta and phi estimates (mean and standard deviation) for each biomarker.\n\n    Args:\n    data (pd.DataFrame): DataFrame containing participant data with columns 'participant', \n        'biomarker', 'measurement', and 'diseased'.\n    # biomarkers (List[str]): A list of biomarker names.\n\n    Returns:\n    Dict[str, Dict[str, float]]: A dictionary where each key is a biomarker name,\n        and each value is another dictionary containing the means and standard deviations \n        for theta and phi of that biomarker, with keys 'theta_mean', 'theta_std', 'phi_mean', \n        and 'phi_std'.\n    \"\"\"\n    # empty hashmap of dictionaries to store the estimates\n    estimates = {}\n    biomarkers = data.biomarker.unique()\n    for biomarker in biomarkers:\n        # Filter data for the current biomarker\n        # reset_index is necessary here because we will use healthy_df.index later\n        biomarker_df = data[data['biomarker']\n                            == biomarker].reset_index(drop=True)\n        theta_mean, theta_std, phi_mean, phi_std = compute_theta_phi_for_biomarker(\n            biomarker_df)\n        estimates[biomarker] = {\n            'theta_mean': theta_mean,\n            'theta_std': theta_std,\n            'phi_mean': phi_mean,\n            'phi_std': phi_std\n        }\n    return estimates\nsimple_clustering_estimates = get_theta_phi_estimates(data = df)\nsimple_clustering_estimates_df = pd.DataFrame.from_dict(\n    simple_clustering_estimates, orient='index')\nsimple_clustering_estimates_df.reset_index(names = 'biomarker', inplace=True)\nsimple_clustering_estimates_df\n\n\n\n\n\n\n\n\nbiomarker\ntheta_mean\ntheta_std\nphi_mean\nphi_std\n\n\n\n\n0\nHIP-FCI\n-10.228327\n4.983578\n4.542993\n2.553737\n\n\n1\nPCC-FCI\n15.809768\n1.650215\n8.263734\n3.139495\n\n\n2\nAB\n164.477459\n29.331669\n270.635016\n37.100502\n\n\n3\nP-Tau\n-46.553651\n16.168641\n-13.821754\n12.241742\n\n\n4\nMMSE\n22.621574\n1.864118\n28.051114\n0.753814\n\n\n5\nADAS\n-19.998640\n4.244514\n-5.924551\n1.307868\n\n\n6\nHIP-GMI\n0.726423\n0.125354\n0.290994\n0.168539\n\n\n7\nAVLT-Sum\n27.808002\n8.561387\n52.524815\n7.989840\n\n\n8\nFUS-GMI\n0.484881\n0.039194\n0.611798\n0.044585\n\n\n9\nFUS-FCI\n-7.061708\n1.928477\n-12.647345\n2.927590\nwith open('files/real_theta_phi.json', 'r') as f:\n    truth = json.load(f)\ntruth_df = pd.DataFrame.from_dict(truth, orient='index')\ntruth_df.reset_index(names = 'biomarker', inplace=True)\ntruth_df\n\n\n\n\n\n\n\n\nbiomarker\ntheta_mean\ntheta_std\nphi_mean\nphi_std\n\n\n\n\n0\nMMSE\n22.0\n2.666667\n28.0\n0.666667\n\n\n1\nADAS\n-20.0\n4.000000\n-6.0\n1.333333\n\n\n2\nAB\n150.0\n16.666667\n250.0\n50.000000\n\n\n3\nP-Tau\n-50.0\n33.333333\n-25.0\n16.666667\n\n\n4\nHIP-FCI\n-5.0\n6.666667\n5.0\n1.666667\n\n\n5\nHIP-GMI\n0.3\n0.333333\n0.4\n0.233333\n\n\n6\nAVLT-Sum\n20.0\n6.666667\n40.0\n15.000000\n\n\n7\nPCC-FCI\n5.0\n3.333333\n12.0\n4.000000\n\n\n8\nFUS-GMI\n0.5\n0.066667\n0.6\n0.066667\n\n\n9\nFUS-FCI\n-20.0\n6.000000\n-10.0\n3.333333\nNow let’s compare the results using plots:\nCode\ndef obtain_theta_phi_params(biomarker, estimate_df, truth):\n    '''This is to obtain both true and estimated theta and phi params for each biomarker '''\n    biomarker_data_est = estimate_df[estimate_df.biomarker == biomarker].reset_index()\n    biomarker_data = truth[truth.biomarker == biomarker].reset_index()\n    # theta for affected\n    theta_mean_est = biomarker_data_est.theta_mean[0]\n    theta_std_est = biomarker_data_est.theta_std[0]\n\n    theta_mean = biomarker_data.theta_mean[0]\n    theta_std = biomarker_data.theta_std[0]\n\n    # phi for not affected\n    phi_mean_est = biomarker_data_est.phi_mean[0]\n    phi_std_est = biomarker_data_est.phi_std[0]\n\n    phi_mean = biomarker_data.phi_mean[0]\n    phi_std = biomarker_data.phi_std[0]\n\n    return theta_mean, theta_std, theta_mean_est, theta_std_est, phi_mean, phi_std, phi_mean_est, phi_std_est\n\ndef make_chart(biomarkers, estimate_df, truth, title):\n    alt.renderers.enable('png')\n    charts = []\n    for biomarker in biomarkers: \n        theta_mean, theta_std, theta_mean_est, theta_std_est, phi_mean, phi_std, phi_mean_est, phi_std_est = obtain_theta_phi_params(\n        biomarker, estimate_df, truth)\n        mean1, std1 = theta_mean, theta_std\n        mean2, std2 = theta_mean_est, theta_std_est\n\n        # Generating points on the x axis\n        x_thetas = np.linspace(min(mean1 - 3*std1, mean2 - 3*std2), \n                        max(mean1 + 3*std1, mean2 + 3*std2), 1000)\n\n        # Creating DataFrames for each distribution\n        df1 = pd.DataFrame({'x': x_thetas, 'pdf': norm.pdf(x_thetas, mean1, std1), 'Distribution': 'Actual'})\n        df2 = pd.DataFrame({'x': x_thetas, 'pdf': norm.pdf(x_thetas, mean2, std2), 'Distribution': 'Estimated'})\n\n        # Combining the DataFrames\n        df3 = pd.concat([df1, df2])\n\n        # Altair plot\n        chart_theta = alt.Chart(df3).mark_line().encode(\n            x='x',\n            y='pdf',\n            color=alt.Color('Distribution:N', legend=alt.Legend(title=\"Theta\"))\n        ).properties(\n            title=f'{biomarker}, Theta',\n            width=100,\n            height=100\n            )\n\n        mean1, std1 = phi_mean, phi_std\n        mean2, std2 = phi_mean_est, phi_std_est\n\n        # Generating points on the x axis\n        x_phis = np.linspace(min(mean1 - 3*std1, mean2 - 3*std2), \n                        max(mean1 + 3*std1, mean2 + 3*std2), 1000)\n\n        # Creating DataFrames for each distribution\n        df1 = pd.DataFrame({'x': x_phis, 'pdf': norm.pdf(x_phis, mean1, std1), 'Distribution': 'Actual'})\n        df2 = pd.DataFrame({'x': x_phis, 'pdf': norm.pdf(x_phis, mean2, std2), 'Distribution': 'Estimated'})\n\n        # Combining the DataFrames\n        df3 = pd.concat([df1, df2])\n\n        # Altair plot\n        chart_phi = alt.Chart(df3).mark_line().encode(\n            x='x',\n            y='pdf',\n            color=alt.Color('Distribution:N', legend=alt.Legend(title=\"Phi\"))\n        ).properties(\n            title=f'{biomarker}, Phi',\n            width=100,\n            height=100\n            )\n        \n        # Concatenate theta and phi charts horizontally\n        hconcat_chart = alt.hconcat(chart_theta, chart_phi).resolve_scale(color=\"independent\")\n\n        # Append the concatenated chart to the list of charts\n        charts.append(hconcat_chart)\n    # Concatenate all the charts vertically\n    final_chart = alt.vconcat(*charts).properties(title = title)\n\n    # Display the final chart\n    final_chart.display()\nmake_chart(\n    biomarkers[0:4], \n    simple_clustering_estimates_df, \n    truth_df, \n    title = \"Comparing Theta and Phi Distributions Using Simple Clustering\"\n)\n\n\n\n\n\n\n\nFigure 4.1: Comparing Theta and Phi Distributions Using Simple Clusering\nIt turns out the result is not very desriable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimate Distribution Parameters</span>"
    ]
  },
  {
    "objectID": "04_estDistParams.html#sec-cop-kmeans",
    "href": "04_estDistParams.html#sec-cop-kmeans",
    "title": "4  Estimate Distribution Parameters",
    "section": "",
    "text": "Tip\n\n\n\nTo use this algorithm, we only need to know (1) whether this participant is diseased; and (2) each biomarker measurement.\n\n\n\n\nThere are two, and only two clusters.\nEach clustes has more than one element (This is to make sure that the standard deviation of this biomarker’s theta or phi is non-zero)\n\n\n\n\nWe use hierarchical clustering first. If the two above mentioned requirements are not met, then\nWe try hard K-Means multiple times; If the two above mentioned requirements are still not met, then\nWe group the measurements into two random clusters; If the two above mentioned requirements are still not met, then raise an error and stop.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimate Distribution Parameters</span>"
    ]
  },
  {
    "objectID": "04_estDistParams.html#sec-conjugate-priors",
    "href": "04_estDistParams.html#sec-conjugate-priors",
    "title": "4  Estimate Distribution Parameters",
    "section": "4.2 Conjugate Priors",
    "text": "4.2 Conjugate Priors\nThe second method we may utilize is conjugate priors. Conjugacy occurs when the posterior distribution is in the same family of distribution as the prior distribution, but with new parameter values.\nWhy conjugacy is important? Because without it, one has to do the integration, which oftentimes is hard.\nThree major conjugate families:\n\nBeta-Binomial\nGamma-Poisson\nNormal-Normal\n\nIn our example, we assume that the measurement data for each biomarker follows a normal distribution; however, we do not know the exact \\(\\mu\\) and \\(\\sigma\\). Our job is to estimate the two parameters for each biomarker based on the data we have.\nAccording to An Introduction to Bayesian Thinking by Clyde et al. (2022), if the data comes from a normal distribution with unknown \\(\\mu\\) and \\(\\sigma\\), the conjugate prior for \\(\\mu\\) has a normal distribution with mean \\(m_0\\) and variance \\(\\frac{\\sigma^2}{n_0}\\). The conjugate prior for \\(\\frac{1}{\\sigma^2}\\) has a Gamma distribution with shape \\(\\frac{v_0}{2}\\) and rate \\(\\frac{v_0 s_0^{2}}{2}\\) where\n\n\\(m_0\\): prior estimate of \\(\\mu\\).\n\\(n_0\\): how strongly is the prior belief in \\(m_0\\) is held.\n\\(s_0^2\\): prior estimate of \\(\\sigma^2\\).\n\\(v_0\\): prior degress of freedome, influencing the certainty of \\(s_0^2\\).\n\nThat is to say:\n\\[\\mu | \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2/n_0)\\]\n\\[1/\\sigma^2 \\sim Gamma\\left(\\frac{v_0}{2}, \\frac{v_0 s_0^2}{2} \\right)\\]\nCombined, we have:\n\\[(\\mu, 1/\\sigma^2) \\sim NormalGamma(m_0, n_0, s_0^2, v_0)\\]\nThe posterior also follows a Normal-Gamma distribution:\n\\[(\\mu, 1/\\sigma^2) | data \\sim NormalGamma(m_n, n_n, s_n^2, v_n)\\]\nMore specifically\n\\[1/\\sigma^2 | data \\sim Gamma(v_n/2, s_n^2 v_n/2)\\]\n\\[\\mu | data, \\sigma^2 \\sim \\mathcal{N}(m_n, \\sigma^2/n_n)\\]\nBased on the above two equations, we know that the mean of posterior mean is \\(m_n\\) and the mean of the posterior variance is \\((s_n^2 v_n/2)/(v_n/2)\\). This is beceause the expected value of \\(Gamma(\\alpha, \\beta)\\) is \\(\\frac{\\alpha}{\\beta}\\).\nwhere\n\n\\(m_n\\): posterior mean, mode, and median for \\(\\mu\\)\n\\(n_n\\): posterior sample size\n\\(s_n^2\\): posterior variance\n\\(v_n\\): posterior degrees of freedome\n\nThe updating rules to get the new hyper-parameters:\n\n\\[m_n = \\frac{n}{n+n_0} \\bar{y} + \\frac{n_0}{n+n_0}m_0\\]\n\\[n_n = n_0 + n\\]\n\\[v_n = v_0 + n\\]\n\\[s_n^2 = \\frac{1}{v_n}\\left[s^2(n-1) + s_0^2v_0 + \\frac{n_0n}{n_n}(\\bar{y}-m_0)^2\\right]\\]\nwhere\n\n\\(n\\): sample size\n\\(\\bar{y}\\): sample mean\n\\(s^2\\): sample variance\n\n\n\n\n\n\n\nTip\n\n\n\nTo apply the algorithm of conjugate priors, we assume we already know \\(S\\) and \\(k_j\\), alongside biomarker measurement (\\(X_{nj}\\)). Based on \\(S\\) and \\(k_j\\), we can infer whether a biomarker is affected by the disease or not.\n\n\n\n\nCode\ndef estimate_params_exact(m0, n0, s0_sq, v0, data):\n    '''This is to estimate means and vars based on conjugate priors\n    Inputs:\n        - data: a vector of measurements \n        - m0: prior estimate of $\\mu$.\n        - n0: how strongly is the prior belief in $m_0$ is held.\n        - s0_sq: prior estimate of $\\sigma^2$.\n        - v0: prior degress of freedome, influencing the certainty of $s_0^2$.\n\n    Outputs:\n        - mu estiate, std estimate\n    '''\n    # Data summary\n    sample_mean = np.mean(data)\n    sample_size = len(data)\n    sample_var = np.var(data, ddof=1)  # ddof=1 for unbiased estimator\n\n    # Update hyperparameters for the Normal-Inverse Gamma posterior\n    updated_m0 = (n0 * m0 + sample_size * sample_mean) / (n0 + sample_size)\n    updated_n0 = n0 + sample_size\n    updated_v0 = v0 + sample_size\n    updated_s0_sq = (1 / updated_v0) * ((sample_size - 1) * sample_var + v0 * s0_sq +\n                                        (n0 * sample_size / updated_n0) * (sample_mean - m0)**2)\n    updated_alpha = updated_v0/2\n    updated_beta = updated_v0*updated_s0_sq/2\n\n    # Posterior estimates\n    mu_posterior_mean = updated_m0\n    sigma_squared_posterior_mean = updated_beta/updated_alpha\n\n    mu_estimation = mu_posterior_mean\n    std_estimation = np.sqrt(sigma_squared_posterior_mean)\n\n    return mu_estimation, std_estimation\n\ndef get_theta_phi_conjugate_priors(biomarkers, data_we_have, theta_phi_kmeans):\n    '''To get estimated parameters, returns a hashmap\n    Input:\n    - biomarkers: biomarkers \n    - data_we_have: participants data filled with initial or updated participant_stages\n    - theta_phi_kmeans: a hashmap of dicts, which are the prior theta and phi values\n        obtained from the initial simple clustering algorithm\n\n    Output: \n    - a hashmap of dictionaries. Key is biomarker name and value is a dictionary.\n    Each dictionary contains the theta and phi mean/std values for a specific biomarker. \n    '''\n    # empty list of dictionaries to store the estimates\n    hashmap_of_means_stds_estimate_dicts = {}\n\n    for biomarker in biomarkers:\n        # Initialize dictionary outside the inner loop\n        dic = {'biomarker': biomarker}\n        for affected in ['affected', 'not_affected']:\n            data_full = data_we_have[(data_we_have.biomarker == biomarker) & (\n                data_we_have.affected_or_not == affected)]\n            if len(data_full) &gt; 1:\n                measurements = data_full.measurement\n                s0_sq = np.var(measurements, ddof=1)\n                m0 = np.mean(measurements)\n                mu_estimate, std_estimate = estimate_params_exact(\n                    m0=m0, n0=1, s0_sq=s0_sq, v0=1, data=measurements)\n                if affected == 'affected':\n                    dic['theta_mean'] = mu_estimate\n                    dic['theta_std'] = std_estimate\n                else:\n                    dic['phi_mean'] = mu_estimate\n                    dic['phi_std'] = std_estimate\n            # If there is only one observation or not observation at all, resort to theta_phi_kmeans\n            # YES, IT IS POSSIBLE THAT DATA_FULL HERE IS NULL\n            # For example, if a biomarker indicates stage of (num_biomarkers), but all participants' stages\n            # are smaller than that stage; so that for all participants, this biomarker is not affected\n            else:\n                print('not enough data here, so we have to use theta phi estimates from simple clustering')\n                # print(theta_phi_kmeans)\n                if affected == 'affected':\n                    dic['theta_mean'] = theta_phi_kmeans[biomarker]['theta_mean']\n                    dic['theta_std'] = theta_phi_kmeans[biomarker]['theta_std']\n                else:\n                    dic['phi_mean'] = theta_phi_kmeans[biomarker]['phi_mean']\n                    dic['phi_std'] = theta_phi_kmeans[biomarker]['phi_std']\n        # print(f\"biomarker {biomarker} done!\")\n        hashmap_of_means_stds_estimate_dicts[biomarker] = dic\n    return hashmap_of_means_stds_estimate_dicts\n\n\n\nconjugate_prior_theta_phi = get_theta_phi_conjugate_priors(\n    biomarkers = biomarkers, \n    data_we_have = df, \n    theta_phi_kmeans = simple_clustering_estimates\n)\ncp_df = pd.DataFrame.from_dict(conjugate_prior_theta_phi, orient='index')\ncp_df.reset_index(drop=True, inplace=True)\ncp_df\n\n\n\n\n\n\n\n\nbiomarker\ntheta_mean\ntheta_std\nphi_mean\nphi_std\n\n\n\n\n0\nHIP-FCI\n-5.378366\n7.233991\n5.092800\n1.514402\n\n\n1\nPCC-FCI\n5.521792\n2.777207\n12.071769\n3.671679\n\n\n2\nAB\n151.143708\n14.806694\n251.973564\n51.382188\n\n\n3\nP-Tau\n-41.768257\n34.857945\n-24.739527\n14.928907\n\n\n4\nMMSE\n23.122406\n2.446874\n28.049683\n0.718493\n\n\n5\nADAS\n-19.633304\n4.582900\n-5.902198\n1.278311\n\n\n6\nHIP-GMI\n0.425625\n0.272876\n0.379542\n0.235348\n\n\n7\nAVLT-Sum\n21.664360\n3.755735\n40.700638\n14.480463\n\n\n8\nFUS-GMI\n0.482745\n0.055585\n0.590434\n0.063730\n\n\n9\nFUS-FCI\n-18.566905\n5.781937\n-9.648705\n3.099195\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we estimate \\(\\theta\\) and \\(\\phi\\) using conjugate priors, we need to use the result from simple clustering as a fall back because it is possible that for a specific biomarker, either the affected or the not_affected group is empty. If that is the case, we are not able to estimate relevant parameters and have to resort to the fallback result.\n\n\n\nmake_chart(\n    biomarkers[0:4], \n    cp_df, \n    truth_df, \n    title = \"Comparing Theta and Phi Distributions Using Conjugate Priors\"\n)\n\n\n\n\n\n\n\nFigure 4.2: Comparing Theta and Phi Distributions Using Conjugate Prior",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimate Distribution Parameters</span>"
    ]
  },
  {
    "objectID": "04_estDistParams.html#sec-soft-kmeans",
    "href": "04_estDistParams.html#sec-soft-kmeans",
    "title": "4  Estimate Distribution Parameters",
    "section": "4.3 Soft K-Means",
    "text": "4.3 Soft K-Means\nConjugate Priors assumes we know \\(k_j\\), which often times is not already known. Our simple clustering algorithm is only taking advantage of \\(X_{nj}\\) and whether participants are diseased or not, leaving \\(S\\), which is known to us, unexploited.\nSoft K-Means is a good alternative to these two because it utilizes \\(S\\) while at the same time not assuming we know \\(k_j\\).\nThe logic of soft-kmeans is this;\n\nIf a participant is diseased, we iterate through all possible disease stages, and calculate the associated likelihood using Equation 2.1. We then normalize these likelihoods to obtain the estimated probability of this participant being at each stage. For example, if there are three possible stages, and the associated likelihoods are [1, 3, 6], then the normalized likelihoods would be [0.1, 0.3, 0.6].\n\n\n\n\n\n\n\nTip\n\n\n\nYou may wonder how we can use Equation 2.1 when we do not know \\(\\theta\\) and \\(\\phi\\) yet (which is exactly what we are trying to do!). If you notice this, it is a very keen observation!.\nIf fact, we are going to use the estimated \\(\\theta\\) and \\(\\phi\\) we obtained above using simple clustering.\n\n\n\nFor each biomarker \\(n\\), we obtain \\(S_n\\) based on \\(S\\). Then we iterate through all participants. If this participant is healthy, we include their biomarker measurement in cluster_phi. If this participant is diseased, we compare between \\(P_{\\theta}\\) and \\(P_{\\phi}\\). If \\(S_n = 2\\), then \\(P_{\\theta} = 0.1 + 0.3 = 0.4\\) and \\(P_{\\phi} = 0.6\\). Because \\(P_{\\phi}\\) is larger, we include this participant’s biomarker measurement in cluster_phi. When the iteration through participants is done, we can calculate the mean and standard deviation of each cluster.\n\n\n\n\n\n\n\nTip\n\n\n\nIf \\(P_{\\theta} =  P_{\\phi}\\), we randomly assign this participant’s biomarker measurement to a cluster.\n\n\n\n\nCode\ndef compute_single_measurement_likelihood(theta_phi, biomarker, affected, measurement):\n    '''Computes the likelihood of the measurement value of a single biomarker\n\n    We know the normal distribution defined by either theta or phi\n    and we know the measurement. This will give us the probability\n    of this given measurement value. \n\n    input:\n    - theta_phi: the dictionary containing theta and phi values for each biomarker\n    - biomarker: an integer between 0 and 9 \n    - affected: boolean \n    - measurement: the observed value for a biomarker in a specific participant\n\n    output: a scalar\n    '''\n    biomarker_dict = theta_phi[biomarker]\n    mu = biomarker_dict['theta_mean'] if affected else biomarker_dict['phi_mean']\n    std = biomarker_dict['theta_std'] if affected else biomarker_dict['phi_std']\n    var = std**2\n    if var &lt;= int(0) or np.isnan(measurement) or np.isnan(mu):\n        print(f\"Invalid values: measurement: {measurement}, mu: {mu}, var: {var}\")\n        likelihood = np.exp(-(measurement - mu)**2 /\n                            (2 * var)) / np.sqrt(2 * np.pi * var)\n    else:\n        likelihood = np.exp(-(measurement - mu)**2 /\n                            (2 * var)) / np.sqrt(2 * np.pi * var)\n    return likelihood\n\ndef fill_up_kj_and_affected(pdata, k_j):\n    '''Fill up a single participant's data using k_j; basically add two columns: \n    k_j and affected\n    Note that this function assumes that pdata already has the S_n column\n\n    Input:\n    - pdata: a dataframe of ten biomarker values for a specific participant \n    - k_j: a scalar\n    '''\n    data = pdata.copy()\n    data['k_j'] = k_j\n    data['affected'] = data.apply(lambda row: row.k_j &gt;= row.S_n, axis=1)\n    return data\n\ndef compute_likelihood(pdata, k_j, theta_phi):\n    '''\n    This function computes the likelihood of seeing this sequence of biomarker values \n    for a specific participant, assuming that this participant is at stage k_j\n    '''\n    data = fill_up_kj_and_affected(pdata, k_j)\n    likelihood = 1\n    for i, row in data.iterrows():\n        biomarker = row['biomarker']\n        measurement = row['measurement']\n        affected = row['affected']\n        likelihood *= compute_single_measurement_likelihood(\n            theta_phi, biomarker, affected, measurement)\n    return likelihood\n\ndef obtain_participants_hashmap(\n        data, \n        prior_theta_phi_estimates,\n):\n    \"\"\"\n    Input:\n        - data: a pd.dataframe. For exrample, 150|200_3.csv\n        - prior_theta_phi_estimates, a hashmap of dicts. \n            This is the result from simple clustering \n    \n    Output: \n        - hashmap: a dictionary whose key is participant id\n            and value value is a dict whose key is stage \n            and value is normalized likelihood\n    \"\"\"\n    # initialize hashmap_of_normalized_stage_likelihood_dicts\n    participants_hashmap = {}\n    non_diseased_participants = data[\n        data.diseased == False]['participant'].unique()\n    disease_stages = data.S_n.unique()\n    for p in data.participant.unique():\n        dic = defaultdict(int)\n        pdata = data[data.participant == p].reset_index(drop = True)\n        if p in non_diseased_participants:\n            dic[0] = 1\n        else:\n            for k_j in disease_stages:\n                kj_ll = compute_likelihood(pdata, k_j, prior_theta_phi_estimates)\n                dic[k_j] = kj_ll\n            # likelihood sum\n            sum_ll = sum(dic.values())\n            epsilon = 1e-10\n            if sum_ll == 0:\n                sum_ll = epsilon\n            normalized_lls = [l/sum_ll for l in dic.values()]\n            normalized_ll_dict = dict(zip(disease_stages, normalized_lls))\n            participants_hashmap[p] = normalized_ll_dict\n    return participants_hashmap \n\ndef calc_soft_kmeans_for_biomarker(\n        data,\n        biomarker,\n        participants_hashmap\n\n):\n    \"\"\"obtain theta, phi estimates using soft kmeans for a single biomarker\n    Inputs:\n        - data: a pd.dataframe. For example, 150|200_3.csv\n        - biomarker: a str, a certain biomarker name\n        - hashmap: a dict, returned result of obtain_hashmap()\n    Outputs:\n        - theta_mean, theta_std, phi_mean, phi_std, a tuple of floats\n    \"\"\"\n    non_diseased_participants = data[\n        data.diseased == False]['participant'].unique()\n    disease_stages = data.S_n.unique()\n     # DataFrame for this biomarker\n    biomarker_df = data[\n        data['biomarker'] == biomarker].reset_index(\n            drop=True).sort_values(\n                by = 'participant', ascending = True)\n    # Extract measurements\n    measurements = np.array(biomarker_df['measurement'])\n\n    this_biomarker_order = biomarker_df.S_n[0]\n\n    affected_cluster = []\n    non_affected_cluster = []\n\n    for p in data.participant.unique():\n        if p in non_diseased_participants:\n            non_affected_cluster.append(measurements[p])\n        else:\n            normalized_ll_dict = participants_hashmap[p]\n            affected_prob = sum(\n                normalized_ll_dict[\n                    kj] for kj in disease_stages if kj &gt;= this_biomarker_order)\n            non_affected_prob = sum(\n                normalized_ll_dict[\n                    kj] for kj in disease_stages if kj &lt; this_biomarker_order)\n            if affected_prob &gt; non_affected_prob:\n                    affected_cluster.append(measurements[p])\n            elif affected_prob &lt; non_affected_prob:\n                non_affected_cluster.append(measurements[p])\n            else:\n                # Assign to either cluster randomly if probabilities are equal\n                if np.random.random() &gt; 0.5:\n                    affected_cluster.append(measurements[p])\n                else:\n                    non_affected_cluster.append(measurements[p])\n    # Compute means and standard deviations\n    theta_mean = np.mean(affected_cluster) if affected_cluster else np.nan\n    theta_std = np.std(affected_cluster) if affected_cluster else np.nan\n    phi_mean = np.mean(\n        non_affected_cluster) if non_affected_cluster else np.nan\n    phi_std = np.std(non_affected_cluster) if non_affected_cluster else np.nan\n    return theta_mean, theta_std, phi_mean, phi_std\n\ndef cal_soft_kmeans_for_biomarkers(\n        data,\n        participants_hashmap,\n        prior_theta_phi_estimates,\n):\n    soft_kmeans_estimates = {}\n    biomarkers = data.biomarker.unique()\n    for biomarker in biomarkers:\n        dic = {'biomarker': biomarker}\n        prior = prior_theta_phi_estimates[biomarker]\n        theta_mean, theta_std, phi_mean, phi_std = calc_soft_kmeans_for_biomarker(\n            data, biomarker, participants_hashmap\n        )\n        if theta_std == 0 or math.isnan(theta_std):\n            theta_mean = prior['theta_mean']\n            theta_std = prior['theta_std']\n        if phi_std == 0 or math.isnan(phi_std):\n            phi_mean = prior['phi_mean']\n            phi_std = prior['phi_std']\n        dic['theta_mean'] = theta_mean\n        dic['theta_std'] = theta_std\n        dic['phi_mean'] = phi_mean\n        dic['phi_std'] = phi_std\n        soft_kmeans_estimates[biomarker] = dic\n    return soft_kmeans_estimates\n\n\n\nparticipants_hashmap = obtain_participants_hashmap(\n    data = df, \n    prior_theta_phi_estimates = cop_kmeans_estimates,\n)\n\nsoft_kmeans_estimates = cal_soft_kmeans_for_biomarkers(\n        data = df,\n        participants_hashmap = participants_hashmap,\n        prior_theta_phi_estimates = cop_kmeans_estimates,\n)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[26], line 3\n      1 participants_hashmap = obtain_participants_hashmap(\n      2     data = df, \n----&gt; 3     prior_theta_phi_estimates = cop_kmeans_estimates,\n      4 )\n      6 soft_kmeans_estimates = cal_soft_kmeans_for_biomarkers(\n      7         data = df,\n      8         participants_hashmap = participants_hashmap,\n      9         prior_theta_phi_estimates = cop_kmeans_estimates,\n     10 )\n\nNameError: name 'cop_kmeans_estimates' is not defined\n\n\n\n\nsoft_kmeans_estimates_df = pd.DataFrame.from_dict(\n    soft_kmeans_estimates, orient='index')\nsoft_kmeans_estimates_df.reset_index(drop=True, inplace=True)\nsoft_kmeans_estimates_df\n\n\n\n\n\n\n\n\nbiomarker\ntheta_mean\ntheta_std\nphi_mean\nphi_std\n\n\n\n\n0\nHIP-FCI\n-5.378366\n7.232544\n5.092800\n1.514369\n\n\n1\nPCC-FCI\n10.397606\n3.698129\n10.572261\n4.472485\n\n\n2\nAB\n157.514640\n19.654237\n232.546088\n61.426283\n\n\n3\nP-Tau\n-61.280491\n21.704315\n-27.032696\n20.645375\n\n\n4\nMMSE\n21.478486\n0.869996\n27.414771\n1.956947\n\n\n5\nADAS\n-23.386222\n3.249392\n-7.343264\n4.453207\n\n\n6\nHIP-GMI\n0.114735\n0.106857\n0.385654\n0.240309\n\n\n7\nAVLT-Sum\n60.569081\n6.389012\n39.177736\n14.855928\n\n\n8\nFUS-GMI\n0.478209\n0.041763\n0.584511\n0.067892\n\n\n9\nFUS-FCI\n-19.200644\n4.688806\n-10.050024\n3.751844\n\n\n\n\n\n\n\n\nmake_chart(\n    biomarkers[0:4], \n    soft_kmeans_estimates_df, \n    truth_df, \n    title = \"Comparing Theta and Phi Distributions Using Soft K-Means\"\n)\n\n\n\n\n\n\n\nFigure 4.3: Comparing Theta and Phi Distributions Using Soft K-Mean",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimate Distribution Parameters</span>"
    ]
  },
  {
    "objectID": "04_estDistParams.html#conclusion",
    "href": "04_estDistParams.html#conclusion",
    "title": "4  Estimate Distribution Parameters",
    "section": "4.4 Conclusion",
    "text": "4.4 Conclusion\nWe compare the above three methods. Simple clustering has the least number of prerequisites: it only needs to know whether participants are healthy or not and biomarker measurements. However, the drawback is that it might not be very accurate. Conjugate priors are extremely accurate; however, it requires knowledge of almost everything: besides what is required by simple clustering, it also requires \\(S\\) and \\(k_j\\). Soft k-kmeans does not require the knowledge of \\(k_j\\) and is an improvement over simple clustering.\nWe also noticed that both conjugate priors and soft k-means need to use the result from simple clustering as a fallback.\n\n\n\n\nBabaki, Behrouz. 2017. “COP-Kmeans Version 1.5.” https://doi.org/10.5281/zenodo.831850.\n\n\nClyde, M, M Cetinkaya-Rundel, C Rundel, D Banks, C Chai, and L Huang. 2022. “An Introduction to Bayesian Thinking: A Companion to the Statistics with r Course. 2020.” https://statswithr.github.io/book/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimate Distribution Parameters</span>"
    ]
  },
  {
    "objectID": "05_estStages.html",
    "href": "05_estStages.html",
    "title": "5  Estimate Participant Stages",
    "section": "",
    "text": "5.1 Challenge\nSuppos we know \\(S, \\theta, \\phi\\). How could we estimate participant stages?\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport json \nfrom collections import Counter\nThis is the data we have. And we want to know fill the missing column of k_j.\noutput_dir = 'data'\ndf = pd.read_csv(f\"{output_dir}/100|200_3.csv\")\nreal_stages_dic = dict(zip(df.participant, df.k_j))\ndf.drop(['k_j', 'affected_or_not'], axis = 1, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nparticipant\nbiomarker\nmeasurement\nS_n\ndiseased\n\n\n\n\n0\n0\nHIP-FCI\n-8.908479\n1\nTrue\n\n\n1\n1\nHIP-FCI\n-1.095464\n1\nFalse\n\n\n2\n2\nHIP-FCI\n0.470754\n1\nTrue\n\n\n3\n3\nHIP-FCI\n2.633455\n1\nTrue\n\n\n4\n4\nHIP-FCI\n4.070208\n1\nFalse",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimate Participant Stages</span>"
    ]
  },
  {
    "objectID": "05_estStages.html#solution",
    "href": "05_estStages.html#solution",
    "title": "5  Estimate Participant Stages",
    "section": "5.2 Solution",
    "text": "5.2 Solution\nOne possible solution looks like this:\n\nFor each diseased participant, we iterate through all possible disease stages and calculate the likelihood using Equation 2.1.\nWe normalize all the likelihoods, construct an array, and randomly sample one possible stage according to that array.\nRun multiple times, for each diseased participant, the mode of the sampled stages will be their stage.\n\n\n\nCode\ndef compute_single_measurement_likelihood(\n        theta_phi, \n        biomarker, \n        affected, \n        measurement):\n    '''Computes the likelihood of the measurement value of a single biomarker\n\n    We know the normal distribution defined by either theta or phi\n    and we know the measurement. This will give us the probability\n    of this given measurement value. \n\n    input:\n    - theta_phi: the dictionary containing theta and phi values for each biomarker\n    - biomarker: an integer between 0 and 9 \n    - affected: boolean \n    - measurement: the observed value for a biomarker in a specific participant\n\n    output: a scalar\n    '''\n    biomarker_dict = theta_phi[biomarker]\n    mu = biomarker_dict['theta_mean'] if affected else biomarker_dict['phi_mean']\n    std = biomarker_dict['theta_std'] if affected else biomarker_dict['phi_std']\n    var = std**2\n    if var &lt;= int(0) or np.isnan(measurement) or np.isnan(mu):\n        print(f\"Invalid values: measurement: {measurement}, mu: {mu}, var: {var}\")\n        likelihood = np.exp(-(measurement - mu)**2 /\n                            (2 * var)) / np.sqrt(2 * np.pi * var)\n    else:\n        likelihood = np.exp(-(measurement - mu)**2 /\n                            (2 * var)) / np.sqrt(2 * np.pi * var)\n    return likelihood\n\ndef fill_up_kj_and_affected(pdata, k_j):\n    '''Fill up a single participant's data using k_j; basically add two columns: \n    k_j and affected\n    Note that this function assumes that pdata already has the S_n column\n\n    Input:\n    - pdata: a dataframe of ten biomarker values for a specific participant \n    - k_j: a scalar\n    '''\n    data = pdata.copy()\n    data['k_j'] = k_j\n    data['affected'] = data.apply(lambda row: row.k_j &gt;= row.S_n, axis=1)\n    return data\n\ndef compute_likelihood(pdata, k_j, theta_phi):\n    '''\n    This function computes the likelihood of seeing this sequence of biomarker values \n    for a specific participant, assuming that this participant is at stage k_j\n    '''\n    data = fill_up_kj_and_affected(pdata, k_j)\n    likelihood = 1\n    for i, row in data.iterrows():\n        biomarker = row['biomarker']\n        measurement = row['measurement']\n        affected = row['affected']\n        likelihood *= compute_single_measurement_likelihood(\n            theta_phi, biomarker, affected, measurement)\n    return likelihood\n\n\nWe first look at the known \\(\\theta, \\phi\\):\n\nwith open('files/real_theta_phi.json', 'r') as f:\n    truth = json.load(f)\ntruth_df = pd.DataFrame.from_dict(truth, orient='index')\ntruth_df.reset_index(names = 'biomarker', inplace=True)\ntruth_df\n\n\n\n\n\n\n\n\nbiomarker\ntheta_mean\ntheta_std\nphi_mean\nphi_std\n\n\n\n\n0\nMMSE\n22.0\n2.666667\n28.0\n0.666667\n\n\n1\nADAS\n-20.0\n4.000000\n-6.0\n1.333333\n\n\n2\nAB\n150.0\n16.666667\n250.0\n50.000000\n\n\n3\nP-Tau\n-50.0\n33.333333\n-25.0\n16.666667\n\n\n4\nHIP-FCI\n-5.0\n6.666667\n5.0\n1.666667\n\n\n5\nHIP-GMI\n0.3\n0.333333\n0.4\n0.233333\n\n\n6\nAVLT-Sum\n20.0\n6.666667\n40.0\n15.000000\n\n\n7\nPCC-FCI\n5.0\n3.333333\n12.0\n4.000000\n\n\n8\nFUS-GMI\n0.5\n0.066667\n0.6\n0.066667\n\n\n9\nFUS-FCI\n-20.0\n6.000000\n-10.0\n3.333333",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimate Participant Stages</span>"
    ]
  },
  {
    "objectID": "05_estStages.html#implementation",
    "href": "05_estStages.html#implementation",
    "title": "5  Estimate Participant Stages",
    "section": "5.3 Implementation",
    "text": "5.3 Implementation\nWe then implement the algorithm mentioned above:\n\ntheta_phi_estimates = truth.copy()\ndisease_stages = df.S_n.unique()\ndiseased_participants = df[df.diseased==True]['participant'].unique()\n\n\ndef update_participant_stages_dic(\n        data,\n        p,\n        disease_stages,\n        theta_phi_estimates,\n        # participant stage dic:\n        psdic,\n        sample_iterations = 20\n):\n    \"\"\"\n    Inputs:\n        - data: pd.dataframe, e.g., 100|200_3.csv\n        - p: int\n        - disease_stages: a list of integers\n        - theta_phi_estimates: a hashmap of dictionaries\n        - psdic: a dictionary\n        - sample_iteration: int. How many times we sample \n    Output:\n        no outputs. Simply update psdic\n    \"\"\"\n    pdata = data[data.participant == p]\n    stage_likelihood_dict = {}\n    for k_j in disease_stages:\n        kj_likelihood = compute_likelihood(\n            pdata, k_j, theta_phi_estimates)\n        # update each stage likelihood for this participant\n        stage_likelihood_dict[k_j] = kj_likelihood\n    # Add a small epsilon to avoid division by zero\n    likelihood_sum = sum(stage_likelihood_dict.values())\n    epsilon = 1e-10\n    if likelihood_sum == 0:\n        # print(\"Invalid likelihood_sum: zero encountered.\")\n        likelihood_sum = epsilon  # Handle the case accordingly\n    normalized_stage_likelihood = [\n        l/likelihood_sum for l in stage_likelihood_dict.values()]\n    sampled_stages = np.random.choice(\n        disease_stages, \n        size = sample_iterations, \n        p=normalized_stage_likelihood, \n        replace=True\n    )\n    mode_result = Counter(sampled_stages).most_common(1)[0][0]\n    psdic[p] = mode_result\n\n\nparticipants = df.participant.unique()\npsdic = {}\nfor p in participants:\n    if p not in diseased_participants:\n        psdic[p] = 0\n    else:\n        update_participant_stages_dic(\n            df,\n            p,\n            disease_stages,\n            theta_phi_estimates,\n            # participant stage dic:\n            psdic,\n            sample_iterations = 10\n    )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimate Participant Stages</span>"
    ]
  },
  {
    "objectID": "05_estStages.html#result",
    "href": "05_estStages.html#result",
    "title": "5  Estimate Participant Stages",
    "section": "5.4 Result",
    "text": "5.4 Result\nThen we compare our results with the actual participants’ stages:\n\ndiff = np.array(list(psdic.values())) - np.array(list(real_stages_dic.values()))\n\n\n\nCode\ndef scatter_plot_of_stage_differences(stage_differences):\n    '''Scatter Plot of the Difference at each index\n    Input:\n    - stage_differences: estimated_stages - actual stages. Result should be a 1-dim np array\n    '''\n    plt.figure(figsize=(10, 6))\n    plt.scatter(range(len(diff)), stage_differences, alpha=0.6)\n    plt.axhline(y=0, color='red', linestyle='--')\n    plt.title(\"Scatter Plot of Stage Difference for Each Participant\")\n    plt.xlabel(\"Participant\")\n    plt.ylabel(\"Difference (Estimated Stage - True Stage)\")\n    plt.grid(True)\n    plt.show()\n\n\n\nscatter_plot_of_stage_differences(diff)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimate Participant Stages</span>"
    ]
  },
  {
    "objectID": "05_estStages.html#discussion",
    "href": "05_estStages.html#discussion",
    "title": "5  Estimate Participant Stages",
    "section": "5.5 Discussion",
    "text": "5.5 Discussion\nFrom the above result, we can see how challenging it is to accurately estimate participant stages, even if we know exactly the \\(\\theta\\) and \\(\\phi\\).\n\n\n\n\n\n\nTip\n\n\n\nWhat if we know only \\(S\\), but not \\(\\theta\\) nor \\(\\phi\\)?\nThe first step for us is to estimate \\(\\theta, \\phi\\) and then follow the above procedures. To do that, refer back to Chapter 4.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimate Participant Stages</span>"
    ]
  },
  {
    "objectID": "06_estS.html",
    "href": "06_estS.html",
    "title": "6  Estimate S",
    "section": "",
    "text": "Now we come to the core part of our project: how can we estimate \\(S\\), without knowing \\(\\theta, \\phi, k_j\\)?\nBasically, this is the data we have:\n\nimport pandas as pd \noutput_dir = 'data'\ndf = pd.read_csv(f\"{output_dir}/150|200_3.csv\").drop(\n    ['k_j', 'S_n', 'affected_or_not'], axis = 1)\ndf.head()\n\n\n\n\n\n\n\n\nparticipant\nbiomarker\nmeasurement\ndiseased\n\n\n\n\n0\n0\nHIP-FCI\n3.135981\nFalse\n\n\n1\n1\nHIP-FCI\n12.593704\nTrue\n\n\n2\n2\nHIP-FCI\n6.220776\nFalse\n\n\n3\n3\nHIP-FCI\n3.545100\nFalse\n\n\n4\n4\nHIP-FCI\n3.966541\nFalse\n\n\n\n\n\n\n\nThe main idea is this:\n\nWe need to know \\(\\theta, \\phi\\) first before we can estimate likelihoods. To estimate \\(\\theta, \\phi\\), there are three approaches covered in Chapter 4.\nWe try many different \\(S\\) and calculate its associated likelihood. We either accept or reject this \\(S\\) according to Metropolis–Hastings algorithm.\n\nIn the following, We will cover three different approaches to estimate \\(S\\):\n\nConstrained K-Means\nConjugate Priors\nSoft K-Means",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimate S</span>"
    ]
  },
  {
    "objectID": "07_simpleClustering.html",
    "href": "07_simpleClustering.html",
    "title": "7  Estimate \\(S\\) with Simple Clustering",
    "section": "",
    "text": "7.1 Implementation\nimport numpy as np \nimport utils \nimport json \nimport pandas as pd \nimport utils \nfrom scipy.stats import kendalltau\nimport sys\nimport os\nCode\ndef calculate_all_participant_ln_likelihood(\n        iteration,\n        data_we_have,\n        current_order_dict,\n        n_participants,\n        non_diseased_participant_ids,\n        theta_phi_estimates,\n        diseased_stages,\n):\n    data = data_we_have.copy()\n    data['S_n'] = data.apply(\n        lambda row: current_order_dict[row['biomarker']], axis=1)\n    all_participant_ln_likelihood = 0\n\n    for p in range(n_participants):\n        pdata = data[data.participant == p].reset_index(drop=True)\n        if p in non_diseased_participant_ids:\n            this_participant_likelihood = utils.compute_likelihood(\n                pdata, k_j=0, theta_phi=theta_phi_estimates)\n            this_participant_ln_likelihood = np.log(\n                this_participant_likelihood + 1e-10)\n        else:\n            # normalized_stage_likelihood_dict = None\n            # initiaze stage_likelihood\n            stage_likelihood_dict = {}\n            for k_j in diseased_stages:\n                kj_likelihood = utils.compute_likelihood(\n                    pdata, k_j, theta_phi_estimates)\n                # update each stage likelihood for this participant\n                stage_likelihood_dict[k_j] = kj_likelihood\n            # Add a small epsilon to avoid division by zero\n            likelihood_sum = sum(stage_likelihood_dict.values())\n\n            # calculate weighted average\n            this_participant_likelihood = np.mean(likelihood_sum)\n            this_participant_ln_likelihood = np.log(\n                this_participant_likelihood + 1e-10)\n        all_participant_ln_likelihood += this_participant_ln_likelihood\n    return all_participant_ln_likelihood\n\ndef metropolis_hastings_simple_clustering(\n    data_we_have,\n    iterations,\n    n_shuffle,\n):\n    '''Implement the metropolis-hastings algorithm using simple clustering\n    Inputs: \n        - data: data_we_have\n        - iterations: number of iterations\n        - log_folder_name: the folder where log files locate\n\n    Outputs:\n        - best_order: a numpy array\n        - best_likelihood: a scalar \n    '''\n    n_participants = len(data_we_have.participant.unique())\n    biomarkers = data_we_have.biomarker.unique()\n    n_biomarkers = len(biomarkers)\n    n_stages = n_biomarkers + 1\n    non_diseased_participant_ids = data_we_have.loc[\n        data_we_have.diseased == False].participant.unique()\n    diseased_stages = np.arange(start=1, stop=n_stages, step=1)\n    # obtain the iniial theta and phi estimates\n    theta_phi_estimates = utils.get_theta_phi_estimates(\n        data_we_have)\n\n    # initialize empty lists\n    acceptance_count = 0\n    all_current_accepted_order_dicts = []\n\n    current_accepted_order = np.random.permutation(np.arange(1, n_stages))\n    current_accepted_order_dict = dict(zip(biomarkers, current_accepted_order))\n    current_accepted_likelihood = -np.inf\n\n    for _ in range(iterations):\n        # in each iteration, we have updated current_order_dict and theta_phi_estimates\n\n        new_order = current_accepted_order.copy()\n        utils.shuffle_order(new_order, n_shuffle)\n        current_order_dict = dict(zip(biomarkers, new_order))\n        all_participant_ln_likelihood = calculate_all_participant_ln_likelihood(\n                _,\n                data_we_have,\n                current_order_dict,\n                n_participants,\n                non_diseased_participant_ids,\n                theta_phi_estimates,\n                diseased_stages,\n            )\n\n\n        # Log-Sum-Exp Trick\n        max_likelihood = max(all_participant_ln_likelihood,\n                             current_accepted_likelihood)\n        prob_of_accepting_new_order = np.exp(\n            (all_participant_ln_likelihood - max_likelihood) -\n            (current_accepted_likelihood - max_likelihood)\n        )\n\n        # prob_of_accepting_new_order = np.exp(\n        #     all_participant_ln_likelihood - current_accepted_likelihood)\n\n        # np.exp(a)/np.exp(b) = np.exp(a - b)\n        # if a &gt; b, then np.exp(a - b) &gt; 1\n\n        # it will definitly update at the first iteration\n        if np.random.rand() &lt; prob_of_accepting_new_order:\n            acceptance_count += 1\n            current_accepted_order = new_order\n            current_accepted_likelihood = all_participant_ln_likelihood\n            current_accepted_order_dict = current_order_dict\n\n        acceptance_ratio = acceptance_count*100/(_+1)\n        all_current_accepted_order_dicts.append(current_accepted_order_dict)\n\n        if (_+1) % 10 == 0:\n            formatted_string = (\n                f\"iteration {_ + 1} done, \"\n                f\"current accepted likelihood: {current_accepted_likelihood}, \"\n                f\"current acceptance ratio is {acceptance_ratio:.2f} %, \"\n                f\"current accepted order is {current_accepted_order_dict.values()}, \"\n            )\n            print(formatted_string)\n\n    # print(\"done!\")\n    return all_current_accepted_order_dicts\nn_shuffle = 2\niterations = 10\nburn_in = 2\nthining = 2\n\nbase_dir = os.getcwd()\nprint(f\"Current working directory: {base_dir}\")\ndata_dir = os.path.join(base_dir, \"data\")\n\ncop_kmeans_dir = os.path.join(base_dir, 'simple_clustering')\ntemp_results_dir = os.path.join(cop_kmeans_dir, \"temp_json_results\")\nimg_dir = os.path.join(cop_kmeans_dir, 'img')\nresults_file = os.path.join(cop_kmeans_dir, \"results.json\")\n\nos.makedirs(cop_kmeans_dir, exist_ok=True)\nos.makedirs(temp_results_dir, exist_ok=True)\nos.makedirs(img_dir, exist_ok=True)\n\nprint(f\"Data directory: {data_dir}\")\nprint(f\"Temp results directory: {temp_results_dir}\")\nprint(f\"Image directory: {img_dir}\")\n\nif __name__ == \"__main__\":\n\n    # Read parameters from command line arguments\n    j = 200\n    r = 0.75\n    m = 3\n\n    print(f\"Processing with j={j}, r={r}, m={m}\")\n\n    combstr = f\"{int(j*r)}|{j}\"\n    heatmap_folder = img_dir\n    \n    img_filename = f\"{int(j*r)}-{j}_{m}\"\n    filename = f\"{combstr}_{m}\"\n    data_file = f\"{data_dir}/{filename}.csv\"\n    data_we_have = pd.read_csv(data_file)\n    n_biomarkers = len(data_we_have.biomarker.unique())\n\n    if not os.path.exists(data_file):\n        print(f\"Data file not found: {data_file}\")\n        sys.exit(1)  # Exit early if the file doesn't exist\n    else:\n        print(f\"Data file found: {data_file}\")\n\n    # Define the temporary result file\n    temp_result_file = os.path.join(temp_results_dir, f\"temp_results_{j}_{r}_{m}.json\")\n    \n    dic = {}\n\n    if combstr not in dic:\n        dic[combstr] = []\n\n    accepted_order_dicts = metropolis_hastings_simple_clustering(\n        data_we_have,\n        iterations,\n        n_shuffle,\n    )\n\n    utils.save_heatmap(\n        accepted_order_dicts,\n        burn_in, \n        thining, \n        folder_name=heatmap_folder,\n        file_name=f\"{img_filename}\", \n        title=f'heatmap of {filename}')\n    \n    most_likely_order_dic = utils.obtain_most_likely_order_dic(\n        accepted_order_dicts, burn_in, thining)\n    most_likely_order = list(most_likely_order_dic.values())\n    tau, p_value = kendalltau(most_likely_order, range(1, n_biomarkers + 1))\n    \n    dic[combstr].append(tau)\n    \n    # Write the results to a unique temporary file inside the temp folder\n    with open(temp_result_file, \"w\") as file:\n        json.dump(dic, file, indent=4)\n    print(f\"{filename} is done! Results written to {temp_result_file}\")\n\nCurrent working directory: /Users/hongtaoh/Desktop/github/ebmBook\nData directory: /Users/hongtaoh/Desktop/github/ebmBook/data\nTemp results directory: /Users/hongtaoh/Desktop/github/ebmBook/simple_clustering/temp_json_results\nImage directory: /Users/hongtaoh/Desktop/github/ebmBook/simple_clustering/img\nProcessing with j=200, r=0.75, m=3\nData file found: /Users/hongtaoh/Desktop/github/ebmBook/data/150|200_3.csv\niteration 10 done, current accepted likelihood: -4383.710267251367, current acceptance ratio is 70.00 %, current accepted order is dict_values([2, 9, 3, 8, 5, 4, 6, 1, 10, 7]), \n150|200_3 is done! Results written to /Users/hongtaoh/Desktop/github/ebmBook/simple_clustering/temp_json_results/temp_results_200_0.75_3.json",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimate $S$ with Simple Clustering</span>"
    ]
  },
  {
    "objectID": "07_simpleClustering.html#result",
    "href": "07_simpleClustering.html#result",
    "title": "7  Estimate \\(S\\) with Simple Clustering",
    "section": "7.2 Result",
    "text": "7.2 Result\nWe plot the resulting \\(S\\) probalistically using a heatmap. We also quantify the difference between our result with the real \\(S\\) using Kendall’s Tau. It ranges from \\(-1\\) (completely different) to \\(1\\) (exactly the same). \\(0\\) indicate complete randomness.\n\n\n\n\n\n\nFigure 7.1: Result of Constrained K-Means\n\n\n\n\ndic\n\n{'150|200': [0.1111111111111111]}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimate $S$ with Simple Clustering</span>"
    ]
  },
  {
    "objectID": "08_softKMeans.html",
    "href": "08_softKMeans.html",
    "title": "8  Estimate \\(S\\) with Soft K-Means",
    "section": "",
    "text": "8.1 Implementation\nimport numpy as np \nimport utils \nimport json \nimport pandas as pd \nimport utils \nfrom scipy.stats import kendalltau\nimport sys\nimport os\nimport math\nCode\ndef calculate_soft_kmeans_for_biomarker(\n        data,\n        biomarker,\n        order_dict,\n        n_participants,\n        non_diseased_participants,\n        hashmap_of_normalized_stage_likelihood_dicts,\n        diseased_stages,\n        seed=None\n):\n    \"\"\"\n    Calculate mean and std for both the affected and non-affected clusters for a single biomarker.\n\n    Parameters:\n        data (pd.DataFrame): The data containing measurements.\n        biomarker (str): The biomarker to process.\n        order_dict (dict): Dictionary mapping biomarkers to their order.\n        n_participants (int): Number of participants in the study.\n        non_diseased_participants (list): List of non-diseased participants.\n        hashmap_of_normalized_stage_likelihood_dicts (dict): Hash map of \n            dictionaries containing stage likelihoods for each participant.\n        diseased_stages (list): List of diseased stages.\n        seed (int, optional): Random seed for reproducibility.\n\n    Returns:\n        tuple: Means and standard deviations for affected and non-affected clusters.\n    \"\"\"\n    if seed is not None:\n        # Set the seed for numpy's random number generator\n        rng = np.random.default_rng(seed)\n    else:\n        rng = np.random \n\n    # DataFrame for this biomarker\n    biomarker_df = data[\n        data['biomarker'] == biomarker].reset_index(\n            drop=True).sort_values(\n                by = 'participant', ascending = True)\n    # Extract measurements\n    measurements = np.array(biomarker_df['measurement'])\n\n    this_biomarker_order = order_dict[biomarker]\n\n    affected_cluster = []\n    non_affected_cluster = []\n\n    for p in range(n_participants):\n        if p in non_diseased_participants:\n            non_affected_cluster.append(measurements[p])\n        else:\n            if this_biomarker_order == 1:\n                affected_cluster.append(measurements[p])\n            else:\n                normalized_stage_likelihood_dict = hashmap_of_normalized_stage_likelihood_dicts[\n                    p]\n                # Calculate probabilities for affected and non-affected states\n                affected_prob = sum(\n                    normalized_stage_likelihood_dict[s] for s in diseased_stages if s &gt;= this_biomarker_order\n                )\n                non_affected_prob = sum(\n                    normalized_stage_likelihood_dict[s] for s in diseased_stages if s &lt; this_biomarker_order\n                )\n                if affected_prob &gt; non_affected_prob:\n                    affected_cluster.append(measurements[p])\n                elif affected_prob &lt; non_affected_prob:\n                    non_affected_cluster.append(measurements[p])\n                else:\n                    # Assign to either cluster randomly if probabilities are equal\n                    if rng.random() &gt; 0.5:\n                        affected_cluster.append(measurements[p])\n                    else:\n                        non_affected_cluster.append(measurements[p])\n\n    # Compute means and standard deviations\n    theta_mean = np.mean(affected_cluster) if affected_cluster else np.nan\n    theta_std = np.std(affected_cluster) if affected_cluster else np.nan\n    phi_mean = np.mean(\n        non_affected_cluster) if non_affected_cluster else np.nan\n    phi_std = np.std(non_affected_cluster) if non_affected_cluster else np.nan\n    return theta_mean, theta_std, phi_mean, phi_std\n\ndef soft_kmeans_theta_phi_estimates(\n        iteration,\n        prior_theta_phi_estimates,\n        data_we_have,\n        biomarkers,\n        order_dict,\n        n_participants,\n        non_diseased_participants,\n        hashmap_of_normalized_stage_likelihood_dicts,\n        diseased_stages,\n        seed=None):\n    \"\"\"\n    Get the DataFrame of theta and phi using the soft K-means algorithm for all biomarkers.\n\n    Parameters:\n        data_we_have (pd.DataFrame): DataFrame containing the data.\n        biomarkers (list): List of biomarkers in string.\n        order_dict (dict): Dictionary mapping biomarkers to their order.\n        n_participants (int): Number of participants in the study.\n        non_diseased_participants (list): List of non-diseased participants.\n        hashmap_of_normalized_stage_likelihood_dicts (dict): Hash map of dictionaries containing stage likelihoods for each participant.\n        diseased_stages (list): List of diseased stages.\n        seed (int, optional): Random seed for reproducibility.\n\n    Returns:\n        a dictionary containing the means and standard deviations for theta and phi for each biomarker.\n    \"\"\"\n    # List of dicts to store the estimates\n    # In each dic, key is biomarker, and values are theta and phi params\n    hashmap_of_means_stds_estimate_dicts = {}\n    for biomarker in biomarkers:\n        dic = {'biomarker': biomarker}\n        prior_theta_phi_estimates_biomarker = prior_theta_phi_estimates[biomarker]\n        theta_mean, theta_std, phi_mean, phi_std = calculate_soft_kmeans_for_biomarker(\n            data_we_have,\n            biomarker,\n            order_dict,\n            n_participants,\n            non_diseased_participants,\n            hashmap_of_normalized_stage_likelihood_dicts,\n            diseased_stages,\n            seed\n        )\n        if theta_std == 0 or math.isnan(theta_std):\n            theta_mean = prior_theta_phi_estimates_biomarker['theta_mean']\n            theta_std = prior_theta_phi_estimates_biomarker['theta_std']\n        if phi_std == 0 or math.isnan(phi_std):\n            phi_mean = prior_theta_phi_estimates_biomarker['phi_mean']\n            phi_std = prior_theta_phi_estimates_biomarker['phi_std']\n        dic['theta_mean'] = theta_mean\n        dic['theta_std'] = theta_std\n        dic['phi_mean'] = phi_mean\n        dic['phi_std'] = phi_std\n        hashmap_of_means_stds_estimate_dicts[biomarker] = dic\n    return hashmap_of_means_stds_estimate_dicts\n\ndef calculate_all_participant_ln_likelihood_and_update_hashmap(\n        iteration,\n        data_we_have,\n        current_order_dict,\n        n_participants,\n        non_diseased_participant_ids,\n        theta_phi_estimates,\n        diseased_stages,\n):\n    data = data_we_have.copy()\n    data['S_n'] = data.apply(\n        lambda row: current_order_dict[row['biomarker']], axis=1)\n    all_participant_ln_likelihood = 0\n    # key is participant id\n    # value is normalized_stage_likelihood_dict\n    hashmap_of_normalized_stage_likelihood_dicts = {}\n    for p in range(n_participants):\n        pdata = data[data.participant == p].reset_index(drop=True)\n        if p in non_diseased_participant_ids:\n            this_participant_likelihood = utils.compute_likelihood(\n                pdata, k_j=0, theta_phi=theta_phi_estimates)\n            this_participant_ln_likelihood = np.log(\n                this_participant_likelihood + 1e-10)\n        else:\n            # normalized_stage_likelihood_dict = None\n            # initiaze stage_likelihood\n            stage_likelihood_dict = {}\n            for k_j in diseased_stages:\n                kj_likelihood = utils.compute_likelihood(\n                    pdata, k_j, theta_phi_estimates)\n                # update each stage likelihood for this participant\n                stage_likelihood_dict[k_j] = kj_likelihood\n            # Add a small epsilon to avoid division by zero\n            likelihood_sum = sum(stage_likelihood_dict.values())\n            epsilon = 1e-10\n            if likelihood_sum == 0:\n                # print(\"Invalid likelihood_sum: zero encountered.\")\n                likelihood_sum = epsilon  # Handle the case accordingly\n            normalized_stage_likelihood = [\n                l/likelihood_sum for l in stage_likelihood_dict.values()]\n            normalized_stage_likelihood_dict = dict(\n                zip(diseased_stages, normalized_stage_likelihood))\n            hashmap_of_normalized_stage_likelihood_dicts[p] = normalized_stage_likelihood_dict\n\n            # calculate weighted average\n            this_participant_likelihood = np.mean(likelihood_sum)\n            this_participant_ln_likelihood = np.log(\n                this_participant_likelihood)\n        all_participant_ln_likelihood += this_participant_ln_likelihood\n    return all_participant_ln_likelihood, hashmap_of_normalized_stage_likelihood_dicts\n\n\ndef metropolis_hastings_soft_kmeans(\n    data_we_have,\n    iterations,\n    n_shuffle,\n):\n    '''Implement the metropolis-hastings algorithm using soft kmeans\n    Inputs: \n        - data: data_we_have\n        - iterations: number of iterations\n        - log_folder_name: the folder where log files locate\n\n    Outputs:\n        - best_order: a numpy array\n        - best_likelihood: a scalar \n    '''\n    n_participants = len(data_we_have.participant.unique())\n    biomarkers = data_we_have.biomarker.unique()\n    n_biomarkers = len(biomarkers)\n    n_stages = n_biomarkers + 1\n    non_diseased_participant_ids = data_we_have.loc[\n        data_we_have.diseased == False].participant.unique()\n    diseased_stages = np.arange(start=1, stop=n_stages, step=1)\n    # obtain the iniial theta and phi estimates\n    prior_theta_phi_estimates = utils.get_theta_phi_estimates(\n        data_we_have)\n    theta_phi_estimates = prior_theta_phi_estimates.copy()\n\n    # initialize empty lists\n    acceptance_count = 0\n    all_current_accepted_order_dicts = []\n\n    current_accepted_order = np.random.permutation(np.arange(1, n_stages))\n    current_accepted_order_dict = dict(zip(biomarkers, current_accepted_order))\n    current_accepted_likelihood = -np.inf\n\n    for _ in range(iterations):\n        # in each iteration, we have updated current_order_dict and theta_phi_estimates\n\n        new_order = current_accepted_order.copy()\n        utils.shuffle_order(new_order, n_shuffle)\n        current_order_dict = dict(zip(biomarkers, new_order))\n        all_participant_ln_likelihood, \\\n            hashmap_of_normalized_stage_likelihood_dicts = calculate_all_participant_ln_likelihood_and_update_hashmap(\n                _,\n                data_we_have,\n                current_order_dict,\n                n_participants,\n                non_diseased_participant_ids,\n                theta_phi_estimates,\n                diseased_stages,\n            )\n\n        # Now, update theta_phi_estimates using soft kmeans\n        # based on the updated hashmap of normalized stage likelihood dicts\n        theta_phi_estimates = soft_kmeans_theta_phi_estimates(\n            _,\n            prior_theta_phi_estimates,\n            data_we_have,\n            biomarkers,\n            current_order_dict,\n            n_participants,\n            non_diseased_participant_ids,\n            hashmap_of_normalized_stage_likelihood_dicts,\n            diseased_stages,\n            seed=None,\n        )\n\n        # Log-Sum-Exp Trick\n        max_likelihood = max(all_participant_ln_likelihood,\n                             current_accepted_likelihood)\n        prob_of_accepting_new_order = np.exp(\n            (all_participant_ln_likelihood - max_likelihood) -\n            (current_accepted_likelihood - max_likelihood)\n        )\n\n        # prob_of_accepting_new_order = np.exp(\n        #     all_participant_ln_likelihood - current_accepted_likelihood)\n\n        # np.exp(a)/np.exp(b) = np.exp(a - b)\n        # if a &gt; b, then np.exp(a - b) &gt; 1\n\n        # it will definitly update at the first iteration\n        if np.random.rand() &lt; prob_of_accepting_new_order:\n            acceptance_count += 1\n            current_accepted_order = new_order\n            current_accepted_likelihood = all_participant_ln_likelihood\n            current_accepted_order_dict = current_order_dict\n\n        acceptance_ratio = acceptance_count*100/(_+1)\n        all_current_accepted_order_dicts.append(current_accepted_order_dict)\n\n        if (_+1) % 10 == 0:\n            formatted_string = (\n                f\"iteration {_ + 1} done, \"\n                f\"current accepted likelihood: {current_accepted_likelihood}, \"\n                f\"current acceptance ratio is {acceptance_ratio:.2f} %, \"\n                f\"current accepted order is {current_accepted_order_dict.values()}, \"\n            )\n            print(formatted_string)\n\n    # print(\"done!\")\n    return all_current_accepted_order_dicts\nn_shuffle = 2\niterations = 10\nburn_in = 2\nthining = 2\n\nbase_dir = os.getcwd()\nprint(f\"Current working directory: {base_dir}\")\ndata_dir = os.path.join(base_dir, \"data\")\n\nsoft_kmeans_dir = os.path.join(base_dir, 'soft_kmeans')\ntemp_results_dir = os.path.join(soft_kmeans_dir, \"temp_json_results\")\nimg_dir = os.path.join(soft_kmeans_dir, 'img')\nresults_file = os.path.join(soft_kmeans_dir, \"results.json\")\n\nos.makedirs(soft_kmeans_dir, exist_ok=True)\nos.makedirs(temp_results_dir, exist_ok=True)\nos.makedirs(img_dir, exist_ok=True)\n\nprint(f\"Data directory: {data_dir}\")\nprint(f\"Temp results directory: {temp_results_dir}\")\nprint(f\"Image directory: {img_dir}\")\n\nif __name__ == \"__main__\":\n\n    # Read parameters from command line arguments\n    j = 200\n    r = 0.75\n    m = 3\n\n    print(f\"Processing with j={j}, r={r}, m={m}\")\n\n    combstr = f\"{int(j*r)}|{j}\"\n    heatmap_folder = img_dir\n    \n    img_filename = f\"{int(j*r)}-{j}_{m}\"\n    filename = f\"{combstr}_{m}\"\n    data_file = f\"{data_dir}/{filename}.csv\"\n    data_we_have = pd.read_csv(data_file)\n    n_biomarkers = len(data_we_have.biomarker.unique())\n\n    if not os.path.exists(data_file):\n        print(f\"Data file not found: {data_file}\")\n        sys.exit(1)  # Exit early if the file doesn't exist\n    else:\n        print(f\"Data file found: {data_file}\")\n\n    # Define the temporary result file\n    temp_result_file = os.path.join(temp_results_dir, f\"temp_results_{j}_{r}_{m}.json\")\n    \n    dic = {}\n\n    if combstr not in dic:\n        dic[combstr] = []\n\n    accepted_order_dicts = metropolis_hastings_soft_kmeans(\n        data_we_have,\n        iterations,\n        n_shuffle,\n    )\n\n    utils.save_heatmap(\n        accepted_order_dicts,\n        burn_in, \n        thining, \n        folder_name=heatmap_folder,\n        file_name=f\"{img_filename}\", \n        title=f'heatmap of {filename}')\n    \n    most_likely_order_dic = utils.obtain_most_likely_order_dic(\n        accepted_order_dicts, burn_in, thining)\n    most_likely_order = list(most_likely_order_dic.values())\n    tau, p_value = kendalltau(most_likely_order, range(1, n_biomarkers + 1))\n    \n    dic[combstr].append(tau)\n    \n    # Write the results to a unique temporary file inside the temp folder\n    with open(temp_result_file, \"w\") as file:\n        json.dump(dic, file, indent=4)\n    print(f\"{filename} is done! Results written to {temp_result_file}\")\n\nCurrent working directory: /Users/hongtaoh/Desktop/github/ebmBook\nData directory: /Users/hongtaoh/Desktop/github/ebmBook/data\nTemp results directory: /Users/hongtaoh/Desktop/github/ebmBook/soft_kmeans/temp_json_results\nImage directory: /Users/hongtaoh/Desktop/github/ebmBook/soft_kmeans/img\nProcessing with j=200, r=0.75, m=3\nData file found: /Users/hongtaoh/Desktop/github/ebmBook/data/150|200_3.csv\n\n\n/var/folders/wx/xz5y_06d15q5pgl_mhv76c8r0000gn/T/ipykernel_12345/1007084789.py:261: RuntimeWarning: overflow encountered in exp\n  prob_of_accepting_new_order = np.exp(\n\n\niteration 10 done, current accepted likelihood: -4558.471198243365, current acceptance ratio is 100.00 %, current accepted order is dict_values([5, 9, 3, 4, 2, 1, 6, 8, 7, 10]), \n150|200_3 is done! Results written to /Users/hongtaoh/Desktop/github/ebmBook/soft_kmeans/temp_json_results/temp_results_200_0.75_3.json",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estimate $S$ with Soft K-Means</span>"
    ]
  },
  {
    "objectID": "08_softKMeans.html#result",
    "href": "08_softKMeans.html#result",
    "title": "8  Estimate \\(S\\) with Soft K-Means",
    "section": "8.2 Result",
    "text": "8.2 Result\nWe plot the resulting \\(S\\) probalistically using a heatmap.\n\n\n\n\n\n\nFigure 8.2: Result of Soft K-Means\n\n\n\n\ndic\n\n{'150|200': [-0.15555555555555553]}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estimate $S$ with Soft K-Means</span>"
    ]
  },
  {
    "objectID": "09_conjugatePriors.html",
    "href": "09_conjugatePriors.html",
    "title": "9  Estimate \\(S\\) with Conjugate Priors",
    "section": "",
    "text": "9.1 Implementation\nimport numpy as np \nimport utils \nimport json \nimport pandas as pd \nimport utils \nfrom scipy.stats import kendalltau\nimport sys\nimport os\nimport math \nimport random\nCode\ndef estimate_params_exact(m0, n0, s0_sq, v0, data):\n    '''This is to estimate means and vars based on conjugate priors\n    Inputs:\n        - data: a vector of measurements \n        - m0: prior estimate of $\\mu$.\n        - n0: how strongly is the prior belief in $m_0$ is held.\n        - s0_sq: prior estimate of $\\sigma^2$.\n        - v0: prior degress of freedome, influencing the certainty of $s_0^2$.\n\n    Outputs:\n        - mu estiate, std estimate\n    '''\n    # Data summary\n    sample_mean = np.mean(data)\n    sample_size = len(data)\n    sample_var = np.var(data, ddof=1)  # ddof=1 for unbiased estimator\n\n    # Update hyperparameters for the Normal-Inverse Gamma posterior\n    updated_m0 = (n0 * m0 + sample_size * sample_mean) / (n0 + sample_size)\n    updated_n0 = n0 + sample_size\n    updated_v0 = v0 + sample_size\n    updated_s0_sq = (1 / updated_v0) * ((sample_size - 1) * sample_var + v0 * s0_sq +\n                                        (n0 * sample_size / updated_n0) * (sample_mean - m0)**2)\n    updated_alpha = updated_v0/2\n    updated_beta = updated_v0*updated_s0_sq/2\n\n    # Posterior estimates\n    mu_posterior_mean = updated_m0\n    sigma_squared_posterior_mean = updated_beta/updated_alpha\n\n    mu_estimation = mu_posterior_mean\n    std_estimation = np.sqrt(sigma_squared_posterior_mean)\n\n    return mu_estimation, std_estimation\n\ndef get_theta_phi_conjugate_priors(biomarkers, data_we_have, theta_phi_kmeans):\n    '''To get estimated parameters, returns a hashmap\n    Input:\n    - biomarkers: biomarkers \n    - data_we_have: participants data filled with initial or updated participant_stages\n    - theta_phi_kmeans: a hashmap of dicts, which are the prior theta and phi values\n        obtained from the initial constrained kmeans algorithm\n\n    Output: \n    - a hashmap of dictionaries. Key is biomarker name and value is a dictionary.\n    Each dictionary contains the theta and phi mean/std values for a specific biomarker. \n    '''\n    # empty list of dictionaries to store the estimates\n    hashmap_of_means_stds_estimate_dicts = {}\n\n    for biomarker in biomarkers:\n        # Initialize dictionary outside the inner loop\n        dic = {'biomarker': biomarker}\n        for affected in [True, False]:\n            data_full = data_we_have[(data_we_have.biomarker == biomarker) & (\n                data_we_have.affected == affected)]\n            if len(data_full) &gt; 1:\n                measurements = data_full.measurement\n                s0_sq = np.var(measurements, ddof=1)\n                m0 = np.mean(measurements)\n                mu_estimate, std_estimate = estimate_params_exact(\n                    m0=m0, n0=1, s0_sq=s0_sq, v0=1, data=measurements)\n                if affected:\n                    dic['theta_mean'] = mu_estimate\n                    dic['theta_std'] = std_estimate\n                else:\n                    dic['phi_mean'] = mu_estimate\n                    dic['phi_std'] = std_estimate\n            # If there is only one observation or not observation at all, resort to theta_phi_kmeans\n            # YES, IT IS POSSIBLE THAT DATA_FULL HERE IS NULL\n            # For example, if a biomarker indicates stage of (num_biomarkers), but all participants' stages\n            # are smaller than that stage; so that for all participants, this biomarker is not affected\n            else:\n                # print(theta_phi_kmeans)\n                if affected:\n                    dic['theta_mean'] = theta_phi_kmeans[biomarker]['theta_mean']\n                    dic['theta_std'] = theta_phi_kmeans[biomarker]['theta_std']\n                else:\n                    dic['phi_mean'] = theta_phi_kmeans[biomarker]['phi_mean']\n                    dic['phi_std'] = theta_phi_kmeans[biomarker]['phi_std']\n        # print(f\"biomarker {biomarker} done!\")\n        hashmap_of_means_stds_estimate_dicts[biomarker] = dic\n    return hashmap_of_means_stds_estimate_dicts\n\ndef compute_all_participant_ln_likelihood_and_update_participant_stages(\n        n_participants,\n        data,\n        non_diseased_participant_ids,\n        estimated_theta_phi,\n        disease_stages,\n        participant_stages,\n):\n    all_participant_ln_likelihood = 0\n    for p in range(n_participants):\n        # this participant data\n        pdata = data[data.participant == p].reset_index(drop=True)\n\n        \"\"\"If this participant is not diseased (i.e., if we know k_j is equal to 0)\n        We still need to compute the likelihood of this participant seeing this sequence of biomarker data\n        but we do not need to estimate k_j like below\n\n        We still need to compute the likelihood because we need to add it to all_participant_ln_likelihood\n        \"\"\"\n        if p in non_diseased_participant_ids:\n            this_participant_likelihood = utils.compute_likelihood(\n                pdata, k_j=0, theta_phi=estimated_theta_phi)\n            this_participant_ln_likelihood = np.log(\n                this_participant_likelihood + 1e-10)\n        else:\n            # initiaze stage_likelihood\n            stage_likelihood_dict = {}\n            for k_j in disease_stages:\n                # even though data above has everything, it is filled up by random stages\n                # we don't like it and want to know the true k_j. All the following is to update participant_stages\n                participant_likelihood = utils.compute_likelihood(\n                    pdata, k_j, estimated_theta_phi)\n                # update each stage likelihood for this participant\n                stage_likelihood_dict[k_j] = participant_likelihood\n            likelihood_sum = sum(stage_likelihood_dict.values())\n            normalized_stage_likelihood = [\n                l/likelihood_sum for l in stage_likelihood_dict.values()]\n            sampled_stage = np.random.choice(\n                disease_stages, p=normalized_stage_likelihood)\n            participant_stages[p] = sampled_stage\n\n            # use weighted average likelihood because we didn't know the exact participant stage\n            # all above to calculate participant_stage is only for the purpous of calculate theta_phi\n            this_participant_likelihood = np.mean(likelihood_sum)\n            this_participant_ln_likelihood = np.log(\n                this_participant_likelihood + 1e-10)\n        \"\"\"\n        All the codes in between are calculating this_participant_ln_likelihood. \n        If we already know kj=0, then\n        it's very simple. If kj is unknown, we need to calculate the likelihood of seeing \n        this sequence of biomarker\n        data at different stages, and get the relative likelihood before \n        we get a sampled stage (this is for estimating theta and phi). \n        Then we calculate this_participant_ln_likelihood using average likelihood. \n        \"\"\"\n        all_participant_ln_likelihood += this_participant_ln_likelihood\n    return all_participant_ln_likelihood\n\ndef update_data_by_the_new_participant_stages(data, participant_stages, n_participants):\n    '''This is to fill up data_we_have. \n    Basically, add two columns: k_j, affected, and modify diseased column\n    based on the initial or updated participant_stages\n    Note that we assume here we've already got S_n\n\n    Inputs:\n        - data_we_have\n        - participant_stages: np array \n        - participants: 0-99\n    '''\n    participant_stage_dic = dict(\n        zip(np.arange(0, n_participants), participant_stages))\n    data['k_j'] = data.apply(\n        lambda row: participant_stage_dic[row.participant], axis=1)\n    data['diseased'] = data.apply(lambda row: row.k_j &gt; 0, axis=1)\n    data['affected'] = data.apply(lambda row: row.k_j &gt;= row.S_n, axis=1)\n    return data\n\n\"\"\"The version without reverting back to the max order\n\"\"\"\ndef metropolis_hastings_with_conjugate_priors(\n    data_we_have,\n    iterations,\n    n_shuffle\n):\n    n_participants = len(data_we_have.participant.unique())\n    biomarkers = data_we_have.biomarker.unique()\n    n_biomarkers = len(biomarkers)\n    n_stages = n_biomarkers + 1\n    diseased_stages = np.arange(start=1, stop=n_stages, step=1)\n\n    non_diseased_participant_ids = data_we_have.loc[\n        data_we_have.diseased == False].participant.unique()\n\n    # initialize empty lists\n    acceptance_count = 0\n    all_current_accepted_order_dicts = []\n\n    # initialize an ordering and likelihood\n    # note that it should be a random permutation of numbers 1-10\n    current_accepted_order = np.random.permutation(np.arange(1, n_stages))\n    current_accepted_order_dict = dict(zip(biomarkers, current_accepted_order))\n    current_accepted_likelihood = -np.inf\n\n    participant_stages = np.zeros(n_participants)\n    for idx in range(n_participants):\n        if idx not in non_diseased_participant_ids:\n            # 1-len(diseased_stages), inclusive on both ends\n            participant_stages[idx] = random.randint(1, len(diseased_stages))\n\n    for _ in range(iterations):\n        new_order = current_accepted_order.copy()\n        utils.shuffle_order(new_order, n_shuffle)\n        current_order_dict = dict(zip(biomarkers, new_order))\n\n        # copy the data to avoid modifying the original\n        data = data_we_have.copy()\n        data['S_n'] = data.apply(\n            lambda row: current_order_dict[row['biomarker']], axis=1)\n        # add kj and affected for the whole dataset based on participant_stages\n        # also modify diseased col (because it will be useful for the new theta_phi_kmeans)\n        data = update_data_by_the_new_participant_stages(\n            data, participant_stages, n_participants)\n        # should be inside the for loop because once the participant_stages change, \n        # the diseased column changes as well. \n        theta_phi_kmeans = utils.get_theta_phi_estimates(\n            data_we_have,\n        )\n        estimated_theta_phi = get_theta_phi_conjugate_priors(\n            biomarkers, data, theta_phi_kmeans)\n\n        all_participant_ln_likelihood = compute_all_participant_ln_likelihood_and_update_participant_stages(\n            n_participants,\n            data,\n            non_diseased_participant_ids,\n            estimated_theta_phi,\n            diseased_stages,\n            participant_stages,\n        )\n\n        # ratio = likelihood/best_likelihood\n        # because we are using np.log(likelihood) and np.log(best_likelihood)\n        # np.exp(a)/np.exp(b) = np.exp(a - b)\n        # if a &gt; b, then np.exp(a - b) &gt; 1\n\n        # Log-Sum-Exp Trick\n        max_likelihood = max(all_participant_ln_likelihood,\n                             current_accepted_likelihood)\n        prob_of_accepting_new_order = np.exp(\n            (all_participant_ln_likelihood - max_likelihood) -\n            (current_accepted_likelihood - max_likelihood)\n        )\n        \n        # it will definitly update at the first iteration\n        if np.random.rand() &lt; prob_of_accepting_new_order:\n            acceptance_count += 1\n            current_accepted_order = new_order\n            current_accepted_likelihood = all_participant_ln_likelihood\n            current_accepted_order_dict = current_order_dict\n\n        acceptance_ratio = acceptance_count*100/(_+1)\n        all_current_accepted_order_dicts.append(current_accepted_order_dict)\n\n        # if _ &gt;= burn_in and _ % thining == 0:\n        if (_+1) % 10 == 0:\n            formatted_string = (\n                f\"iteration {_ + 1} done, \"\n                f\"current accepted likelihood: {current_accepted_likelihood}, \"\n                f\"current acceptance ratio is {acceptance_ratio:.2f} %, \"\n                f\"current accepted order is {current_accepted_order_dict.values()}, \"\n            )\n            \n    return all_current_accepted_order_dicts\nn_shuffle = 2\niterations = 10\nburn_in = 2\nthining = 2\n\nbase_dir = os.getcwd()\nprint(f\"Current working directory: {base_dir}\")\ndata_dir = os.path.join(base_dir, \"data\")\nconjugate_priors_dir = os.path.join(base_dir, 'conjugate_priors')\ntemp_results_dir = os.path.join(conjugate_priors_dir, \"temp_json_results\")\nimg_dir = os.path.join(conjugate_priors_dir, 'img')\nresults_file = os.path.join(conjugate_priors_dir, \"results.json\")\n\nos.makedirs(conjugate_priors_dir, exist_ok=True)\nos.makedirs(temp_results_dir, exist_ok=True)\nos.makedirs(img_dir, exist_ok=True)\n\nprint(f\"Data directory: {data_dir}\")\nprint(f\"Temp results directory: {temp_results_dir}\")\nprint(f\"Image directory: {img_dir}\")\n\nif __name__ == \"__main__\":\n\n    # Read parameters from command line arguments\n    j = 200\n    r = 0.75\n    m = 3\n\n    print(f\"Processing with j={j}, r={r}, m={m}\")\n\n    combstr = f\"{int(j*r)}|{j}\"\n    heatmap_folder = img_dir\n    \n    img_filename = f\"{int(j*r)}-{j}_{m}\"\n    filename = f\"{combstr}_{m}\"\n    data_file = f\"{data_dir}/{filename}.csv\"\n    data_we_have = pd.read_csv(data_file)\n    n_biomarkers = len(data_we_have.biomarker.unique())\n\n    if not os.path.exists(data_file):\n        print(f\"Data file not found: {data_file}\")\n        sys.exit(1)  # Exit early if the file doesn't exist\n    else:\n        print(f\"Data file found: {data_file}\")\n\n    # Define the temporary result file\n    temp_result_file = os.path.join(temp_results_dir, f\"temp_results_{j}_{r}_{m}.json\")\n\n    # temp_result_file = f\"{temp_results_dir}/temp_results_{j}_{r}_{m}.json\"\n    \n    dic = {}\n\n    if combstr not in dic:\n        dic[combstr] = []\n\n    accepted_order_dicts = metropolis_hastings_with_conjugate_priors(\n        data_we_have,\n        iterations,\n        n_shuffle,\n    )\n\n    utils.save_heatmap(\n        accepted_order_dicts,\n        burn_in, \n        thining, \n        folder_name=heatmap_folder,\n        file_name=f\"{img_filename}\", \n        title=f'heatmap of {filename}')\n    \n    most_likely_order_dic = utils.obtain_most_likely_order_dic(\n        accepted_order_dicts, burn_in, thining)\n    most_likely_order = list(most_likely_order_dic.values())\n    tau, p_value = kendalltau(most_likely_order, range(1, n_biomarkers + 1))\n    \n    dic[combstr].append(tau)\n    \n    # Write the results to a unique temporary file inside the temp folder\n    with open(temp_result_file, \"w\") as file:\n        json.dump(dic, file, indent=4)\n    print(f\"{filename} is done! Results written to {temp_result_file}\")\n\nCurrent working directory: /Users/hongtaoh/Desktop/github/ebmBook\nData directory: /Users/hongtaoh/Desktop/github/ebmBook/data\nTemp results directory: /Users/hongtaoh/Desktop/github/ebmBook/conjugate_priors/temp_json_results\nImage directory: /Users/hongtaoh/Desktop/github/ebmBook/conjugate_priors/img\nProcessing with j=200, r=0.75, m=3\nData file found: /Users/hongtaoh/Desktop/github/ebmBook/data/150|200_3.csv\n150|200_3 is done! Results written to /Users/hongtaoh/Desktop/github/ebmBook/conjugate_priors/temp_json_results/temp_results_200_0.75_3.json",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimate $S$ with Conjugate Priors</span>"
    ]
  },
  {
    "objectID": "09_conjugatePriors.html#result",
    "href": "09_conjugatePriors.html#result",
    "title": "9  Estimate \\(S\\) with Conjugate Priors",
    "section": "9.2 Result",
    "text": "9.2 Result\nWe plot the resulting \\(S\\) probalistically using a heatmap.\n\n\n\n\n\n\nFigure 9.2: Result of Conjugate Priors\n\n\n\n\ndic\n\n{'150|200': [0.28888888888888886]}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimate $S$ with Conjugate Priors</span>"
    ]
  },
  {
    "objectID": "10_aggResults.html",
    "href": "10_aggResults.html",
    "title": "10  Final Results",
    "section": "",
    "text": "We only used 10 iterations in the previous three chapters to demonstrate how our algorithm works. However, to get them really work, we need to run several thousand iterations.\nAlso, to cancel out noises and randomness, we need to use all the fifity variations of data.\nWith the help of The Center for High Throughput Computing at the University of Wisconsin-Madison, we were able to run these tests. In the following, we present our results.\n\n\n\n\n\n\nFigure 10.1: Results of Soft K-Means\n\n\n\n\n\n\n\n\n\nFigure 10.2: Results of Conjugate Priors\n\n\n\nFrom the two results, we are able to see that it is better to more participants, because that offers more information for our models. In terms of healthy ratio, it seems \\(50\\%\\) is a sweet spot.\nAlso, we notice that conjugate priors performs better than soft K-Means.\nWe also tested the two algorithms developed by UCL POND group with the same 750 datasets: EBM Basic and KDE EBM. See the following for results:\n\n\n\n\n\n\nFigure 10.3: Results of EBM\n\n\n\nThe parameters we used in the package of ebm are:\n\nn_iter = 10000\ngreedy_n_iter=10\ngreedy_n_init=5\n\nMore specific configirations can be found at https://github.com/hongtaoh/ucl_ebm/blob/master/implement/calc_tau_basic.ipynb\n\n\n\n\n\n\nFigure 10.4: Results of KDE EBM\n\n\n\nThe parameters we used in the package of kde-ebm are:\n\nn_iter = 10000\ngreedy_n_iter=10\ngreedy_n_init=5\n\nMore specific configirations can be found at https://github.com/hongtaoh/ucl_kde_ebm/blob/master/implement/calc_tau_basic.ipynb\nWe can see that the performance of KDE EBM is more stable but EBM basic performs well when the healthy ratio is below \\(50\\%\\).\nNeither of these methods had results as good as conjugate priors. We have to point out, that, even though their accuracy is not as high, their speed is really high. Both of these two algorithms can generate results with a single laptop GPU in just one hour; however, it might take days for our algorithms.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Final Results</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Babaki, Behrouz. 2017. “COP-Kmeans Version 1.5.” https://doi.org/10.5281/zenodo.831850.\n\n\nChen, Guangyu, Hao Shu, Gang Chen, B Douglas Ward, Piero G Antuono,\nZhijun Zhang, Shi-Jiang Li, Alzheimer’s Disease Neuroimaging Initiative,\net al. 2016. “Staging Alzheimer’s Disease Risk by Sequencing Brain\nFunction and Structure, Cerebrospinal Fluid, and Cognition\nBiomarkers.” Journal of Alzheimer’s Disease 54 (3):\n983–93.\n\n\nClyde, M, M Cetinkaya-Rundel, C Rundel, D Banks, C Chai, and L Huang.\n2022. “An Introduction to Bayesian Thinking: A Companion to the\nStatistics with r Course. 2020.” https://statswithr.github.io/book/.\n\n\nFonteijn, Hubert M, Marc Modat, Matthew J Clarkson, Josephine Barnes,\nManja Lehmann, Nicola Z Hobbs, Rachael I Scahill, et al. 2012. “An\nEvent-Based Model for Disease Progression and Its Application in\nFamilial Alzheimer’s Disease and Huntington’s Disease.”\nNeuroImage 60 (3): 1880–89.",
    "crumbs": [
      "References"
    ]
  }
]